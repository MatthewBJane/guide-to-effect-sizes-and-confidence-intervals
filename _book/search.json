[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Guide to Effect Sizes and Confidence Intervals",
    "section": "Welcome",
    "text": "Welcome\nThis effect sizes and confidence intervals collaborative guide aims to provide academics, students and researchers with hands-on, step-by-step instructions for calculating effect sizes and confidence intervals for common statistical tests used in the behavioral, cognitive and social sciences, particularly when original data are not available and when reported information is incomplete. It also introduces general background information on effect sizes and confidence intervals, as well as useful R packages for their calculation. Many of the methods and procedures described in this Guide are based on R or R-based Shiny Apps developed by the science community. We were motivated to focus on R as we aim to maximize the reproducibility of our research outcomes and encourage the most reproducible study planning and data analysis workflow, though we also document other methods whenever possible for the reference of our readers. We regularly update this open educational resource, as packages are updated frequently and new packages are developed from time to time in this rapidly changing Open Scholarship era."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Guide to Effect Sizes and Confidence Intervals",
    "section": "Introduction",
    "text": "Introduction\nEffect sizes and confidence intervals are critical metrics for interpreting results and quantifying the magnitude of findings in scientific research. However, calculating these values can be challenging, particularly when original data are unavailable or results are incompletely reported in prior publications. To address this need, our collaborative guide provides hands-on instructions for calculating effect sizes and confidence intervals for common statistical tests in the behavioral, cognitive, and social sciences. Our guide includes background information on these concepts as well as recommendations for useful R packages that can automate many of these computations. R is emphasized due to its capabilities for reproducible analyses; however, we also cover alternative methods for those without expertise in R. This guide is intended to be an evolving open educational resource, updated as new methods and packages become available in this fast-changing era of open scholarship. By compiling these applied instructions, our goal is to enable students and researchers to easily obtain these metrics, facilitating robust and transparent quantification of results, as well as cumulative scientific progress.\n\nGuidelines for contribution\nAll are encouraged to contribute to this Guide. Please note that this Guide is in continuous development such that it will remain a work in progress for an indefinite period of time. This is intended because we hope the Guide to always reflect the state of the art on the topics of effect sizes and confidence intervals. To contribute, there are now two options:\n\nYou can suggest edits and make comments in the following google doc: mgto.org/effectsizeguide.\nYou can suggest edits directly in the online book using Hypothes.is. To do this you will need to create a free account on hypothes.is (hypothes.is/signup; this will take about a minute). Then when you navigate to the online book, you can open the panel on the top right of the screen. There you can suggest edits and create comments with code and latex!\n\n\n\nNotes\n\nPlease use the headings and style as set forth in this document. You can use keyboard shortcuts such as Ctrl + Alt + 1/2/3. The normal text is in Times New Roman font, font size 11. The codes are formatted using the Code Blocks add-on of Google Docs, github theme, font size 8.\nUse the Suggesting mode rather than the Editing mode. Suggesting is now the default mode for this document. Therefore, please do not hesitate to correct mistakes or modify the contents directly.\nAdd a comment to the document if you find anything missing or improper, or if you feel that things are better organized in a different way. We appreciate your suggestions. If you have any questions, please also add a comment. We will reply and seek to clarify in the document body.\nPlease make proper citations (in APA 7th format) and provide relevant links when you refer to any source that is not your own.\n\n\n\nCredit and authorship\nIf you believe you have made sufficient contribution that qualifies you as an author, and you would like to be listed as an author of this Guide, please do not hesitate and list your name and contact information below. The administrators (M. B. J., Q. X., S. K. Y., and G. F.) of this Guide will verify your contribution and add you to the author list. We welcome comments from any person, regardless of whether they want to be an author. You are also welcome to request content to be added to this Guide (please see the Things to add to the guide section in the end).\nThe authorship order is such that M. B. J. and Q. X. will be the first two authors, S. K. Y. will be second author, and G. F. will be the last and the corresponding author. All other contributors will be listed alphabetically in the middle and are all considered joint third authors. Contributors are by default given investigation, writing - original draft, and writing - review & editing CRediT authorship roles. It is possible to take on more roles if contributors prefer. Any change in this authorship order rule will have to be approved by all who are already listed as an author.\n\n\nCite this book\nThis will change soon, but for now you can cite this book with the following citation:\nAPA:\nJané, M. B., Xiao, Q., Yeung, S. K., Ben-Shachar, M. S., Caldwell, A. R., Cousineau, D., Dunleavy, D. J., Elsherif, M., Johnson, B. T., Moreau, D., Riesthuis, P., Röseler, L., Steele, J., Vieira, F. F., Zloteanu, M., & Feldman, G. (2024). Guide to effect sizes and confidence intervals. https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/. Pre\nBibTeX:\n@misc{EffectSizeGuide, \n  title={Guide to effect sizes and confidence intervals}, \n  author={Jané, Matthew B and Xiao, Qinyu and Yeung, Siu Kit and Ben-Shachar, Mattan S and Caldwell, Aaron R and Cousineau, Denis and Dunleavy, Daniel J and Elsherif, Mahmoud and Johnson, Blair T and Moreau, David and Riesthuis, Paul and Röseler, Lukas and Steele, James and Vieira, Felipe F. and Zloteanu, Mircea and Feldman, Gilad},\n  year={2024},\n  url={https://matthewbjane.quarto.pub/effect-size-and-confidence-intervals-guide/}\n}"
  },
  {
    "objectID": "Defining-Effect-Sizes.html",
    "href": "Defining-Effect-Sizes.html",
    "title": "1  Defining Effect Sizes",
    "section": "",
    "text": "Effect sizes quantify the magnitude of effects (i.e., strength of a relationship, size of a difference), which are the outcomes of our empirical research. Effect sizes are by no means a new concept. However, reporting them remained largely optional for many years, and only until recently does it become a community standard: scientists now see reporting effect sizes (in addition to the traditional statistical significance) as a must and journals also start to require such reporting. Notably, in 2001 and 2010, The Publication Manual of the American Psychological Association 5th and 6th editions emphasized that it is “almost always necessary” (Divine et al. 2018) to report effect sizes (APA 2010, 34; see Fritz, Morris, and Richler 2012, which provides a comprehensive summary on history and importance of effect size reporting).\nEffects sizes can be grouped in broad categories as (1) raw effect sizes, and (2) standardized effect sizes. The raw effect sizes are a summary of the results that are expressed in the same units as the raw data. For example, when kilograms are measured, a raw effect size reports a measure in kilograms. Consider the effect of a diet on a treatment group; a control group receives no diet. The change in weight can be expressed as the mean difference between the groups. This measure is also in kg and so is a raw effect size. Standardized effect sizes expressed on a standardized scale which has no longer any unit but which have a universal interpretation. A z score is an example of a standardized measure. This document is concerned exclusively on standardized effect sizes.\n\n\n\n\nAPA. 2010. Publication Manual of the American Psychological Association. American Psychological Association. https://thuvienso.hoasen.edu.vn/handle/123456789/8327.\n\n\nDivine, George W, H James Norton, Anna E Barón, and Elizabeth Juarez-Colunga. 2018. “The Wilcoxon–Mann–Whitney Procedure Fails as a Test of Medians.” The American Statistician 72 (3): 278–86.\n\n\nFritz, Catherine O., Peter E. Morris, and Jennifer J. Richler. 2012. “Effect Size Estimates: Current Use, Calculations, and Interpretation.” Journal of Experimental Psychology: General 141 (1): 2–18. https://doi.org/10.1037/a0024338."
  },
  {
    "objectID": "Benchmarks.html#footnotes",
    "href": "Benchmarks.html#footnotes",
    "title": "2  Benchmarks",
    "section": "",
    "text": "Sawilowsky (2009) expanded Cohen’s benchmarks to include very small effects (\\(d\\) = 0.01), very large effects (\\(d\\) = 1.20), and huge effects (\\(d\\) = 2.0). It has to be noted that very large and huge effects are very rare in experimental social psychology.↩︎\nAccording to this recent meta-analysis on the effect sizes in social psychology studies, “It is recommended that correlation coefficients of .1, .25, and .40 and Hedges’ \\(g\\) (or Cohen’s \\(d\\)) of 0.15, 0.40, and 0.70 should be interpreted as small, medium, and large effects for studies in social psychology.↩︎\nNote, for paired samples, this does not refer to the probability of an increase/decrease in paired samples but rather the probability of a randomly sampled value of X. This is also referred to as the “relative” effect in the literature. Therefore, the results will differ from the concordance probability provided below.↩︎\nThese benchmarks are also recommended by Gignac and Szodorai (2016). Funder and Ozer (2019) expanded them to also include very small effects (\\(r\\) = .05) and very large effects (\\(r\\) = .40 or greater). According to them, […] an effect-size \\(r\\) of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size \\(r\\) of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size \\(r\\) of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication.” But see here for controversies with this paper.↩︎\nThe benchmarks for Cramer’s V are dependent on the size of the contingency table on which the effect is calculated. According to Cohen, use benchmarks for phi coefficient divided by the square root of the smaller dimension minus 1. For example, a medium effect for a Cramer’s V from a 4 by 3 table would be .3 / sqrt(3 - 1) = .21.↩︎"
  },
  {
    "objectID": "Reporting-Effect-Sizes.html#transparency",
    "href": "Reporting-Effect-Sizes.html#transparency",
    "title": "3  Reporting Effect Sizes",
    "section": "3.1 Transparency",
    "text": "3.1 Transparency\nWhen reporting effect sizes and their calculations, you should prioritize transparency and reproducibility. No matter what tool you used to calculate your effect size (R is the most recommended tool here), you must make sure that others can easily follow your procedures and obtain the same results. This means that if you use online calculators (which is discouraged) or standalone programs (JAMOVI is most recommended; you can also use JASP, which however does not allow access to syntax at this moment), you should include screenshots that capture the input and output, with clear explanations. If you use R, Python or other programming languages, you should copy-and-paste your codes into your supplementary document (or submit your scripts to open online repositories), ideally with annotations and comments explaining the codes. inputs and outputs."
  },
  {
    "objectID": "Reporting-Effect-Sizes.html#directionality",
    "href": "Reporting-Effect-Sizes.html#directionality",
    "title": "3  Reporting Effect Sizes",
    "section": "3.2 Directionality",
    "text": "3.2 Directionality\nSome effect sizes are directional (e.g., Cohen’s \\(d\\), Pearson correlations \\(r\\)), which means that they can be positive or negative. Their signs carry important information, and therefore cannot be omitted. When you report these effect sizes, make it clear what is compared to what (i.e., the direction of comparison). Better still, make sure your comparison is inline with the theory. For instance, a theory predicts that your group X should score higher on an item than your Group Y,1 you should hypothesize accordingly that Group X will have a higher mean than Group Y on the item, and subtract mean(Y) from mean(X) (rather than the other way around) to obtain the mean difference. You should then expect your \\(t\\) statistic to be positive, and your \\(d\\) value as well. In other words, avoid reporting anything like \\(t\\) = -5.14, \\(d\\) = 0.36, where the signs of the statistics do not match."
  },
  {
    "objectID": "Reporting-Effect-Sizes.html#precision",
    "href": "Reporting-Effect-Sizes.html#precision",
    "title": "3  Reporting Effect Sizes",
    "section": "3.3 Precision",
    "text": "3.3 Precision\nEffect sizes may be very precisely estimated from the available data, the used methodology, and how the population was sampled. It might also be estimated with little confidence on the resulting number. This may be the case for example when the sample is very small, when the population displays a lot of variability, when a between-group design is used instead of a paired-sample design, and finally, when clustered sampling is used instead of randomized sampling. Precision can be estimated using various tools, but probably the most commonly used one is the Confidence intervals. This interval has a confidence level, frequently 95%.\n\n\n\n\nGelman, Andrew. 2011. “Why It Doesn’t Make Sense in General to Form Confidence Intervals by Inverting Hypothesis Tests | Statistical Modeling, Causal Inference, and Social Science.” https://statmodeling.stat.columbia.edu/2011/08/25/why_it_doesnt_m/.\n\n\nMorey, Richard D., Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers. 2016. “The Fallacy of Placing Confidence in Confidence Intervals.” Psychonomic Bulletin & Review 23 (1): 103–23. https://doi.org/10.3758/s13423-015-0947-8."
  },
  {
    "objectID": "Reporting-Effect-Sizes.html#footnotes",
    "href": "Reporting-Effect-Sizes.html#footnotes",
    "title": "3  Reporting Effect Sizes",
    "section": "",
    "text": "Of course, if a theory/effect predicts Group X has a higher mean than Group Y, then it also predicts the reverse, i.e., Group Y has a lower mean than Group X. But theories/effects are commonly articulated in a certain way. It is more common that we say, for example, people prefer the status quo rather than that people do not prefer the non-status quo, when we refer to the status quo bias. Consider another “theory”: teenagers get taller when they get older. It just does not make sense to say the same thing reversely, i.e., teenagers get shorter when they get younger, because people cannot get younger, at least in the 2020s.↩︎"
  },
  {
    "objectID": "Interpreting-Confidence-Intervals.html",
    "href": "Interpreting-Confidence-Intervals.html",
    "title": "4  Interpreting Confidence Intervals",
    "section": "",
    "text": "What is the correct interpretation of a confidence interval? Imagine you conducted a study where you compared two groups. You obtained a Cohen’s \\(d\\) = 0.3, 95% CI [0.2, 0.4]. How do you interpret this confidence interval?\nConfidence intervals are yielded by a certain procedure, such that when the procedure is repeatedly applied to a series of hypothetical datasets drawn from the studied population/populations, it yields intervals that contain the true parameter value (in our example, it means the true difference between the two groups) in 95% of the cases. For the effect estimate and confidence intervals to be valid, the data and test must meet the assumptions of the estimating procedure.\nIn colloquial terms, if we conduct this research over and over (repeating the same sampling procedure, administering the same experimental manipulation, conducting the same statistical analysis, etc.), because of sampling variability (our samples are slightly different at each time), we will get different Cohen’s \\(d\\) values. For each of these \\(d\\) values, we calculate a 95% interval. Then, among all these many intervals, we expect that 95% of them will contain the true \\(d\\), which we never know exactly.\nThere is also a common criticism levied against the confidence interval interpretation: “There is a 95% probability that the true parameter exists within the 95% confidence interval”. However this criticism is unwarranted in the specific case of a single observed confidence interval, that is, as long as there is a single realized confidence interval sampled from the population, this interpretation is fine (Vos and Holbert 2022). It is important to note however, this interpretation is incorrect when there are multiple realized confidence intervals randomly sampled from the same population. The criticized interpretation also tends to be more practical than the interpretation using repeated sampling, the following example described by Vos and Holbert (2022) illustrates this,\n\nThe distinction between these interpretations can be understood with the simple example of the probability of rolling a ‘6’ with a fair die. The probability is 1/6 because if you roll the die repeatedly the proportion of times that the face with ‘6’ comes up will be come very close to 1/6. Or, the probability is 1/6 because it is equivalent to a random selection from an urn where exactly one of 6 balls is labelled with ‘6’. The distinction in this simple example is less useful since repeatedly rolling a die is less problematic than repeatedly conducting the same randomized trial.\n\nFor further reading on confidence interpretations, see Hoekstra et al. (2014) and Morey et al. (2016).\n\n\n\n\nHoekstra, Rink, Richard D. Morey, Jeffrey N. Rouder, and Eric-Jan Wagenmakers. 2014. “Robust Misinterpretation of Confidence Intervals.” Psychonomic Bulletin & Review 21 (5): 1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nMorey, Richard D., Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers. 2016. “The Fallacy of Placing Confidence in Confidence Intervals.” Psychonomic Bulletin & Review 23 (1): 103–23. https://doi.org/10.3758/s13423-015-0947-8.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical Inference Without Repeated Sampling.” Synthese 200 (2): 89. https://doi.org/10.1007/s11229-022-03560-x."
  },
  {
    "objectID": "Reporting-Confidence-Intervals.html",
    "href": "Reporting-Confidence-Intervals.html",
    "title": "5  Reporting Confidence Intervals",
    "section": "",
    "text": "Confidence intervals must be calculated and reported for every effect size that you obtained and mentioned in your manuscript. If you are doing a replication and your target article/study did not report CIs for its effect sizes, you should calculate CIs and report them.\nNormally, we calculate 95% confidence intervals (i.e., 95% of such intervals are expected to contain the true parameter value if we conduct an infinite number of identical studies).\n\n\n\n\n\n\nAlpha level\n\n\n\nThe confidence interval depends on the alpha level, that is, the proportion of CIs upon repeated sampling that will not contain the true parameter. If the true effect is zero (or null), the the alpha level represents the false positive rate (i.e., the rate of observing a significant effect when there is none). The 95% CI is based on an alpha level of .05, however researchers can choose any value (between 0 and 1), as long as it is properly justified (Lakens 2022).\n\n\nNonetheless, for some effect sizes (e.g., eta-squared, partial eta-squared, R-squared), we calculate 90% confidence intervals. This is because \\(\\eta^2\\) is squared and always positive, and F-tests are one-sided. Reporting 95% CI for eta squared may result in situations in which the CI includes zero but the p-value falls below .05, whereas reporting 90% CI prevents such a problem. For further information regarding this issue, read Daniel Lakens blog on confidence intervals and Steiger (2004).\nConfidence intervals should be reported immediately after an effect size, e.g., Cohen’s d = 0.40, 95% CI [0.20, 0.60]. After the first time reporting them in a manuscript, every subsequent CI can be simply denoted by brackets without the “95% CI” preceding it.\nUnless you are measuring something that is meaningful in real life (e.g., income, years of experience, amount that a person is willing to donate), please make sure that the CI you calculated is a CI of the effect size, not of other statistics, such as the test statistics or mean difference in raw units.\nIf you see that the effect size estimate is not included within your CI, you likely have an issue, check carefully. For means and for difference in means, the estimate should be precisely the midpoint of your CI; for other statistics (e.g., correlation, proportion, frequency, standard deviation), one arm might be longer than the other so the estimate may not be the midpoint.\nFor further reading related to the calculation and reporting of effect sizes and confidence intervals, see Steiger (2004) and Lakens (2014).\n\n\n\n\nLakens, Daniël. 2014. “The 20.” http://daniellakens.blogspot.com/2014/06/calculating-confidence-intervals-for.html.\n\n\n———. 2022. “Sample Size Justification.” Collabra: Psychology 8 (1): 33267. https://doi.org/10.1525/collabra.33267.\n\n\nSteiger, James H. 2004. “Beyond the f Test: Effect Size Confidence Intervals and Tests of Close Fit in the Analysis of Variance and Contrast Analysis.” Psychological Methods 9 (2): 164–82. https://doi.org/10.1037/1082-989X.9.2.164."
  },
  {
    "objectID": "Useful-R-Packages.html#why-use-r",
    "href": "Useful-R-Packages.html#why-use-r",
    "title": "6  Using R",
    "section": "6.1 Why Use R?",
    "text": "6.1 Why Use R?\nWe strongly recommend using open-source software such as R or Python for computing effect sizes and confidence intervals. In this guide, we focus on R, which has several advantages:\n\nReproducibility: R syntax can be shared to allow others to reproduce your analyses. This promotes transparency and reliability in research.\nFlexibility: CRAN repositories contain thousands of user-contributed packages for specialized statistical techniques. This allows calculating a diverse range of effect size and CI metrics.\nFree and open source: R is free to download and use. The open source nature means community-driven innovation and packages.\nVisualizations: R makes it easy to create publication-quality graphics to visualize your results.\nScripting: Automating analyses through R scripts improves efficiency and consistency.\nRange of packages: Packages like effectsize, MBESS, metafor, and more contain a variety of effect size and CI functions.\n\nMany (if not all) of these advantages are shared with Python and a number of other programming languages. While online calculators or GUI software can also allow calculating confidence intervals and effect sizes, open-source software such as R provide transparency, reproducibility, and access to a vast array of techniques. In the case of R, the learning curve is well worth it for doing robust, state-of-the-art effect size and confidence interval estimation."
  },
  {
    "objectID": "Useful-R-Packages.html#useful-r-packages",
    "href": "Useful-R-Packages.html#useful-r-packages",
    "title": "6  Using R",
    "section": "6.2 Useful R Packages",
    "text": "6.2 Useful R Packages\nThe following R packages are handy for effect size and CI calculations, conversions among different effect sizes, and conversion of test statistics to effect sizes. If you use one of the packages below, please make sure you cite them to give the authors their due credit! To obtain citations for packages, you can use the citation() function and input the name of the package as a string.\n\nMOTE (Buchanan et al. 2019): This is a highly recommended package for calculating effect sizes, which is capable of handling a wide variety of effect sizes in the difference family (the d family) and variance-overlap family (r, eta, omega, epsilon). The functions also provide non-central confidence intervals for each effect size and output in APA style in LaTeX. MOTE has an online shiny application (doomlab.shinyapps.io/mote/). The CRAN project can be found here: cran.r-project.org/package=MOTE.\neffectsize (Ben-Shachar, Lüdecke, and Makowski 2020): This package is particularly useful in data analysis. A major advantage of this package is that it takes in many different model objects and directly outputs effect sizes and CIs. It also implements conversions between a wide array of indices and features functions to perform automated effect size interpretations based on existing benchmark thresholds. The CRAN project can be found here: cran.r-project.org/package=effectsize.\nMBESS (Kelley 2022): One of the most comprehensive and useful packages for effect size and confidence interval calculations. It provides functions that can calculate ESs and CIs from test statistics and the p-value. The CRAN project can be found here: cran.r-project.org/package=MBESS.\nmetafor (Viechtbauer 2010): Probably the most comprehensive meta-analysis package currently available. Includes the function, escalc(), that calculates various types of effect sizes from test-statistics, summary statistics, and more. The CRAN project can be found here: cran.r-project.org/package=metafor.\npsych (William Revelle 2023): One of the most comprehensive and general packages for common statistical procedures in psychology research. It also includes some effect size and CI calculation functions (e.g., cohen.d()). The CRAN project can be found here: cran.r-project.org/package=psych.\nesc (Lüdecke 2019): This package can help convert among different effect sizes (pp. 4-12 in the reference manual). It’s also helpful when only incomplete information (e.g., only descriptives, or only p-values) have been provided in the paper, and we want to calculate effect sizes from them. Another package that provides similar conversion functions is the compute.es package. The CRAN project can be found here: cran.r-project.org/package=esc.\npsychmeta (Dahlke and Wiernik 2019): This package is mainly used for psychometric meta-analyses. It has a function for converting different effect sizes/test statistics (convert_es, p. 38 in the reference manual), including \\(r\\), \\(d\\), \\(t\\)-statistic (and its p-value), \\(F\\) (and its p-value in two-group one-way ANOVA), chi-squared (one degree of freedom), etc., to \\(r\\), \\(d\\) and the common language effect sizes (CLES, A, AUC). The CRAN project can be found here cran.r-project.org/package=psychmeta.\neffsize (Torchiano 2020): This is a relatively lightweight package that handles d, g, Cliff delta, and Vargha-Delaney A). The CRAN project can be found here: cran.r-project.org/package=effsize.\nMAd (W. T. Hoyt 2014): This package is a collection of functions for conducting a meta-analysis with mean differences data. It also provides conversion functions. The CRAN project can be found here: cran.r-project.org/package=MAd.\nTOSTER (Läkens 2017; Caldwell 2022): This package is designed for equivalence testing. It contains many functions to test for differences in effect sizes along with other useful functions for effect size comparisons. The CRAN project can be found here: cran.r-project.org/package=TOSTER.\nDeclareDesign (Blair et al. 2019): This simulation framework can be used to assess whether procedures for calculating confidence intervals are valid and can be used for arbitrary designs. The diagnose_design() function calculates coverage for designs with estimation strategies that produce confidence intervals. The CRAN project can be found here: cran.r-project.org/package=DeclareDesign.\n\n\n\n\n\nBen-Shachar, Mattan S., Daniel Lüdecke, and Dominique Makowski. 2020. “effectsize: Estimation of Effect Size Indices and Standardized Parameters.” Journal of Open Source Software 5 (56): 2815. https://doi.org/10.21105/joss.02815.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. 2019. “Declaring and Diagnosing Research Designs.” American Political Science Review 113: 838–59. https://declaredesign.org/paper.pdf.\n\n\nBuchanan, Erin M., Amber Gillenwaters, John E. Scofield, and K. D. Valentine. 2019. MOTE: Measure of the Effect: Package to Assist in Effect Size Calculations and Their Confidence Intervals. http://github.com/doomlab/MOTE.\n\n\nCaldwell, Aaron R. 2022. “Exploring Equivalence Testing with the Updated TOSTER r Package.” PsyArXiv. https://doi.org/10.31234/osf.io/ty8de.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “psychmeta: An r Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nKelley, Ken. 2022. MBESS: The MBESS r Package. https://CRAN.R-project.org/package=MBESS.\n\n\nLäkens, Daniel. 2017. “Equivalence Tests: A Practical Primer for t-Tests, Correlations, and Meta-Analyses.” Social Psychological and Personality Science 1: 1–8. https://doi.org/10.1177/1948550617697177.\n\n\nLüdecke, Daniel. 2019. Esc: Effect Size Computation for Meta Analysis (Version 0.5.1). https://doi.org/10.5281/zenodo.1249218.\n\n\nTorchiano, Marco. 2020. Effsize: Efficient Effect Size Computation. https://doi.org/10.5281/zenodo.1480624.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in R with the metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\nW. T. Hoyt, A. C. Del Re &. 2014. MAd: Meta-Analysis with Mean Differences. R Package. https://CRAN.R-project.org/package=MAd.\n\n\nWilliam Revelle. 2023. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#reporting-a-t-test-with-effect-size-and-ci",
    "href": "Standardized-Mean-Differences.html#reporting-a-t-test-with-effect-size-and-ci",
    "title": "7  Mean Differences",
    "section": "7.1 Reporting a t-test with effect size and CI",
    "text": "7.1 Reporting a t-test with effect size and CI\nWhatever effect size and CI you choose to report, you can report it alongside the t-test statistics (i.e., t-value and the p value). For example,\n\nThe treatment group had a significantly higher mean than the control group (t = 2.76, p = .009, n = 35, d = 0.47 [0.11, 0.81])."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#sec-single-group",
    "href": "Standardized-Mean-Differences.html#sec-single-group",
    "title": "7  Mean Differences",
    "section": "7.2 Single Group Designs",
    "text": "7.2 Single Group Designs\nFor a single group design, we have one group and we want to compare the mean of that group to some constant, \\(C\\) (i.e., a target value). The standardized mean difference for a single group can be calculated by (equation 2.3.3, Cohen 1988),\n\\[\nd_s = \\frac{M-C}{S_1}\n\\]\nA positive \\(d_s\\) value would indicate that the mean of group 1 is larger than the target value, \\(C\\). This formulation assumes that the sample is drawn from a normal distribution. The standardizer (i.e., the denominator) is the sample standard deviation. The corresponding standard error for \\(d_s\\) is (see documentation for Caldwell 2022),\n\\[\nSE_{d_s} = \\sqrt{\\frac{1}{n}+\\frac{d_s^2}{2n}}.\n\\]\nIn R, we can use the d.single.t function from the MOTE package to calculate the single group standardized mean difference.\n\n# Install packages if not already installed:\n# install.packages('MOTE')\n# Cohen's d for one group\n\n# For example:\n# Sample Mean = 30.4, SD = 22.53, N = 96\n# Target Value, C = 15\n\nlibrary(MOTE)\n\nstats &lt;- d.single.t(\n  m = 30.4,\n  u = 15,\n  sd = 22.53,\n  n = 96\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 0.684 0.460 0.904\n\n\nAs you can see, the output shows that the effect size is \\(d_s\\) = 0.68, 95% CI [0.46, 0.90]. Note the apa function in MOTE takes a value and returns an APA formatted effect size value (i.e., leading zero and three decimal places)."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#sec-two-ind-group",
    "href": "Standardized-Mean-Differences.html#sec-two-ind-group",
    "title": "7  Mean Differences",
    "section": "7.3 Two Independent Groups Design",
    "text": "7.3 Two Independent Groups Design\n\n7.3.1 Standardize by Pooled Standard Deviation (\\(d_p\\))\nFor a two group design (i.e., between-groups design), we want to compare the means of two groups (group 1 and group 2). The standardized mean difference between two groups can be calculated by (equation 5.1, Glass, McGaw, and Smith 1981),\n\\[\nd_p = \\frac{M_1-M_2}{S_p}.\n\\]\nA positive \\(d_p\\) value would indicate that the mean of group 1 is larger than the mean of group 2. Dividing the mean difference by the pooled standard deviation, \\(S_p\\), is the classic formulation of Cohen’s \\(d\\). The pooled standard deviation, \\(S_p\\), can be calculated as the square root of the average variance (weighted by the degrees of freedom, \\(df=n-1\\)) of group 1 and group 2 (pp. 108, Glass, McGaw, and Smith 1981):\n\\[\nS_p = \\sqrt{\\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}}\n\\]\nNote that the term variance refers to the square of the standard deviation (\\(S^2\\)). Cohen’s \\(d_p\\) has is related to the t-statistic from an independent samples t-test. In fact, we can calculate the \\(d_p\\) value from the \\(t\\)-statistic with the following formula (equation 5.3, Glass, McGaw, and Smith 1981):\n\\[\nd = t\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}.\n\\]\nThe corresponding standard error of \\(d_p\\) is,\n\\[\nSE_{d_p} = \\sqrt{\\frac{n_1+n_2}{n_1 n_2}+\\frac{d_p^2}{2(n_1+n_2)}}.\n\\]\nIn R, we can use the d.ind.t function from the MOTE package to calculate the two group standardized mean difference. Since we have already loaded in the MOTE package, we do not need to again.\n\n# Cohen's d for two independent groups\n# given means and SDs\n\n# For example:\n# Group 1 Mean = 30.4, SD = 22.53, N = 96\n# Group 2 Mean = 21.4, SD = 19.59, N = 96\n\nstats &lt;- d.ind.t(\n  m1 = 30.4,\n  m2 = 21.4,\n  sd1 = 22.53,\n  sd2 = 19.59,\n  n1 = 96,\n  n2 = 96,\n  a = 0.05\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 0.426 0.140 0.712\n\n\nThe output shows that the effect size is \\(d_p\\) = 0.43, 95% CI [0.14, 0.71].\n\n\n7.3.2 Standardize by Control Group Standard Deviation (\\(d_{\\Delta}\\))\nWhen two groups differ substantially in their standard deviations, we can instead standardize by the control group standard deviation (\\(S_C\\)), such that,\n\\[\nd_{\\Delta} = \\frac{M_T-M_C}{S_C}.\n\\]\nWhere the subscripts, \\(T\\) and \\(C\\), denotes the treatment group and control group, respectively. This formulation is commonly referred to as Glass’ \\(\\Delta\\) (Glass 1981). The standard error for \\(d_{\\Delta}\\) can be defined as,\n\\[\nSE_{d_{\\Delta}} = \\sqrt{\\frac{n_T+n_C}{n_T n_C} + \\frac{d_\\Delta^2}{n_C+1} }\n\\]\nNotice that when we only standardize by the standard deviation of the control group (rather than pooling), we he will have less degrees of freedom (\\(df=n_C-1\\)) and therefore more sampling error than we do when we divide by the pooled standard deviation (\\(df= n_T + n_C - 2\\)).In R, we can use the delta.ind.t.diff function from the MOTE package to calculate \\(d_\\Delta\\).\n\n# Cohen's dz for difference scores\n# given difference score means and SDs\n\n# For example:\n# Control group Mean = 30.4, SD = 22.53, N = 96\n# Treatment group Mean = 21.4, SD = 19.59, N = 96\n# correlation between conditions: r = .40\n\nstats &lt;- delta.ind.t(\n  m1 = 30.4,\n  m2 = 21.4,\n  sd1 = 22.53,\n  sd2 = 19.59,\n  n1 = 96,\n  n2 = 96,\n  a = 0.05\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 0.399 0.140 0.712"
  },
  {
    "objectID": "Standardized-Mean-Differences.html#sec-repeated-measures",
    "href": "Standardized-Mean-Differences.html#sec-repeated-measures",
    "title": "7  Mean Differences",
    "section": "7.4 Repeated Measures Designs",
    "text": "7.4 Repeated Measures Designs\nIn a repeated-measures design, the same subjects (or items, etc.) are measured on two or more separate occasions, or in multiple conditions within a single session, and we want to know the mean difference between those occasions or conditions (Baayen, Davidson, and Bates 2008; Barr et al. 2013). An example of this would be in a pre/post comparison where subjects are tested before and after undergoing some treatment (see Figure 7.1 for a visualization). A standardized mean difference in a repeated-measures design can take on a few different forms that we define below.\n\n\n\n\n\nFigure 7.1: Figure displaying simulated data of a repeated measures design, the x-axis shows the condition (e.g., pre-test and post-test) and y-axis is the scores. Lines indicate within person pre/post change.\n\n\n\n\n\n7.4.1 Difference Score \\(d\\) (\\(d_z\\))\nInstead of comparing the means of two sets of scores, a within subject design allows us to subtract the scores obtained in condition 1 from the scores in condition 2. These difference scores (\\(X_{\\text{diff}}=X_2-X_1\\)) can be used similarly to the single group design (if the target value was zero, i.e., \\(C=0\\)) such that (equation 2.3.5, Cohen 1988),\n\\[\nd_z = \\frac{M_{\\text{diff}}}{S_{\\text{diff}}}\n\\]\nWhere the difference between this formulation and the single group design is the nature of the scores (difference scores rather than raw scores). The convenient thing about \\(d_z\\) is that it has a straight-forward relationship with the \\(t\\)-statistic, \\(d_z=\\frac{t}{\\sqrt{n}}\\). This makes it very useful for power analyses. If the standard deviation of difference scores are not accessible, then it can be calculated using the standard deviation of condition 1 (\\(S_1\\)), the standard deviation of condition 2 (\\(S_2\\)), and the correlation between conditions (\\(r\\)) (equation 2.3.6, Cohen 1988):\n\\[\nS_{\\text{diff}}=\\sqrt{S^2_1 + S^2_2 - 2 r S_1 S_2}\n\\]\nIt is important to note that when the correlation between groups is large, then the \\(d_z\\) value will also be larger, whereas a small correlation will return a smaller \\(d_z\\) value. The standard error of \\(d_z\\) can be calculated similarly to the single group design such that,\n\\[\nSE_{d_z} = \\sqrt{\\frac{1}{n}+\\frac{d_z^2}{2n}}\n\\]\nIn R, we can use the d.ind.t.diff function from the MOTE package to calculate \\(d_z\\).\n\n# Cohen's dz for difference scores\n# given difference score means and SDs\n\n# For example:\n# Difference Score Mean = 21.4, SD = 19.59, N = 96\n\nlibrary(MOTE)\n\nstats &lt;- d.dep.t.diff(\n  m = 21.4,\n  sd = 19.59,\n  n = 96,\n  a = 0.05\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 1.092 0.837 1.344\n\n\nThe output shows that the effect size is \\(d_z\\) = 1.09, 95% CI [0.84, 1.34].\n\n\n7.4.2 Repeated Measures \\(d\\) (\\(d_{rm}\\))\nFor a within-group design, we want to compare the means of scores obtained from condition 1 and condition 2. The repeated measures standardized mean difference between the two conditions can be calculated by (equation 9, Lakens 2013),\n\\[\nd_{rm} = \\frac{M_2-M_1}{S_w}.\n\\]\nA positive \\(d_{rm}\\) value would indicate that the mean of condition 2 is larger than the mean of condition 1. The standardizer here is the within-subject standard deviation, \\(S_w\\). The within-subject standard deviation can be defined as,\n\\[\nS_{w}=\\sqrt{\\frac{S^2_1 + S^2_2 - 2 r S_1 S_2}{2(1-r)}}.\n\\]\nWe can also express \\(S_w\\) in terms of the standard deviation of difference scores (\\(S_{\\text{diff}}\\)),\n\\[\nS_w = \\frac{S_{\\text{diff}}}{ \\sqrt{2(1-r)} }.\n\\]\nFurthermore, we can even express \\(d_{rm}\\) in terms of the difference score standardized mean difference (\\(d_z\\)),\n\\[\nd_{rm} = d_z \\times \\sqrt{2(1-r)}.\n\\]\nUltimately the \\(d_{rm}\\) is more appropriate as an effect size estimate for use in meta-analysis whereas \\(d_z\\) is more appropriate for power analysis (Lakens 2013). The standard error for \\(d_{rm}\\) can be computed as,\n\\[\nSE_{d_{rm}} = \\sqrt{\\left(\\frac{1}{n} + \\frac{d^2_{rm}}{2n}\\right) \\times 2(1-r)}\n\\]\nIn R, we can use the d.ind.t.rm function from the MOTE package to calculate the repeated measures standardized mean difference (\\(d_{rm}\\)).\n\n# Cohen's d for repeated measures\n# given means and SDs and correlation\n\n# For example:\n# Condition 1 Mean = 30.4, SD = 22.53, N = 96\n# Condition 2 Mean = 21.4, SD = 19.59, N = 96\n# correlation between conditions: r = .40\n\nstats &lt;- d.dep.t.rm(\n  m1 = 30.4,\n  m2 = 21.4,\n  sd1 = 22.53,\n  sd2 = 19.59,\n  r = .40,\n  n = 96,\n  a = 0.05\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 0.425 0.215 0.633\n\n\nThe output shows that the effect size is \\(d_{rm}\\) = 0.42, 95% CI [0.21, 0.63].\n\n\n7.4.3 Average Variance \\(d\\) (\\(d_{av}\\))\nThe problem with \\(d_{z}\\) and \\(d_{rm}\\), is that they require the correlation between conditions. In practice, correlations between conditions are frequently not reported. An alternative estimator of Cohen’s \\(d\\) in repeated measures design is to simply use the classic variation of cohen’s \\(d\\) (i.e., pooled standard deviation). In a repeated measures design, the sample size does not change between conditions. Therefore weighting the variance of condition 1 and condition 2 by their respective degrees of freedom (i.e., \\(df=n-1\\)) is an unnecessary step. Instead, we can standardize by the square root of the average the variances of condition 1 and 2 (see equation 5, Algina and Keselman 2003):\n\\[\nd_{av} = \\frac{M_2 - M_1}{\\sqrt{\\frac{S_1^2 + S_2^2}{2}}}\n\\]\nThis formulation is convenient especially when the correlation is not present, however without the correlation it fails to take into account the consistency of change between conditions. The standard error of the \\(d_{av}\\) can be expressed as (equation 9, Algina and Keselman 2003),\n\\[\nSE_{d_{av}}= \\sqrt{\\frac{2(S^2_1 + S^2_2 - 2rS_1S_2)}{n(S_1^2+S^2)}}\n\\]\nIn R, we can use the d.ind.t.rm function from the MOTE package to calculate the repeated measures standardized mean difference (\\(d_{rm}\\)).\n\n# Cohen's d for repeated measures (average variance)\n# given means and SDs \n\n# For example:\n# Condition 1 Mean = 30.4, SD = 22.53, N = 96\n# Condition 2 Mean = 21.4, SD = 19.59, N = 96\n\nstats &lt;- d.dep.t.avg(\n  m1 = 30.4,\n  m2 = 21.4,\n  sd1 = 22.53,\n  sd2 = 19.59,\n  n = 96,\n  a = 0.05\n)\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(stats$d), \n           dlow = apa(stats$dlow), \n           dhigh = apa(stats$dhigh))\n\n      d  dlow dhigh\n1 0.427 0.217 0.635\n\n\nThe output shows that the effect size is \\(d_{av}\\) = 0.43, 95% CI [0.22, 0.64].\n\n\n7.4.4 Becker’s \\(d\\) (\\(d_b\\))\nAn even simpler variant of repeated measures \\(d\\) value comes from Becker (1988). Becker’s \\(d\\) standardizes simply by the pre-test standard deviation when the comparison is a pre/post design,\n\\[\nd_b = \\frac{M_{\\text{post}}-M_{\\text{pre}}}{S_{\\text{pre}}}.\n\\]\nThe convenient interpretation of “change in baseline standard deviations” can be quite useful. We can also obtain the standard error with (equation 13, Becker 1988),\n\\[\nSE_{d_b} = \\sqrt{\\frac{2(1-r)}{n}+\\frac{d_b^2}{2n}}\n\\]\nNotice that even though the formula for calculating \\(d_b\\) did not include the correlation coefficient, the standard error does.\nIn base R, we can calculate Becker’s formulation of standardized mean difference using the equations above.\n\n# Install the package below if not done so already\n# install.packages(escalc)\n# Cohen's d for repeated measures (becker's d)\n# given means, the pre-test SDs, and the correlation\n\n# For example:\n# Pre-test Mean = 21.4, SD = 19.59, N = 96\n# Post-test Mean = 30.4, N = 96\n# Correlation between conditions: r = .40\n\nMpre &lt;- 21.4\nMpost &lt;- 30.4\nSpre &lt;- 19.59\nr &lt;- .40\nn &lt;- 96\na &lt;- 0.05\n\nd &lt;- (Mpost - Mpre) / Spre\n\nSE &lt;- sqrt( 2*(1-r)/n + d^2/(2*n) )\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(d), \n           dlow = apa(d - 1.96*SE), \n           dhigh = apa(d + 1.96*SE))\n\n      d  dlow dhigh\n1 0.459 0.231 0.688\n\n\nThe output shows that the effect size is \\(d_{rm}\\) = 0.46, 95% CI [0.23, 0.69].\n\n\n7.4.5 Comparing Repeated Measures \\(d\\) values\nFigure 7.2 shows repeated measures designs with a high (\\(r=\\) .95) and low (\\(r=\\) .05) correlation between conditions. Let us fix the standard deviations and means for both conditions (i.e., high and low correlation) and only vary the correlation. Now we can compare the repeated measures estimators based on these two conditions shown in Figure 7.2:\n\nHigh correlation:\n\n\\(d_z=1.24\\)\n\\(d_{rm}=0.39\\)\n\\(d_{av}=0.43\\)\n\\(d_{b}=0.40\\)\n\nLow correlation:\n\n\\(d_z=0.31\\)\n\\(d_{rm}=0.43\\)\n\\(d_{av}=0.43\\)\n\\(d_{b}=0.40\\)\n\n\nWe notice that the correlation greatly influences \\(d_z\\) more than any other estimator. The \\(d_{rm}\\) value has very little change, whereas \\(d_{av}\\) and \\(d_{b}\\) do not take into account the correlation at all.\n\n\n\n\n\nFigure 7.2: Figure displaying simulated data of a repeated measures design, the x-axis shows the condition (e.g., pre-test and post-test) and y-axis is the scores. Left panel shows a high pre/post correlation (\\(r\\) = .95) and right panel shows a low correlation condition (\\(r\\) = .05). Lines indicate within person pre/post change."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#sec-ppc",
    "href": "Standardized-Mean-Differences.html#sec-ppc",
    "title": "7  Mean Differences",
    "section": "7.5 Pretest-Posttest-Control Group Designs",
    "text": "7.5 Pretest-Posttest-Control Group Designs\nIn many areas of research both between and within group factors are incorporated. For example, in research involving the examination of the effects of an intervention often a sample is randomised into two seperate groups (intervention and control) and then they are measured on the outcome of interest both before (pretest) and after (posttest) the intervention/control period. In these types of 2x2 (group x time) study designs it is usually the difference between the standardised mean change for the intervention/treatment (\\(T\\)) and control (\\(C\\)) groups that is of interest. For a visualization of a pretest-posttest-control group design see Figure 7.3.\nMorris (2008) details three effect sizes for this pretest-posttest-control (PPC).\n\n\n\n\n\nFigure 7.3: Illustration of a pre-post control design. Left panel shows the pre-post difference in the control group and right panel shows the pre-post difference in the intervention/treatment group. Lines indicate within person pre/post change.\n\n\n\n\n\n7.5.1 PPC1 - separate pre-test standard deviations\nThe separate pre-test (i.e., baseline) standard deviations are used to standardize the pre/post mean difference in the intervention group and the control group respectively (see equation 4, Morris 2008),\n\\[\nd_T = \\frac{M_{T,\\text{post}} - M_{T,\\text{pre}}}{S_{T,\\text{pre}}}\n\\]\n\\[\nd_C = \\frac{M_{C,\\text{post}} - M_{C,\\text{pre}}}{S_{C,\\text{pre}}}\n\\]\nNote that these effect sizes are identical to the Becker’s \\(d\\) formulation of the SMD (see Section 7.4.4). Therefore the pretest-posttest-control group effect size is simply the difference between the intervention and control pre/post SMD (equation 15, Becker 1988),\n\\[\nd_{PPC1} = d_T - d_C\n\\]\nThe asymptotic standard error of \\(d_{PPC2}\\) was first derived by Becker (1988) and can be expressed as the square root of the sum of the sampling variances (equation 16, Becker 1988)\n\\[\nSE_{d_{PPC1}} = \\sqrt{\\left[\\frac{2(1-r_T)}{n_T} + \\frac{d_T}{2n_T}\\right] + \\left[\\frac{2(1-r_C)}{n_C} + \\frac{d_C}{2n_C}\\right]}\n\\]\nWe can calculate \\(d_{PPC1}\\) and it’s confidence intervals using base R:\n\n# Example:\n\n# Control Group (N = 90)\n## Pre-test Mean = 20, SD = 6\n## Post-test Mean = 25, SD = 7\n## Pre/post correlation = .50\nM_Cpre &lt;- 20\nM_Cpost &lt;- 25\nSD_Cpre &lt;- 6\nSD_Cpost &lt;- 7\nrC &lt;- .50\nnC &lt;- 90\n\n# Intervention Group (N = 90)\n## Pre-test Mean = 20, SD = 5\n## Post-test Mean = 27, SD = 8\n## Pre/post correlation = .50\nM_Tpre &lt;- 20\nM_Tpost &lt;- 27\nSD_Tpre &lt;- 5\nSD_Tpost &lt;- 8\nrT &lt;- .50\nnT &lt;- 90\n\n# calculate the observed standardized mean difference\ndT &lt;- (M_Tpost- M_Tpre) / SD_Tpre\ndC &lt;- (M_Cpost - M_Cpre) / SD_Cpre\ndPPC1 &lt;- dT - dC\n\n# calculate the standard error\nSE &lt;- sqrt( 2*(1-rT)/nT + dPPC1^2/(2*nT) + 2*(1-rC)/nC + dPPC1^2/(2*nC) )\n\n# print the d value and confidence intervals\ndata.frame(d = MOTE::apa(dPPC1),\n           dlow = MOTE::apa(dPPC1 - 1.96*SE),\n           dhigh = MOTE::apa(dPPC1 + 1.96*SE))\n\n      d  dlow dhigh\n1 0.567 0.252 0.881\n\n\nThe output shows a pre-post intervention effect of \\(d_{PPC1}\\) = 0.57 [0.25, 0.88].\n\n\n7.5.2 PPC2 - pooled pre-test standard deviations\nThe pooled pre-test (i.e., baseline) standard deviations can be used to standardized the difference in pre/post change between intervention and control groups such that (equation 8, Morris 2008),\n\\[\nd_{PPC2} = \\frac{(M_{T,\\text{post}} - M_{T,\\text{pre}}) - (M_{C,\\text{post}} - M_{C,\\text{pre}})}{S_{p,\\text{pre}}}\n\\]\nwhere\n\\[\nS_{p,\\text{pre}} = \\sqrt{\\frac{(n_T-1)S^2_{T,\\text{pre}} + (n_C - 1)S^2_{C,\\text{post}}}{n_T + n_C - 2}}.\n\\]\nThe distribution of \\(d_{PPC2}\\) was described by Morris (2008) and can be expressed as (adapted from equation 16, Morris 2008),\n\\[\n\\small{SE_{d_{PPC2}} = \\sqrt{2\\left(1-\\frac{n_T r_T + n_C r_C}{n_T + n_C}\\right)\\left(\\frac{n_T + n_C}{n_T n_C}\\right)\\left[1 + \\frac{d^2_{PPC2}}{2\\left(1-\\frac{n_T r_T + n_C r_C}{n_T + n_C}\\right)\\left(\\frac{n_T + n_C}{n_T n_C}\\right)}\\right] - d^2_{PPC2}}}\n\\]\nNote the original equation shown in the paper by Morris (2008) uses the population pre/post correlation \\(\\rho\\), however in the equation above we replace \\(\\rho\\) with the sample size weighted average of the Pearson correlation computed in the treatment group and the control group (i.e., \\(\\rho \\approx \\frac{n_T r_T + n_C r_C}{n_T + n_C}\\)).\nWe can use base R to obtain \\(d_{PPC2}\\) and confidence intervals:\n\n# Example:\n\n# Control Group (N = 90)\n## Pre-test Mean = 20, SD = 6\n## Post-test Mean = 25, SD = 7\n## Pre/post correlation = .50\nM_Cpre &lt;- 20\nM_Cpost &lt;- 25\nSD_Cpre &lt;- 6\nSD_Cpost &lt;- 7\nrC &lt;- .50\nnC &lt;- 90\n\n# Intervention Group (N = 90)\n## Pre-test Mean = 20, SD = 5\n## Post-test Mean = 27, SD = 8\n## Pre/post correlation = .50\nM_Tpre &lt;- 20\nM_Tpost &lt;- 27\nSD_Tpre &lt;- 5\nSD_Tpost &lt;- 8\nrT &lt;- .50\nnT &lt;- 90\n\n# calculate the observed standardized mean difference\ndPPC2 &lt;- ((M_Tpost- M_Tpre) - (M_Cpost - M_Cpre)) / sqrt( ( (nT - 1)*(SD_Tpre^2) + (nC - 1)*(SD_Cpre^2) ) / (nT + nC - 2) )\n\n# calculate the standard error\nSE &lt;-  sqrt(2*(1-( (nT*rT+nC*rC)/(nT + nC))) * ((nT+nC)/(nT*nC)) * (1 + (dPPC2^2 / (2*(1 - ((nT*rT+nC*rC)/(nT+nC))) * ((nT+nC)/(nT*nC)))))) - dPPC2\n\n# print the d value and confidence intervals\ndata.frame(d = MOTE::apa(dPPC2),\n           dlow = MOTE::apa(dPPC2 - 1.96*SE),\n           dhigh = MOTE::apa(dPPC2 + 1.96*SE))\n\n      d  dlow dhigh\n1 0.362 0.304 0.420\n\n\nThe output shows a pre-post intervention effect of \\(d_{PPC2}\\) = 0.36 [0.30, 0.42].\n\n\n7.5.3 PPC3 - pooled pre- and post-test\nThe two previous effect sizes only use the pretest standard deviation. But if we are happy to assume that pretest and posttest variances are homogenous1 the pooled pre-test and post-test standard deviations can be used to standardized the difference in pre/post change between intervention and control groups such that (equation 8, Morris 2008),\n\\[\nd_{PPC3} = \\frac{(M_{T,\\text{post}} - M_{T,\\text{pre}}) - (M_{C,\\text{post}} - M_{C,\\text{pre}})}{S_{p,\\text{pre-post}}},\n\\]\nwhere,\n\\[\nS_{p,\\text{pre-post}} = \\sqrt{\\frac{(n_T-1)\\left(S^2_{T,\\text{pre}} + S^2_{T,\\text{post}}\\right) + (n_C - 1)\\left(S^2_{C,\\text{pre}} + S^2_{C,\\text{post}}\\right)}{n_T + n_C - 2}}.\n\\]\nThe standard error for \\(d_{PPC2}\\) is currently unknown. An option to estimate this standard error is to use a non-parametric or parametric bootstrap by repeatedly sampling the raw data, or if the raw data is not available resample simulated data. We can do this in base R by simulating pre/post data using the mvrnorm() function from the MASS package (Venables and Ripley 2002):\n\n# Install the package below if not done so already\n# install.packages(MASS)\n\n# Example:\n\n# Control Group (N = 90)\n## Pre-test Mean = 20, SD = 6\n## Post-test Mean = 25, SD = 7\n## Pre/post correlation = .50\nM_Cpre &lt;- 20\nM_Cpost &lt;- 25\nSD_Cpre &lt;- 6\nSD_Cpost &lt;- 7\nrC &lt;- .50\nnC &lt;- 90\n\n# Intervention Group (N = 90)\n## Pre-test Mean = 20, SD = 5\n## Post-test Mean = 27, SD = 8\n## Pre/post correlation = .50\nM_Tpre &lt;- 20\nM_Tpost &lt;- 27\nSD_Tpre &lt;- 5\nSD_Tpost &lt;- 8\nrT &lt;- .50\nnT &lt;- 90\n\n# simulate data\nset.seed(1) # set seed for reproducibility\nboot_dPPC3 &lt;- c()\nfor(i in 1:1000){\n  # simulate control group pre-post data\n  data_C &lt;- MASS::mvrnorm(n = nC,\n                          # input observed means\n                          mu = c(M_Cpre,M_Cpost),\n                          # input observed covariance matrix\n                          Sigma = data.frame(pre = c(SD_Cpre^2, rC*SD_Cpre*SD_Cpost), \n                                             post = c(rC*SD_Cpre*SD_Cpost,SD_Cpost^2)))\n  # simulate intervention group pre-post data\n  data_T &lt;- MASS::mvrnorm(n = nT,\n                          # input observed means\n                          mu = c(M_Tpre,M_Tpost),\n                          # input observed covariance matrix\n                          Sigma = data.frame(pre = c(SD_Tpre^2, rT*SD_Tpre*SD_Tpost), \n                                             post = c(rT*SD_Tpre*SD_Tpost,SD_Tpost^2)))\n  \n  # calculate the mean difference in pre/post change (the numerator)\n  MeanDiff &lt;- (mean(data_T[,2]) - mean(data_T[,1])) - (mean(data_C[,2]) - mean(data_C[,1]))\n  \n  # calculate the pooled pre-post standard deviation (the denominator)\n  S_Pprepost &lt;-  sqrt( ( (nT - 1)*(sd(data_T[,1])^2+sd(data_T[,2])^2) + (nC - 1)*(sd(data_C[,1])^2+sd(data_C[,2])^2) ) / (nT + nC - 2) )\n  \n  # calculate the standardized mean difference for each bootstrap iteration\n  boot_dPPC3[i] &lt;- MeanDiff / S_Pprepost\n}\n\n# calculate bootstrapped standard error\nSE &lt;- sd(boot_dPPC3)\n\n# calculate the observed standardized mean difference\ndPPC3 &lt;- ((M_Tpost- M_Tpre) - (M_Cpost - M_Cpre)) / sqrt( ( (nT - 1)*(SD_Tpre^2+SD_Tpost^2) + (nC - 1)*(SD_Cpre^2+SD_Cpost^2) ) / (nT + nC - 2) )\n\n#print the d value and confidence intervals\ndata.frame(d = MOTE::apa(dPPC3),\n           dlow = MOTE::apa(dPPC3 - 1.96*SE),\n           dhigh = MOTE::apa(dPPC3 + 1.96*SE))\n\n      d  dlow dhigh\n1 0.214 0.002 0.427\n\n\nThe output shows a pre-post intervention effect of \\(d_{PPC3}\\) = 0.21 [0.002, 0.43]."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#small-sample-bias-in-d-values",
    "href": "Standardized-Mean-Differences.html#small-sample-bias-in-d-values",
    "title": "7  Mean Differences",
    "section": "7.6 Small Sample Bias in \\(d\\) values",
    "text": "7.6 Small Sample Bias in \\(d\\) values\nAll the estimators of \\(d\\) listed above are biased estimates of the population \\(d\\) value, specifically they all over-estimate the population value in small sample sizes. To adjust for this bias, we can apply a correction factor based on the degrees of freedom. The degrees of freedom will largely depend on the estimator used. The degrees of freedom for each estimator is listed below:\n\nSingle Group design (\\(d_s\\)): \\(df = n-1\\)\nBetween Groups - Pooled Standard Deviation (\\(d_p\\)): \\(df = n_1+n_2-2\\)\nBetween Groups - Control Group Standard Deviation (\\(d_\\Delta\\)): \\(df = n_C-1\\)\nRepeated Measures - all types (\\(d_z\\), \\(d_{rm}\\), \\(d_{av}\\), \\(d_{b}\\)): \\(df = n-1\\)\nPretest-Posttest-Control Separate Standard Deviation (\\(d_{PPC1}\\)): \\(df=n_C−1\\)\nPretest-Posttest-Control Pooled Pretest Standard Deviation (\\(d_{PPC2}\\)): \\(df=n_T+n_C−2\\)\nPretest-Posttest-Control Pooled Pretest and Posttest Standard Deviation (\\(d_{PPC3}\\)): \\(df=2(n_T+n_C−2)\\)\n\nWith the appropriate degrees of freedom, we can use the following correction factor, \\(CF\\), to obtain an unbiased estimate of the population standardized mean difference:\n\\[\nCF = \\frac{\\Gamma\\left(\\frac{df}{2}\\right)}{\\Gamma\\left(\\frac{df-1}{2}\\right)\\sqrt{\\frac{df}{2}}}\n\\]\nWhere \\(\\Gamma(\\cdot)\\) is the gamma function. An approximation of this complex formula given by Hedges (1981) can be written as \\(CF\\approx 1-\\frac{3}{4\\cdot df -1}\\). In R, this can be calculated using,\n\n# Example:\n# Group 1 sample size = 20\n# Group 2 sample size = 18\n\nn1 &lt;- 20\nn2 &lt;- 18\n\ndf &lt;- n1 + n2 - 2\n\nCF &lt;- gamma(df/2) / ( sqrt(df/2) * gamma((df-1)/2) )\n\nCF\n\n[1] 0.9789964\n\n\nThis correction factor can then be applied to any of the estimators mentioned above,\n\\[\nd^* = d\\times CF\n\\]\nThe corrected \\(d\\) value, \\(d^*\\), is commonly referred to as Hedges’ \\(g\\) or just \\(g\\). To avoid notation confusion we will just add an asterisk to \\(d\\) to denote the correction. We also need to correct the standard error for \\(d^*\\)\n\\[\nSE_{d^*} = SE_{d} \\times CF\n\\]\nThese standard errors can then be used to calculate the confidence interval of the corrected \\(d\\) value,\n\\[\nCI_{d*} = d^* \\pm 1.96\\times SE_{d^*}\n\\]\n\n# Example:\n# Cohen's d = .50, SE = .10\n\nd = .50\nSE = .10\n\n# correct d value and CIs small sample bias\nd_corrected &lt;- d * CF\nSE_corrected &lt;- SE * CF\ndlow_corrected &lt;- d_corrected - 1.96*SE_corrected\ndhigh_corrected &lt;- d_corrected + 1.96*SE_corrected\n\n# print just the d value and confidence intervals\ndata.frame(d = apa(d), \n           dlow = apa(dlow_corrected), \n           dhigh = apa(dhigh_corrected))\n\n      d  dlow dhigh\n1 0.500 0.298 0.681\n\n\nThe output shows that the corrected effect size is \\(d^*\\) = 0.50, 95% CI [0.30, 0.68]."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#sec-rr",
    "href": "Standardized-Mean-Differences.html#sec-rr",
    "title": "7  Mean Differences",
    "section": "7.7 Ratios of Means",
    "text": "7.7 Ratios of Means\nAnother common approach, particularly within the fields of ecology and evolution, is to take the natural logarithm of the ratio between two means; the so-called Response Ratio (\\(lnRR\\)). This is sometimes more favorable as, due to its construction using the standard deviation in some form as a denominator, the various versions of standardized mean differences are impacted by the estimate of this parameter for which studies are often less powered compared to mean magnitudes (Yang et al. 2022). For the \\(lnRR\\) however the standard deviation only impacts its variance estimation and not the point estimate. A limitation of the lnRR however is that it is limited to data that are observed on a ratio scale (i.e., have an absolute zero and instances of it are related ordinally and additively meaning both means will be positive).\nAlthough strictly speaking the \\(lnRR\\) is not a difference in means in an additive sense as the above standardized mean difference effect sizes are, it can in one sense be considered to reflect the difference in means on the multiplicative scale. In fact, after calculation it is often transformed to reflect the percentage difference or change between means: \\(100\\times \\exp(lnRR)-1\\). However, this can introduce transformation induced bias because a non-linear transformation of a mean value is not generally equal to the mean of the transformed value. In the context of meta-analysis combining \\(lnRR\\) estimated across studies a correct factor can be applied: \\(100\\times \\exp(lnRR+0.5 S^2_\\text{total})-1\\), where \\(S^2_\\text{total}\\) is the variance of all \\(lnRR\\) values.\nSimilarly to the various standardized mean differences, there are varied calculations for the lnRR dependent upon the study design being used (see Senior, Viechtbauer, and Nakagawa 2020).\n\n7.7.1 lnRR for Independent Groups (\\(lnRR_\\text{ind}\\))\nThe lnRR can be calculated when groups are independent as follows,\n\\[\nlnRR_\\text{ind}=\\ln\\left(\\frac{M_T}{M_C}\\right)+CF\n\\]\nWhere \\(M_T\\) and \\(M_C\\) are the means for the treatment and control group respectively and \\(CF\\) is the small sample correction factor calculated as,\n\\[\nCF = \\frac{S^2_T}{2n_TM_T^2} - \\frac{S^2_C}{2n_CM_C^2}\n\\]\nThe standard error can be calculated as,\n\\[\nSE_{lnRR_\\text{ind}} = \\sqrt{ \\frac{S^2_T}{n_T M_T^2} + \\frac{S^2_C}{n_C M_C^2} +\\frac{S^4_T}{2n^2_T M_T^4} + \\frac{S^4_C}{2n^2_C M_C^4}}\n\\]\nUsing R we can easily calculate this effect size using the escalc() function in the metafor package (Viechtbauer 2010):\n\n# lnRR for two independent groups\n# given means and SDs\n\n# For example:\n# Group 1 Mean = 30.4, Standard deviation = 22.53, Sample size = 96\n# Group 2 Mean = 21.4, Standard deviation = 19.59, Sample size = 96\n\nlibrary(metafor)\n\n\n# prepare the data\nM1 &lt;- 30.4\nM2 &lt;- 21.4\nSD1 &lt;- 22.53\nSD2 &lt;- 19.59\nN1 = 96\nN2 = 96\n\n# calculate lnRRind and standard error\nlnRRind &lt;- escalc(measure = \"ROM\", \n               m1i = M1,\n               m2i = M2,\n               sd1i = SD1,\n               sd2i = SD2,\n               n1i = N1,\n               n2i = N2)\n\nlnRRind$SE &lt;- sqrt(lnRRind$vi)\n\n# calculate confidence interval\nlnRRind$CIlow &lt;- lnRRind$yi - 1.96*lnRRind$SE\nlnRRind$CIhigh &lt;-  lnRRind$yi + 1.96*lnRRind$SE\n\n# print the VR value and confidence intervals\ndata.frame(lnRRind = MOTE::apa(lnRRind$yi),\n           lnRRind_low = MOTE::apa(lnRRind$CIlow),\n           lnRRind_high = MOTE::apa(lnRRind$CIhigh))\n\n  lnRRind lnRRind_low lnRRind_high\n1   0.351       0.115        0.587\n\n\nThe example shwos a natural log response ratio of \\(lnRR_\\text{ind}\\) = 0.35 [0.12, 0.59].\n\n\n7.7.2 lnRR for dependent groups (\\(lnRR_\\text{dep}\\))\nThe lnRR can be calculated when groups are dependent (i.e., same subjects in both conditions), for example a pre-post comparison, as follows,\n\\[\nlnRR_\\text{dep} = \\ln\\left(\\frac{M_2}{M_1}\\right) + CF\n\\]\nWhere \\(CF\\) is the small sample correct factor calculated as,\n\\[\nCF = \\frac{S^2_2}{2nM^2_2} - \\frac{S^2_1}{2nM^2_1}\n\\]\nThe standard error can then be calculated as,\n\\[\n\\small{SE_{lnRR_\\text{dep}} = \\sqrt{ \\frac{S^2_1}{n M_1^2} + \\frac{S^2_2}{n M_2^2} + \\frac{S^4_1}{2n^2M^4_1} +  \\frac{S^4_2}{2n^2M^4_2} + \\frac{2rS_1 S_2}{n M_1 M_2} + \\frac{r^2S^2_1 S^2_2 (M_1^4 + M_2^4)}{2n^2 M_1^4 M_2^4}}}\n\\]\nUsing R we can easily calculate this effect size using the escalc() function from the metafor package as follows:\n\n# lnRR for two dependent groups\n# given means and SDs\n\n\n# For example:\n# Mean 1 = 30.4, Standard deviation 1 = 22.53\n# Mean 2 = 21.4, Standard deviation 2 = 19.59\n# Sample size = 96\n# Correlation = 0.4\n\nlibrary(metafor)\n\n\n# prepare the data\nM1 &lt;- 30.4\nM2 &lt;- 21.4\nSD1 &lt;- 22.53\nSD2 &lt;- 19.59\nN = 96\nR = 0.4\n\n\n# calculate lnRR and standard error\nlnRRdep &lt;- escalc(measure = \"ROMC\", \n               m1i = M1,\n               m2i = M2,\n               sd1i = SD1,\n               sd2i = SD2,\n               ni = N,\n               ri = R)\n\n# obtain standard error from sqrt of sampling variance\nlnRRdep$SE &lt;- sqrt(lnRRdep$vi)\n\n\n# calculate confidence interval\nlnRRdep$CIlow &lt;- lnRRdep$yi - 1.96*lnRRdep$SE\nlnRRdep$CIhigh &lt;-  lnRRdep$yi + 1.96*lnRRdep$SE\n\n\n\n\n# print the VR value and confidence intervals\ndata.frame(lnRRdep = MOTE::apa(lnRRdep$yi),\n           lnRRdep_low = MOTE::apa(lnRRdep$CIlow),\n           lnRRdep_high = MOTE::apa(lnRRdep$CIhigh))\n\n  lnRRdep lnRRdep_low lnRRdep_high\n1   0.351       0.167        0.535\n\n\nThe example shwos a natural log response ratio of \\(lnRR_\\text{dep}\\) = 0.35 [0.17, 0.54].\n\n\n\n\nAlgina, James, and H. J. Keselman. 2003. “Approximate Confidence Intervals for Effect Sizes.” Educational and Psychological Measurement 63 (4): 537–53. https://doi.org/10.1177/0013164403256358.\n\n\nBaayen, R Harald, Douglas J Davidson, and Douglas M Bates. 2008. “Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items.” Journal of Memory and Language 59 (4): 390–412.\n\n\nBarr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” Journal of Memory and Language 68 (3): 255–78.\n\n\nBecker, Betsy J. 1988. “Synthesizing Standardized Mean-Change Measures - UConn Library.” British Journal of Mathematical and Statistical Psychology 41 (2): 257278. https://doi.org/https://doi.org/10.1111/j.2044-8317.1988.tb00901.x.\n\n\nCaldwell, Aaron R. 2022. “Exploring Equivalence Testing with the Updated TOSTER r Package.” PsyArXiv. https://doi.org/10.31234/osf.io/ty8de.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nGlass, Gene V. 1981. “Meta-Analysis in Social Research.” (No Title). https://cir.nii.ac.jp/crid/1130000795088566912.\n\n\nGlass, Gene V., Barry McGaw, and Mary L. Smith. 1981. “Meta-Analysis in Social Research.” (No Title). https://cir.nii.ac.jp/crid/1130000795088566912.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\nLakens, Daniël. 2013. “Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and ANOVAs.” Frontiers in Psychology 4. https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863.\n\n\nMorris, Scott B. 2008. “Estimating Effect Sizes From Pretest-Posttest-Control Group Designs.” Organizational Research Methods 11 (2): 364–86. https://doi.org/10.1177/1094428106291059.\n\n\nSenior, Alistair M., Wolfgang Viechtbauer, and Shinichi Nakagawa. 2020. “Revisiting and Expanding the Meta-Analysis of Variation: The Log Coefficient of Variation Ratio.” Research Synthesis Methods 11 (4): 553–67. https://doi.org/10.1002/jrsm.1423.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in R with the metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\nYang, Yefeng, Helmut Hillebrand, Malgorzata Lagisz, Ian Cleasby, and Shinichi Nakagawa. 2022. “Low Statistical Power and Overestimated Anthropogenic Impacts, Exacerbated by Publication Bias, Dominate Field Studies in Global Change Biology.” Global Change Biology 28 (3): 969–89. https://doi.org/10.1111/gcb.15972."
  },
  {
    "objectID": "Standardized-Mean-Differences.html#footnotes",
    "href": "Standardized-Mean-Differences.html#footnotes",
    "title": "7  Mean Differences",
    "section": "",
    "text": "Note, this may not be the case especially where there is a mean-variance relationship and one (usually the intervention) group has a higher posttest mean score.↩︎"
  },
  {
    "objectID": "Correlations.html",
    "href": "Correlations.html",
    "title": "8  Correlation between Two Continuous Variables",
    "section": "",
    "text": "To quantify the relationship between two continuous variables, the most common method is to use a Pearson correlation coefficient (denoted with the letter \\(r\\)). The pearson correlation takes the covariance between a continuous independent (\\(X\\)) and dependent (\\(Y\\)) variable and standardizes it by the standard deviations of \\(X\\) and \\(Y\\),\n\\[\nr = \\frac{\\text{Cov}(X,Y)}{S_{X} S_{Y}}.\n\\]\nWe can visualize what a correlation between two variables looks like with scatter plots. Figure 8.1 shows scatter plots with differing levels of correlation.\n\n\n\n\n\nFigure 8.1: Simulated data from a bivariate normal distribution displaying 6 different correlations, r = 0, .20, .40, .60, .80, and 1.00.\n\n\n\n\nThe standard error of the Pearson correlation coefficient is,\n\\[\nSE_r = \\sqrt{\\frac{\\left(1-r^2\\right)^2}{n-1}}\n\\]\nUnlike Cohen’s \\(d\\) and other effect size measures, The correlation coefficient is bounded by -1 and positive 1, with positive 1 being a perfectly positive correlation, -1 being a perfectly negative correlation, and zero indicating no correlation between the two variables. The bounding has the consequence of making the confidence interval asymmetric around \\(r\\) (e.g., if the correlation is positive, the lower bound is farther away from \\(r\\) than the upper bound is). It is important to note that with a correlation of zero, the confidence interval is symmetric and approximately normal. Instead, to obtain the confidence intervals of \\(r\\), we first need to apply a Fisher’s Z transformation. A Fisher’s Z transformation is a hyperbolic arctangent transformation of a Pearson correlation coefficient and can be computed as,\n\\[\nZ_r = \\text{arctanh}(r)\n\\]\nThe Fisher Z transformation ensures \\(Z_r\\) has a symmetric and approximately normal sampling distribution. This then allows us to calculate the confidence interval from the standard error of \\(Z_r\\) (\\(SE_{Z_r} = \\frac{1}{\\sqrt{n-3}}\\)). We can also back-transform the confidence into a Pearson correlation scale,\n\\[\nCI_{r} = \\text{tanh}(Z_r \\pm 1.96\\times SE_{Z_r})\n\\]\nWe can then back-transform the upper bound and lower bound into the upper and lower bound of \\(r\\) by taking the hyperbolic tangent (the inverse of the arctangent).\nIn R, the full process of obtaining confidence intervals can be done quite easily. Note if you have raw data for \\(X\\) and \\(Y\\), then you can compute the correlation with base R, cor(X,Y).\n\n# example: r = .50, n = 50\nr &lt;- .50\nn &lt;- 50\n\n# compute Zr\nZr &lt;- atanh(r)\n\n# calculate standard error of Zr\nSE_Zr &lt;- 1/sqrt(n-3)\n\n# compute confidence interval of Zr\nZlow &lt;- Zr - 1.96 * SE_Zr\nZhigh &lt;- Zr + 1.96 * SE_Zr\n\n# backtransform CI of Z to CI of Pearson correlation\nrlow &lt;- tanh(Zlow) \nrhigh &lt;- tanh(Zhigh)\n\n# print pearson correlation and confidence intervals\ndata.frame(r = MOTE::apa(r), \n           rlow = MOTE::apa(rlow), \n           rhigh = MOTE::apa(rhigh))\n\n      r  rlow rhigh\n1 0.500 0.257 0.683\n\n\nThe output shows that the correlation and its confidence intervals are \\(r\\) = 0.50, 95% CI [0.26, 0.68]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-phi",
    "href": "Categorical-Proportional-Data.html#sec-phi",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.1 Phi Coefficient (\\(\\phi\\))",
    "text": "9.1 Phi Coefficient (\\(\\phi\\))\nPhi coefficient (\\(\\phi\\)) is a measure of association between two binary variables (therefore, it ONLY applies to 2 by 2 contingency tables, i.e., each variable has only two levels). It is a special case of the Pearson correlation coefficient and an \\(r\\) for two binary variables is equal to phi. Note that unlike \\(r\\) that ranges from -1 to 1, phi ranges from 0 to 1. Also, the sign of \\(r\\) indicates the direction of association, whereas to get the direction of an association given a 2 by 2 contingency table, we need to look at the table itself; phi only provides a measure of strength. The 2 by 2 contingency table is illustrated by Table 9.1.\n\n\nTable 9.1: Contingency table between two binary variables\n\n\n\n\\(X=0\\)\n\\(X=1\\)\n\n\n\n\n\\(Y=0\\)\n\\(n_{00}\\)\n\\(n_{10}\\)\n\n\n\\(Y=1\\)\n\\(n_{01}\\)\n\\(n_{11}\\)\n\n\n\n\nThe sample sizes within each cell provide us with the necessary information to estimate the relationship between the two variables. A large phi coefficient would be expected to have relatively large sample sizes in the diagonal cells (\\(n_{00}\\) and \\(n_{11}\\)) and relatively low sample sizes in the off-diagonal cells (\\(n_{01}\\) and \\(n_{10}\\)). To calculate phi, it can be calculated from the cells of the contingency table directly (adapted from equation 1, Guilford 1965),\n\\[\n\\phi = \\frac{n_{11}n_{00} -n_{10}n_{01}}{\\sqrt{(n_{00} + n_{01})(n_{10} + n_{11})(n_{00} + n_{10})(n_{01} + n_{11})}}\n\\]\nor more conveniently, from the \\(\\chi^2\\)-statistic (equation 7.2.5, Cohen 1988),\n\\[\n\\phi = \\sqrt{\\frac{\\chi^2}{n}}\n\\]\nWhere \\(n\\) is the total sample size (i.e., the sum of all the cells). Using the psych package in R, we can calculate the the phi coefficient using the phi function directly from the contingency table\n\n# Example contingency table:\n#  40  17\n#  11  45\n\nlibrary(effectsize)\n\ncontingency_table &lt;- matrix(c(40, 11,\n                              17, 45),ncol = 2)\n\nphi_coefficient &lt;- phi(contingency_table, alternative = \"two.sided\")\n\nphi_coefficient\n\nPhi (adj.) |       95% CI\n-------------------------\n0.50       | [0.31, 0.69]\n\n\nIn our example we obtained a phi coefficient of \\(\\phi\\) = .50 [0.31, 0.69]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-v",
    "href": "Categorical-Proportional-Data.html#sec-v",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.2 Cramer’s \\(V\\)",
    "text": "9.2 Cramer’s \\(V\\)\nCramer’s V, sometimes also referred to as Cramer’s phi (\\(\\phi\\)), is a generalized effect size measure of the association between two nominal variables. It applies to contingency tables of any size (\\(2\\times 2\\), \\(3\\times 3\\), \\(3\\times 4\\), \\(5\\times 3\\), etc.). Cramer’s \\(V\\) on a \\(2\\times 2\\) contingency table is equivalent to the phi coefficient. For an illustration of a higher order contingency table, Table 9.2 represents a \\(3\\times 4\\) contingency table of two variables.\n\n\nTable 9.2: Contingency table between two categorical variables\n\n\n\n\\(X=0\\)\n\\(X=1\\)\n\\(X=2\\)\n\\(X=3\\)\n\n\n\n\n\\(Y=0\\)\n\\(n_{00}\\)\n\\(n_{10}\\)\n\\(n_{21}\\)\n\\(n_{31}\\)\n\n\n\\(Y=1\\)\n\\(n_{01}\\)\n\\(n_{11}\\)\n\\(n_{21}\\)\n\\(n_{31}\\)\n\n\n\\(Y=2\\)\n\\(n_{02}\\)\n\\(n_{12}\\)\n\\(n_{22}\\)\n\\(n_{32}\\)\n\n\n\n\nSimilarly to the phi coefficient, the value of Cramer’s \\(V\\) ranges from 0 to 1 and can interpreted in a similar way to a phi coefficient. Again we can use the \\(\\chi^2\\) statistic to compute the value, however, since there can be more than 2 levels to each variable, we also need to take into account the number of levels, \\(k\\), of the variable with the least number of levels (e.g., a \\(3 \\times 4\\) contingency table, \\(k\\) would be equal to 3). Cramer’s \\(V\\) is defined as (equation 7.2.6, Cohen 1988),\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n(k-1)}}\n\\]\nThe standard error of a Cramer’s \\(V\\) is similar to that of a Pearson correlation and a \\(\\phi\\) coefficient.\n\\[\nSE_V = \\sqrt{\\frac{\\left(1-V^2\\right)^2}{n-1}}\n\\]\nWhere \\(n\\) is the total sample size (i.e., the sum of all cells). Like the pearson correlation, we can not calculate the confidence interval directly from the standard error, instead, we must convert \\(V\\) to a Fisher’s Z statistic, \\(Z_V = \\text{arctanh}(V)\\). We can then calculate the 95% confidence interval for \\(V\\) by back-transforming the confidence interval for \\(Z_V\\):\n\\[\nSE_{Z_V} = \\frac{1}{\\sqrt{n-3}}\n\\]\n\\[\nCI_{V} = \\tanh(Z_V \\pm 1.96\\times SE_{Z_V})\n\\]\nUsing the ufs package (Peters and Gruijters 2023), we can calculate Cramer’s \\(V\\) and it’s 95% confidence interval using the Fisher’s Z method described above. For the example, we can example data from a 3 \\(\\times\\) 3 contingency table.\n\n# Example contingency table:\n#  40  14  12\n#  11  27   9\n#   5  10  34\n\nlibrary(ufs)\n\ncontingency_table &lt;- matrix(c(40, 11,  5,\n                              14, 27, 10,\n                              12,  9, 34),ncol = 3)\n\n\nV &lt;- cramersV(contingency_table)\nCI &lt;- confIntV(contingency_table)\n\n# print pearson correlation and confidence intervals\ndata.frame(V = MOTE::apa(V$output$cramersV), \n           Vlow = MOTE::apa(CI$output$confIntV.fisher[1]), \n           Vhigh = MOTE::apa(CI$output$confIntV.fisher[2]))\n\n      V  Vlow Vhigh\n1 0.442 0.309 0.558\n\n\nIn our example we obtained a Cramer’s \\(V\\) of \\(V\\) = .44 [.31, .56]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-h",
    "href": "Categorical-Proportional-Data.html#sec-h",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.3 Cohen’s \\(h\\)",
    "text": "9.3 Cohen’s \\(h\\)\nCohen’s \\(h\\) is a measure of distance between two proportions or probabilities. It is sometimes also referred to as the “difference between arcsines”. For a given proportion \\(p\\), its arcsine transformation is given by (equation 6.2.1, Cohen 1988):\n\\[\n\\psi = 2\\cdot \\text{arcsin}(\\sqrt{p}).\n\\]\nCohen’s \\(h\\) is the difference between the arcsine transformations of two proportions (equation 6.2.2, Cohen 1988):\n\\[\nh = \\psi_1 - \\psi_2\n\\]\nCohen’s \\(h\\) is commonly used for the power analysis of proportion tests. We can calculate the standard error in Cohen’s \\(h\\) It is the required effect size measure in the program G Power (Faul et al. 2009).\n\\[\nSE_h = \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nSince the sampling distribution of \\(h\\) is symmetric, we can calculate the confidence intervals from the standard error,\n\\[\nCI_h = h \\pm1.96\\times SE_h\n\\]\nTo calculate Cohen’s \\(h\\), we can use the cohens_h function in the effectsize package in R.\n\n# install package if not done so already\n# install.packages('effectsize')\n# Example proportions: p1 = .45, p2 = .30\n\nlibrary(effectsize)\n\ncontingency_table &lt;- matrix(c(40, 11,\n                              14, 27),ncol = 2)\n\ncohens_h(contingency_table)\n\nCohen's h |       95% CI\n------------------------\n0.93      | [0.52, 1.34]\n\n\nFrom the example, the R code outputted a Cohen’s \\(h\\) value of \\(h\\) = .93 [0.52, 1.34]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-w",
    "href": "Categorical-Proportional-Data.html#sec-w",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.4 Cohen’s \\(w\\)",
    "text": "9.4 Cohen’s \\(w\\)\nCohen’s \\(w\\) is a measurem of association analogous to the phi coefficient but on tables that are larger than 2x2. Although Cohen’s \\(w\\) is useful for power analyses, it is not so useful as a stand-alone effect size. As Cohen (1988) states (pp. 221):\n\nAs a measure of association, [Cohen’s \\(w\\)] lacks familiarity and convenience\n\nCohen’s \\(w\\) has the exact same formula as the phi coefficient with the only difference being that the \\(\\chi^2\\) statistic comes from a contingency table of any size (equation 7.2.5, Cohen 1988),\n\\[\nw = \\sqrt{\\frac{\\chi^2}{n}}\n\\]\nAnd can also be calculated directly from Cramer’s \\(V\\) (equation 7.2.7, Cohen 1988),\n\\[\nw = V \\times \\sqrt{k-1}\n\\]\nWhere \\(k\\) is the number of categories in the variable with the least number of categories. We can use the cohens_w() function in the effectsize package (Ben-Shachar, Lüdecke, and Makowski 2020).\n\n# Example contingency table\n# 40 14\n# 11 27\n\ncontingency_table &lt;- matrix(c(40, 11, \n                              14, 27),ncol = 2)\n\ncohens_w(contingency_table,\n         alternative = \"two.sided\")\n\nCohen's w |       95% CI\n------------------------\n0.45      | [0.24, 0.65]\n\n\nFrom the example code, the cohens_w function returned Cohen’s \\(w\\) value of \\(w\\) = .45 [0.24, 0.65]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-fei",
    "href": "Categorical-Proportional-Data.html#sec-fei",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.5 Ben-Shachar’s Fei (פ)",
    "text": "9.5 Ben-Shachar’s Fei (פ)\nBen-Shachar et al. (2023) introduced a new effect size for one-dimensional tables of counts/proportions that they label with the Hebrew letter, פ. Ben-Shachar’s פ is a correction to Cohen’s \\(w\\) that adjusts for the expected value and consequently bounds the value between 0 and 1. The equation for פ is defined as,\n\\[\n\\mathbf{פ }= \\sqrt{\\frac{\\chi^2}{n \\left(\\frac{1}{\\min\\left(P_E\\right)} -1\\right)}}\n\\]\nWhere \\(\\min(P_E)\\) is the smallest expected probability. The formula for Ben-Schachar’s פ can be also be expressed in terms of Cohen’s \\(\\omega\\),\n\\[\n\\mathbf{פ }= \\frac{\\omega}{\\sqrt{\\left(\\frac{1}{\\max(P_E)} -1\\right)}}\n\\]\nIn R, we can calculate Ben-Shachar’s פ using the fei() function in the effectsize package (Ben-Shachar, Lüdecke, and Makowski 2020).\n\n# Example:\n# Observed counts: 20, 50, 100 (observed proportions: .12, .29, .59)\n# Expected proportions: .5, .2, .3\n\nobserved_counts &lt;- c(20,50,100)\nexpected_probabilities &lt;- c(.5,.2,.3)\n\n\nfei(observed_counts,\n    p = expected_probabilities,\n    alternative = \"two.sided\")\n\nFei  |       95% CI\n-------------------\n0.39 | [0.31, 0.47]\n\n- Adjusted for uniform expected probabilities.\n\n\nFrom the example code, the fei function returned Ben-Shachar’s פ value of .39 [0.31, 0.47]."
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-or",
    "href": "Categorical-Proportional-Data.html#sec-or",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.6 Odds Ratio (\\(OR\\))",
    "text": "9.6 Odds Ratio (\\(OR\\))\nOdds ratio measures the effect size between two binary variables. It is commonly used in medical and behavioral intervention research, and notably, in meta-analysis.\nLet’s imagine a study conducted to investigate the association between smoking and the development of major depressive disorder (MDD). The study includes a sample of 251 individuals, categorizing them into two groups: 125 smokers and 126 non-smokers. The researchers are interested in understanding the odds of having major depressive disorder (MDD) among smokers compared to non-smokers. Say we find that 25 smokers were diagnosed with MDD while 100 were not, but in the non-smoker group, 12 individuals were diagnosed with MDD while 120 were not. The odds ratio would then be:\n\\[\nOR = \\frac{25/100}{12/120}= \\frac{.25}{.10} = 2.50\n\\]\nIn general, we can can compute the odds-ratio from a contingency table between binary variables \\(X\\) (i.e., the treatment) and \\(Y\\) (i.e., the outcome; see Table 9.3).\n\n\nTable 9.3: Contingency table between two binary variables\n\n\n\n\\(X=T\\)\n\\(X=C\\)\n\n\n\n\n\\(Y=0\\)\n\\(n_{T0}\\)\n\\(n_{C0}\\)\n\n\n\\(Y=1\\)\n\\(n_{T1}\\)\n\\(n_{C1}\\)\n\n\n\n\nUltimately, we want to compare the outcome between the treatment group (\\(X=T\\)) and the control group (\\(X=C\\)). Therefore we can compute the odds ratio as,\n\\[\nOR = \\frac{n_{T1}/n_{T0}}{n_{C1}/n_{C0}}\n\\]\nThe standard distribution of the odds-ratio is asymmetric. To calculate confidence intervals, we can first convert the odds ratio to a log odds ratio (\\(LOR= \\log(OR)\\)). Then we can calculate the standard error of the log odds ratio,\n\\[\nSE_{LOR} = \\sqrt{\\frac{1}{n_{T0}} + \\frac{1}{n_{T1}} + \\frac{1}{n_{C0}} + \\frac{1}{n_{C1}}}\n\\]\nWith the standard error of the log odds ratio we can then calculate the confidence interval of the odds ratio by back-transforming using the exponential function,\n\\[\nCI_{OR} = \\exp(LOR \\pm 1.96\\times SE_{LOR})\n\\]\nIn R, we can use the effectsize package to calculate the odds ratio and it’s confidence interval:\n\n# Example:\n# Treatment Group: 10 diseased, 43 healthy\n# Control Group:  24 diseased, 41 healthy\n\n\ncontingency_table &lt;- matrix(c(10, 24,\n                              43, 41),ncol = 2)\n\noddsratio(contingency_table,\n          alternative = \"two.sided\")\n\nOdds ratio |       95% CI\n-------------------------\n0.40       | [0.17, 0.93]\n\n\nThe code output for this example shows an odds ratio of \\(OR\\) = 0.40 [0.17, 0.93]"
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-rd",
    "href": "Categorical-Proportional-Data.html#sec-rd",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.7 Risk Difference (\\(RD\\))",
    "text": "9.7 Risk Difference (\\(RD\\))\nRisk difference can be used to interpret the difference between two proportions. If we use the contingency table from Table 9.3, and calculate a risk difference between the treatment group and the control group. We can first calculate the proportion of cases where the outcome is \\(Y=1\\) within the control group and the treatment group:\n\\[\np_C=\\frac{n_{C1}}{n_{C0}+n_{C1}}\n\\]\n\\[\np_T=\\frac{n_{T1}}{n_{T0}+n_{T1}}\n\\]\nThen using these proportions we can calculate the risk difference (\\(RD\\)),\n\\[\nRD = p_T - p_C.\n\\]\nThe corresponding standard error is,\n\\[\nSE_{RD} = \\sqrt{\\frac{p_C(1-p_C)}{n_C} + \\frac{p_T(1-p_T)}{n_T} }\n\\]\nWhere \\(n_C\\) and \\(n_T\\) are the total sample sizes within the control and treatment group, respectively. The standard error can then be used to compute the 95% confidence intervals,\n\\[\nCI_{RD} = RD \\pm 1.96 \\times SE_{RD}\n\\]\nThe risk difference formula is fairly simple, so we can compute it using base R.\n\n# Example: \n# Treatment group: proportion of cases = .5, sample size = 40\n# Control group: proportion of cases = .3, sample size = 45\n\npT &lt;- .50\npC &lt;- .30\nnT &lt;- 40\nnC &lt;- 45\n\nRD &lt;- pT - pC\n\nSE &lt;- sqrt( pC*(1-pC)/nC + pT*(1-pT)/nT )\n\n# compute 95% CIs\nRDlow &lt;- RD - 1.96*SE\nRDhigh &lt;- RD + 1.96*SE\n\ndata.frame(\n  RD = MOTE::apa(RD),\n  RDlow = MOTE::apa(RDlow),\n  RDhigh = MOTE::apa(RDhigh)\n  )\n\n     RD  RDlow RDhigh\n1 0.200 -0.005  0.405"
  },
  {
    "objectID": "Categorical-Proportional-Data.html#sec-rr",
    "href": "Categorical-Proportional-Data.html#sec-rr",
    "title": "9  Effect Sizes for Categorical Variables",
    "section": "9.8 Relative Risk (\\(RR\\))",
    "text": "9.8 Relative Risk (\\(RR\\))\nThe relative risk, often referred to as the “risk ratio,” calculates the ratio between the proportion of cases in the treatment group and the proportion of cases in the control group. It provides a straightforward interpretation: “individuals receiving the treatment have a \\(RR\\) times higher odds of experiencing the outcome compared to controls.” To calculate relative riskm, first we need to calculate the proportion of outcome cases in the treatment and control group\n\\[\np_C=\\frac{n_{C1}}{n_{C0}+n_{C1}}\n\\]\n\\[\np_T=\\frac{n_{T1}}{n_{T0}+n_{T1}}\n\\]\nThen we can calculate the relative risk,\n\\[\nRR=\\frac{p_T}{p_C}\n\\]\nThe corresponding standard error can be computed as,\n\\[\nSE_{RR} = \\sqrt{\\frac{p_T}{n_T} + \\frac{p_C}{n_C}}\n\\]\nThe confidence intervals can be computed from the standard error,\n\\[\nCI_{RR} = RR\\pm 1.96\\times SE_{RR}\n\\]\nTo compute relative risk, we can simply use the equations above in base R.\n\n# Example:\n# Treatment Group: 10 diseased, 43 healthy, 53 total\n# Control Group:  24 diseased, 41 healthy, 65 total\n\npT &lt;- 10/(43+10)\npC &lt;- 24/(41+24)\nnT &lt;- 53\nnC &lt;- 65\n\nRR &lt;- pT / pC\n\nSE &lt;- sqrt(pT/nT + pC/nC)\n\nRRlow &lt;- RR - 1.96*SE\nRRhigh &lt;- RR + 1.96*SE\n\n# print pearson correlation and confidence intervals\ndata.frame(RR = MOTE::apa(RR), \n           RRlow = MOTE::apa(RRlow), \n           RRhigh = MOTE::apa(RRhigh))\n\n     RR RRlow RRhigh\n1 0.511 0.323  0.699\n\n\n\n\n\n\nBen-Shachar, Mattan S., Daniel Lüdecke, and Dominique Makowski. 2020. “effectsize: Estimation of Effect Size Indices and Standardized Parameters.” Journal of Open Source Software 5 (56): 2815. https://doi.org/10.21105/joss.02815.\n\n\nBen-Shachar, Mattan S., Indrajeet Patil, Rémi Thériault, Brenton M. Wiernik, and Daniel Lüdecke. 2023. “Phi, Fei, Fo, Fum: Effect Sizes for Categorical Data That Use the Chi-Squared Statistic.” Mathematics 11 (9): 1982. https://doi.org/10.3390/math11091982.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nFaul, Franz, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. “Statistical Power Analyses Using G*Power 3.1: Tests for Correlation and Regression Analyses.” Behavior Research Methods 41 (4): 1149–60. https://doi.org/10.3758/BRM.41.4.1149.\n\n\nGuilford, J. P. 1965. “The Minimal Phi Coefficient and the Maximal Phi.” Educational and Psychological Measurement 25 (1): 3–8. https://doi.org/10.1177/001316446502500101.\n\n\nPeters, Gjalt-Jorn Ygram, and Stefan Gruijters. 2023. Ufs: A Collection of Utilities. https://ufs.opens.science."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#anovas",
    "href": "Effect-Sizes-for-ANOVAs.html#anovas",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.1 ANOVAs",
    "text": "10.1 ANOVAs\nFor ANOVAs/F-tests, you will always need to report two kinds of effects: the omnibus effect of the factor(s) and the effect of planned contrasts or post hoc comparisons.\nFor instance, imagine that you are comparing three groups/conditions with a one-way ANOVA. The ANOVA will first return an F-statistic, the degrees of freedom, and the associated p-value. Here, you need to calculate the size of this omnibus factor effect in eta-squared, partial eta-squared, or generalized eta-squared.\nSuppose the omnibus effect is significant. You now know that there is at least one group that differs from the others. You want to know which group(s) differ from the others, and how much they differ. Therefore, you conduct post hoc comparisons on these groups. Because post hoc comparisons compare each group with the others in pairs, you will get a t-statistic and p-value for each comparison. For this, you can calculate and report a standardized mean difference.\nImagine that you have two independent variables or factors, and you conduct a two-by-two factorial ANOVA. The first thing to do then is look at the interaction. If the interaction is significant, you again report the associated omnibus effect size measures, and proceed to analyze the simple effects. Depending on your research question, you compare the levels of one IV on each level of the other IV. You will report d or g for these simple effects. If the interaction is not significant, you look at the main effects and report the associated omnibus effect. You then proceed to analyze the main effect by comparing the levels of one IV while collapsing/aggregating the levels of the other IV. You will report d or g for these pairwise comparisons. Note that lower-order effects are not directly interpretable if higher-order effects are significant. If you have a significant interaction in a two-way ANOVA, you cannot interpret the main effects directly. If you have a significant three-way interaction in a three-way ANOVA, you cannot interpret the main effects or the two-way interactions directly, regardless of whether they are significant or not."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#sec-aov-table",
    "href": "Effect-Sizes-for-ANOVAs.html#sec-aov-table",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.2 ANOVA tables",
    "text": "10.2 ANOVA tables\nAn ANOVA table generally consists of the grouping factors (+ residuals), the sum of squares, the degrees of freedom, the mean square, the F-statistic, and the p-value. Using base R, we can construct an ANOVA table using the aov() function to generate the ANOVA model and then using summary.aov() to extract the table. For an example case, we will use the palmerpenguins data set package and we will investigate the differences in the body mass (the outcome) of three penguin species (the predictor/grouping variable):\n\nlibrary(palmerpenguins)\n\n# construct anova model \n# formula structure: outcome ~ grouping variable\nANOVA_mdl &lt;- aov(body_mass_g ~ species, \n                 data = penguins) # dataset\n\nANOVA_table &lt;- summary.aov(ANOVA_mdl)\nANOVA_table\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies       2 146864214 73432107   343.6 &lt;2e-16 ***\nResiduals   339  72443483   213698                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\n\nBy default, summary.aov() does not report the \\(\\eta^2\\) value, however we will discuss this more in Section 10.7.1. The results show that the mean body mass between the three penguin species (Adelie, Gentoo, Chinstrap) differ significantly from one another."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#one-way-between-subjects-anova",
    "href": "Effect-Sizes-for-ANOVAs.html#one-way-between-subjects-anova",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.3 One-way between-subjects ANOVA",
    "text": "10.3 One-way between-subjects ANOVA\nOne-way between-subject ANOVA is an extension of independent-samples t-tests. The null hypothesis is that all k means of k independent groups are identical, whereas the alternative hypothesis is that there are at least two means from these k groups differ. The assumptions include: (1) independence of observations, (2) normality of residuals, and (3) equality (or homogeneity) of variances (homoscedasticity).1\nNote. Sometimes you may encounter a between-subject one-way ANOVA which compares only two conditions, particularly when the paper is old. This is essentially a t-test, and the F-statistic is just t-squared. It is preferable to report Cohen’s d for these tests. If you are calculating the effect size for such tests, it’s best to calculate Cohen’s d, or convert the provided eta-squared to Cohen’s d, as Cohen’s d can show the direction of the effect. Subsequent analyses (e.g., power analysis) can also be based on Cohen’s d.\nIt’s very easy to determine eta-squared with an F-statistic and the two degrees of freedom from a one-way ANOVA 2. Note that in the case of a one-way between-subject ANOVA, eta-squared is equal to partial eta-squared.\n\n10.3.1 Determining degrees of freedom\nPlease refer to the following table to determine the degrees of freedom for ANOVA effects, if they are not reported or if you are doubtful that they have been misreported.\n\n\n\nDegrees of freedom\n\n\n\n\n\nBetween subjects ANOVA\n\n\n\nEffect\n\\(k-1\\)\n\n\nError\n\\(n-k\\)\n\n\nTotal\n\\(n-1\\)\n\n\n\n\n\n10.3.2 Calculating eta-squared from F-statistic and degrees of freedom\nUsing the formula below, we can calculate \\(\\eta^2\\) of an ANOVA model using the F-statistic and the degrees of freedom,\n\\[\n\\eta^2 = \\frac{df_\\text{effect}\\times F}{df_\\text{effect} \\times F + df_\\text{error}}.\n\\]\nIn R, we can use the F_to_eta2() function from the effectsize package (Ben-Shachar, Lüdecke, and Makowski 2020):\n\nlibrary(effectsize)\n\nn = 154 # number of subjects\nk = 3 # number of groups\nf = 84.3 # F-statistic\n\ndf_effect = k - 1\ndf_error = n - k\n\nF_to_eta2(f = f,\n          df = df_effect,\n          df_error = df_error,\n          alternative = 'two.sided') # obtain two sided CIs\n\nEta2 (partial) |       95% CI\n-----------------------------\n0.53           | [0.42, 0.61]\n\n\n\n\n10.3.3 Calculating eta-squared from an ANOVA table\nLet’s use the table from the ANOVA model in Section 10.2:\n\n\nOne-way ANOVA table\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nspecies\n2\n146864214\n73432107.1\n343.6263\n0\n\n\nResiduals\n339\n72443483\n213697.6\nNA\nNA\n\n\n\n\nFrom this table we can use the sum of squares from the grouping variable (species) and the total sum of squares (\\(SS_\\text{total} = SS_\\text{effect} + SS_\\text{error}\\)) to calculate the \\(\\eta^2\\) value using the following equation:\n\\[\n\\eta^2 = \\frac{SS_\\text{effect}}{SS_\\text{total}} = \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error}}\n\\]\nIn R, we can use the eta.full.SS() function in the MOTE package (Buchanan et al. 2019) to obtain \\(\\eta^2\\) from an ANOVA table.\n\nlibrary(MOTE)\n\neta &lt;- eta.full.SS(dfm = 2,  # effect degrees of freedom\n                   dfe = 339, # error degrees of freedom\n                   ssm = 146864214, # sum of squares for the effect\n                   sst = 146864214 + 72443483, # total sum of squares\n                   Fvalue = 343.6263,\n                   a = .05)\n\ndata.frame(eta_squared = apa(eta$eta),\n           etalow = apa(eta$etalow),\n           etahigh = apa(eta$etahigh))\n\n  eta_squared etalow etahigh\n1       0.670  0.606   0.722\n\n\nThe example code outputs \\(\\eta^2\\) = .67 [.61, .72]. This suggests that species accounts for 67% of the total variation in body mass between penguins.\n\n\n10.3.4 Calculating Cohen’s d for post-hoc comparisons\nIn an omnibus ANOVA, the p-value is telling us whether the means from all groups come from the same population mean, however this does not inform us about which groups differ and by how much. Using the same example as before, let’s say we want to answer a specific question such as: what is the difference in body mass between Adelie penguins and Gentoo penguins? To answer this question, we can calculate the raw mean difference between the two groups. In R, we can do that with the following code:\n\nMadelie &lt;- mean(penguins$body_mass_g[penguins$species=='Adelie'], na.rm=T)\nMgentoo &lt;- mean(penguins$body_mass_g[penguins$species=='Gentoo'], na.rm=T)\n\nMgentoo - Madelie\n\n[1] 1375.354\n\n\nBased on the mean difference, Gentoo penguins are on average 1375 grams heavier than Adelia penguins in total body mass. We can also calculate a standardized mean difference using the escalc() function in the metafor package (Viechtbauer 2010).\n\nlibrary(metafor)\n\n# Means, SDs, and sample sizes for each group\nMadelie &lt;- mean(penguins$body_mass_g[penguins$species=='Adelie'], na.rm=T)\nMgentoo &lt;- mean(penguins$body_mass_g[penguins$species=='Gentoo'], na.rm=T)\nSDadelie &lt;- sd(penguins$body_mass_g[penguins$species=='Adelie'], na.rm=T)\nSDgentoo &lt;- sd(penguins$body_mass_g[penguins$species=='Gentoo'], na.rm=T)\nNadelie &lt;- sum(penguins$species=='Adelie', na.rm=T)\nNgentoo &lt;- sum(penguins$species=='Gentoo', na.rm=T)\n\nsummary(\n  escalc(measure = 'SMD',\n         m1i = Mgentoo,\n         m2i = Madelie,\n         sd1i = SDgentoo,\n         sd2i = SDadelie,\n         n1i = Ngentoo,\n         n2i = Nadelie)\n)\n\n\n      yi     vi    sei      zi   pval  ci.lb  ci.ub \n1 2.8602 0.0295 0.1716 16.6629 &lt;.0001 2.5237 3.1966 \n\n\nThe standardized mean difference between Adelie and Gentoo penguins is \\(d\\) = 2.86 [2.52, 3.19], demonstrating that Gentoo penguins have body mass 2.86 standard deviations larger than Adelie penguins.\nWe can also quantify contrasts from summary statistics reported from the ANOVA table and the within group means. We can calculate the standardized mean difference using the means from both groups and the mean squared error (\\(MSE\\)) the following equation:\n\\[\nd = \\frac{M_1 - M_2}{\\sqrt{MSE}}\n\\]\nThis method gives a standardized mean difference equivalent to the Cohen’s \\(d\\) with the pooled standard deviation in the denominator (see chapter on mean differences). Therefore if we obtain the mean squared errors (i.e., MS of residuals) from Section 10.3.3 and we obtain the means (means: Gentoo = 5076, Adelie = 3701), we can calculate the standardized mean difference as: \\(\\frac{5076 - 3701}{\\sqrt{213697.6}} = \\frac{1375}{462.27 } = 2.974\\). The discrepency between the standardized mean difference provided by the escalc() function is due to the fact that the function automatically applies a small sample correction factor thus reducing the overall effect.\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nspecies\n2\n146864214\n73432107.1\n343.6263\n0\n\n\nResiduals\n339\n72443483\n213697.6\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nBeware the assumptions.\n\n\n\nNote that this method is ONLY valid when you are willing to assume equal variances among groups (homoscedasticity), and when you conduct a Fisher’s one-way ANOVA (rather than Welch’s). This method is also impractical if you are calculating from reported statistics, and MSE is not reported (which is typically the case).\nIf you are unwilling to assume homogeneity of variances, then calculate Cohen’s d between groups as if there are only two groups for comparison. However, you should know that it also makes little sense to conduct a Fisher’s ANOVA in such situations. You may want to switch to Welch’s ANOVA, which does not assume homoscedasticity. If variances differ greatly, you may want to use alternative standardized effect size measures, such as Glass’ delta, and calculate confidence intervals using bootstrap."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#one-way-repeated-measures-anova",
    "href": "Effect-Sizes-for-ANOVAs.html#one-way-repeated-measures-anova",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.4 One-way repeated measures ANOVA",
    "text": "10.4 One-way repeated measures ANOVA\nOne-way repeated measures ANOVA (rmANOVA) is an extension of paired-samples t-tests, with the difference being it can be used in two or more groups.\n\n10.4.1 Determining degrees of freedom\nPlease refer to the following table to determine the degrees of freedom for repeated measure ANOVA effects.\n\n\n\nDegrees of freedom\n\n\n\n\n\nWithin-subject ANOVA (repeated measures)\n\n\n\nEffect\n\\(k-1\\)\n\n\nError-between\n\\((n-1)\\times(k-1)\\)\n\n\nError-within\n\\((n-1)\\cdot (k-1)\\)\n\n\nTotal (within)\n\\(n\\cdot (k-1)\\)\n\n\n\n\n\n10.4.2 Eta-squared from rmANOVA statistics\nCommonly, we use eta-squared (\\(\\eta^2\\)) or partial eta-squared (\\(\\eta_p^2\\)) as the effect size measure for one-way rmANOVAs, for which these two are in fact equal. Let’s construct an rmANOVA model use example data from the datarium package (Kassambara 2019). The selfesteem data set simply shows self-esteem scores over three repeated measurements within the same subjects.\n\n### load in and re-format data\nlibrary(tidyr)\ndata(\"selfesteem\", package = \"datarium\")\nselfesteem &lt;- tidyr::pivot_longer(selfesteem,cols = c(\"t1\",\"t2\",\"t3\"))\ncolnames(selfesteem) &lt;- c(\"subject\",\"time\",\"self_esteem\")\n####\n\nrmANOVA_mdl = aov(formula = self_esteem ~ time + Error(subject),\n                  data = selfesteem)\nsummary(rmANOVA_mdl)\n\n\nError: subject\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  1 0.07667 0.07667               \n\nError: Within\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime       2 102.46   51.23   63.07 1.06e-10 ***\nResiduals 26  21.12    0.81                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nThere are two tables displayed here, the table on top displays the between subject effects and the table below shows the within subject effects.The equations and functions to calculate \\(\\eta^2\\) mentioned in the one-way between-subjects ANOVAs section also apply here:\n\\[\n\\eta^2 = \\frac{df_\\text{effect}\\times F}{df_\\text{effect} \\times F + df_\\text{error-within}},\n\\]\n\\[\n\\eta^2 = \\frac{SS_\\text{effect}}{SS_\\text{total}}\n\\]\nNote that here \\(SS_\\text{total}\\) does not include \\(SS_\\text{error-between}\\) because we are not interested in it by conducting a rmANOVA. This analysis targets an effect that we think should happen on each subject, regardless of how these subjects will vary from each other. In other words, between-subjects variance can be large or small, but we do not care about it when we examine whether there is an effect or not across repeated measures. Therefore the total sum of squares can be defined as\n\\[\nSS_\\text{total} = SS_\\text{effect} + SS_\\text{error-within}\n\\]\nTherefore we can calculate \\(\\eta^2\\) from the rmANOVA table as,\n\\[\n\\eta^2 = \\frac{102.46}{21.12 + 102.46} = .83\n\\]\nWe can plug the rmANOVA model into the eta_squared() function from the effectsize package in R (Ben-Shachar, Lüdecke, and Makowski 2020) to calculate \\(\\eta^2\\).\n\nlibrary(effectsize)\n\neta_squared(rmANOVA_mdl,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nGroup  | Parameter | Eta2 (partial) |       95% CI\n--------------------------------------------------\nWithin |      time |           0.83 | [0.69, 0.89]\n\n\nAs expected, we find the same point-estimate from our hand calculation. To calculate \\(\\eta^2\\) from the F-statistic and degrees of freedom we can use the MOTE package (Buchanan et al. 2019) as we did in Section 10.3.3\n\nlibrary(MOTE)\n\neta &lt;- eta.full.SS(dfm = 2,  # effect degrees of freedom\n                   dfe = 26, # error degrees of freedom\n                   ssm = 102.46, # sum of squares for the effect\n                   sst = 102.46 + 21.12, # total sum of squares\n                   Fvalue = 63.07,\n                   a = .05)\n\ndata.frame(eta_squared = apa(eta$eta),\n           etalow = apa(eta$etalow),\n           etahigh = apa(eta$etahigh))\n\n  eta_squared etalow etahigh\n1       0.829  0.644   0.910\n\n\nNote the discrepency between confidence intervals returned by MOTE and effectsize this is due to differences in the calculation."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#two-way-between-subjects-anova",
    "href": "Effect-Sizes-for-ANOVAs.html#two-way-between-subjects-anova",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.5 Two-Way between-subjects ANOVA",
    "text": "10.5 Two-Way between-subjects ANOVA\nTwo-way between-subjects ANOVA is used when there are two predictor grouping variables in the model. Note again that between subjects means that each group contain different subjects.\n\n10.5.1 Determining degrees of freedom\nPlease refer to the following table to determine the degrees of freedom for two-way ANOVA effects (Morse 2018). Note that \\(k_1\\) is the number of groups in the first variable, and \\(k_2\\) is the number of groups in the second variable.\n\n\n\nDegrees of freedom\n\n\n\n\n\nWithin subjects ANOVA\n\n\n\nMain Effect (of one variable)\n\\(k_1-1\\) or \\(k_2-1\\)\n\n\nInteraction Effect\n\\((k_1-1)\\times (k_2-1)\\)\n\n\nError\n\\(n-k_1\\cdot k_2\\)\n\n\nTotal\n\\(n-1\\)\n\n\n\n\n\n10.5.2 Eta-squared from Two-Way ANOVA statistics\nFor Two-way ANOVAs we can obtain \\(\\eta^2_p\\) for each predictor in the model. Let’s construct our ANOVA model using data from the palmerpenguins dataset (Horst, Hill, and Gorman 2020). In this example we want to see how the species and the sex of the penguin explains variance in body mass.\n\nlibrary(palmerpenguins)\n\nANOVA2_mdl &lt;- aov(body_mass_g ~ species + sex + species:sex,\n                  data = penguins)\n\nsummary(ANOVA2_mdl)\n\n             Df    Sum Sq  Mean Sq F value   Pr(&gt;F)    \nspecies       2 145190219 72595110 758.358  &lt; 2e-16 ***\nsex           1  37090262 37090262 387.460  &lt; 2e-16 ***\nspecies:sex   2   1676557   838278   8.757 0.000197 ***\nResiduals   327  31302628    95727                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n11 observations deleted due to missingness\n\n\n\n\n\n\n\nThe results show that species, sex, and the interaction between the two account for substantial variance in body mass. We can obtain the contributions of species, sex, and their interaction by computing the partial eta-squared value (\\(\\eta_p^2\\)). To do this using similar formulas to \\(\\eta^2\\) from the one-way ANOVAs. The difference between the formulas for \\(\\eta_p^2\\) anf \\(\\eta^2\\) is that \\(\\eta_p^2\\) does not use the total sum of squares in the denominator, instead it uses the residual sum of squares (\\(SS_\\text{error}\\)) and the sum of squares from the effect of interest (\\(SS_\\text{effect}\\); i.e., species or sex but not both). For example,\n\\[\n\\small{\\text{For species:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error}} = \\frac{145190219}{145190219+ 31302628} = .82}\n\\] \\[\n\\small{\\text{For sex:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error}} = \\frac{37090262}{37090262 + 31302628} = .54}\n\\] \\[\n\\small{\\text{For sex}\\times\\text{species:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error}} = \\frac{1676557}{1676557+ 31302628} = .05}\n\\] We can also easily do this in R using the eta_squared function in the effectsize package (Ben-Shachar, Lüdecke, and Makowski 2020) and setting the argument partial = TRUE.\n\nlibrary(effectsize)\n\neta_squared(ANOVA2_mdl,\n            partial = TRUE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nspecies     |           0.82 | [0.79, 0.85]\nsex         |           0.54 | [0.48, 0.60]\nspecies:sex |           0.05 | [0.01, 0.10]"
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#two-way-repeated-measures-anova",
    "href": "Effect-Sizes-for-ANOVAs.html#two-way-repeated-measures-anova",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.6 Two-way repeated measures ANOVA",
    "text": "10.6 Two-way repeated measures ANOVA\nA two-way repeated measures ANOVA (rmANOVA) would indicate that subjects are exposed to each condition along two variables.\n\n10.6.1 Determing degrees of freedom\nPlease refer to the following table to determine the degrees of freedom for two-way rmANOVA effects (Morse 2018). Note that \\(k_1\\) is the number of groups in the first variable, and \\(k_2\\) is the number of groups in the second variable.\n\n\n\nDegrees of freedom\n\n\n\n\n\nBetween subjects ANOVA\n\n\n\nMain Effect (of one variable)\n\\(k_1-1\\) or \\(k_2-1\\)\n\n\nInteraction Effect\n\\((k_1-1)\\times (k_2-1)\\)\n\n\nError-between\n\\((k_1 \\cdot k_2) - 1\\)\n\n\nError-within\n\\((n - 1)\\times (k_1\\cdot k_2 - 1)\\)\n\n\nTotal\n\\(n-1\\)\n\n\n\n\n\n10.6.2 Eta-squared from Two-way rmANOVA\nFor a two-way repeated measures ANOVA, we can use the weightloss data set from the datarius package (Kassambara 2019). This data set contains a diet condition and a control condition that tracked subjects across time (3 time points) for each of condition.\n\n### load in and re-format data\nlibrary(tidyr)\ndata(\"weightloss\", package = \"datarium\")\nweightloss &lt;- tidyr::pivot_longer(weightloss,cols = c(\"t1\",\"t2\",\"t3\"))\ncolnames(weightloss) &lt;- c(\"subject\",\"diet\",\"exercises\",\"time\", \"weight_loss\")\nweightloss &lt;- weightloss[weightloss$diet=='no',] # remove the diet intervention trials\n####\n\nrmANOVA2_mdl = aov(formula = weight_loss ~ time + exercises + time:exercises + Error(subject),\n                   data = weightloss)\nsummary(rmANOVA2_mdl)\n\n\nError: subject\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 11  20.64   1.877               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime            2 129.26   64.63   50.57 3.45e-13 ***\nexercises       1 101.03  101.03   79.05 3.16e-12 ***\ntime:exercises  2  92.55   46.28   36.21 9.26e-11 ***\nResiduals      55  70.29    1.28                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nFrom the table and graph above, we can see that there is substantial within-person change in weight loss under the exercise condition and no discernible increase in weight loss without exercising. This suggests that there is a substantial interaction effect. Like we did in the between-subjects two-way ANOVA, we can calculate the partial eta squared values from the ANOVA table\n\\[\n\\small{\\text{For time:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error-within}} = \\frac{129.26}{129.26+ 70.29} = .65}\n\\] \\[\n\\small{\\text{For exercise:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error-within}} = \\frac{101.03}{101.03 + 70.29} = .59}\n\\] \\[\n\\small{\\text{For sex}\\times\\text{species:}\\;\\;\\;\\; \\eta_p^2= \\frac{SS_\\text{effect}}{SS_\\text{effect} + SS_\\text{error-within}} = \\frac{92.55}{92.55+ 70.29} = .57}\n\\]\nRemember for the partial eta-squared, the denominator is not the total sum of squares rather it is the effect sum of squares and the error. In the repeated measures ANOVA, the error should only be for the within subject error because the variance between subjects is not something we are interested about. We can also calculate this in R using the eta_squared() function again.\n\nlibrary(effectsize)\n\neta_squared(rmANOVA2_mdl,\n            partial = TRUE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nGroup  |      Parameter | Eta2 (partial) |       95% CI\n-------------------------------------------------------\nWithin |           time |           0.65 | [0.49, 0.75]\nWithin |      exercises |           0.59 | [0.42, 0.70]\nWithin | time:exercises |           0.57 | [0.39, 0.69]"
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#effect-sizes-for-anovas",
    "href": "Effect-Sizes-for-ANOVAs.html#effect-sizes-for-anovas",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.7 Effect Sizes for ANOVAs",
    "text": "10.7 Effect Sizes for ANOVAs\nANOVA (Analysis of Variance) is a statistical method used to compare means across multiple groups or conditions. It is mostly used when the outcome variable is continuous and the predictor variables are categorical. Commonly used effect size measures for ANOVAs / F-tests include: eta-squared (\\(\\eta^2\\)), partial eta-squared (\\(\\eta_p^2\\)), generalized eta-squared (\\(\\eta^2_G\\)), omega-squared (\\(\\omega^2\\)), partial omega-squared (\\(\\omega\\)), generalized omega-squared (\\(\\omega^2_G\\)), Cohen’s \\(f\\).\n\n\n\n\n\n\n\n\nType\nDescription\nSection\n\n\n\n\n\\(\\eta^2\\) - eta-squared\nMeasures the variance explained of the whole ANOVA model.\nSection 10.7.1\n\n\n\\(\\eta^2_p\\) - Partial eta-squared\nMeasures the variance explained by a specific factor in the model.\nSection 10.7.2\n\n\n\\(\\eta^2_G\\) - Generalized eta-squared\nSimilar to \\(\\eta^2\\), but uses the sum of squares of all non-manipulated variables in the calculation. This allows meta-analysts to compare \\(\\eta_G\\) across different designs.\nSection 10.7.3\n\n\n\\(\\omega^2,\\omega^2_p,\\omega^2_G\\) - Omega squared corrections\nCorrections to bias observed in \\(\\eta^2\\) measures. Can be interpreted in the same way as \\(\\eta^2\\).\nSection 10.7.4\n\n\n\\(f\\) - Cohen’s f\nThis effect size can be interpreted as the average Cohen’s \\(d\\) between each group.\nSection 10.7.5\n\n\n\n\n10.7.1 Eta-Squared (\\(\\eta^2\\))\nEta-squared is the ratio between the between-group variance and the total variance. It describes the proportion of the total variability in the data that are accounted for by a particular factor. Therefore, it is a measure of variance explained. To calculate eta-squared (\\(\\eta^2\\)) we need to first calculate the total sum of squares (\\(SS_{\\text{total}}\\)) and the effect sum of squares (\\(SS_{\\text{effect}}\\)),\n\\[\nSS_{\\text{total}} = \\sum_{i=1}^n (y_i-\\bar{y})^2\n\\]\nWhere \\(\\bar{y}\\) is the grand mean (i.e., the mean of all data points collapsed across groups). To calculate the sum of squares of the effect, we can take the predicted \\(y\\) values (\\(\\hat{y}_i\\)). In the case of categorical predictors, \\(\\hat{y}_i\\) is equal to the mean of the outcome within that individual’s respective group. Therefore the sum of squares of the effect can be calculated using the following formula:\n\\[\nSS_{\\text{effect}} = \\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2.\n\\]\nNow we can calculate the eta-squared value,\n\\[\n\\eta^2 = \\frac{SS_{\\text{effect}}}{SS_{\\text{total}}}\n\\]\nThe standard error of eta-square can be approximated from Olkin and Finn (1995):\n\\[\nSE_{\\eta^2}=\\sqrt{\\frac{4\\eta^2\\left(1-\\eta^2\\right)^2\\left(n+k-1\\right)^2}{\\left(n^2-1\\right)\\left(3+n\\right)}}\n\\]\nThe sampling distribution for \\(\\eta^2\\) is asymmetric as all the values are bounded in the range, 0 to 1. The confidence interval surrounding \\(\\eta^2\\) will likewise be asymmetric so instead of calculating the confidence interval from the standard error, we can instead use a non-central F-distribution using the degrees of freedom between groups (e.g., for three groups: \\(df_b=k-1=3-1=2\\)) and the degrees of freedom within groups (e.g., for 100 subjects and three groups: \\(df_b=n-k=100-3=97\\)) to obtain the confidence intervals. Another option is to use bootstrapping procedure (i.e., resampling the observed data points to construct a sampling distribution around \\(\\eta^2\\), see Kirby and Gerlanc 2013) and then take the .025 and .975 quantiles of that distribution. The R code below will compute the proper confidence interval.\nWhere \\(n\\) is the total sample size and \\(k\\) is the number of predictors. In R, we can calculate \\(\\eta^2\\) from a one-way ANOVA using the penguin data set from the palmerpenguins data package. The aov function in base R allows the analyst to model an ANOVA with categorical predictors on the right side (species) of the ~ and the outcome on the left side (body mass of penguin). We can then use the eta_squared function in the effectsize package to calculate the point estimate and confidence intervals.\n\n# Example:\n# group: species\n# outcome: body mass\n\nlibrary(palmerpenguins)\nlibrary(effectsize)\n\n# One-Way ANOVA\nmdl1 &lt;- aov(data = penguins,\n           body_mass_g ~ species)\n\neta_squared(mdl1, \n            partial = FALSE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\nspecies   | 0.67 | [0.62, 0.71]\n\n\nThe species of the penguin explains the majority of the variation in body mass showing an eta-squared value of \\(\\eta^2\\) = .67 [.62, .71]. Let us now do the same thing with a two-way ANOVA, using both species and sex as our categorical predictors.\n\n# Example:\n# group: species and sex\n# outcome: body mass\n\n# Two-Way ANOVA\nmdl2 &lt;- aov(data = penguins,\n           body_mass_g ~ species + sex)\n\neta_squared(mdl2, \n            partial = FALSE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\nspecies   | 0.67 | [0.62, 0.72]\nsex       | 0.17 | [0.10, 0.24]\n\n\nNotice that the \\(\\eta^2\\) does not change for species since the sum of squares is divided by the total sum of squares rather than the residual sum of squares (see partial eta squared). The example shows an eta-squared value for species of \\(\\eta^2\\) = .67 [.62, .72] and for sex \\(\\eta^2\\) = .17 [.10, .24].\n\n\n10.7.2 Partial Eta-Squared (\\(\\eta^2_p\\))\nPartial eta-squared is the most commonly reported effect size measure for F-tests. It describes the proportion of variability associated with an effect when the variability associated with all other effects identified in the analysis has been removed from consideration (hence, it is “partial”). If you have access to an ANOVA table, the partial eta-squared for an effect is calculated as:\n\\[\n\\eta_p^2 = \\frac{ SS_{\\text{effect}}}{SS_{\\text{effect}}+SS_{\\text{error}}}\n\\]\nThere are two things to take note of here:\n\nIn a one-way ANOVA (one categorical predictor), partial eta-squared and eta-squared are equivalent since \\(SS_{\\text{total}} = SS_{\\text{effect}}+SS_{\\text{error}}\\)\nIf there are multiple predictors, the denominator will only include the sum of squares of the effect of interest rather than the effect of all predictors (which is the case for the non-partial eta squared).\n\nIn R, let us compare the partial eta-squared values for a one-way ANOVA and a two-way ANOVA using the eta_squared function in the effectsize package.\n\n# Example:\n# group: species\n# outcome: body mass\n\n\n# One-Way ANOVA\nmdl1 &lt;- aov(data = penguins,\n           body_mass_g ~ species)\n\neta_squared(mdl1, \n            partial = TRUE,\n            alternative = \"two.sided\") \n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nspecies   | 0.67 | [0.62, 0.71]\n\n\nThe species of the penguin explains the majority of the variation in body mass showing a partial eta-squared value of \\(\\eta^2\\) = \\(\\eta^2_p\\) = .67 [.62, .71]. Let us now do the same thing with a two-way ANOVA, using both species and sex as our categorical predictors.\n\n# Example:\n# group: species and sex\n# outcome: body mass\n\n# Two-Way ANOVA\nmdl2 &lt;- aov(data = penguins,\n           body_mass_g ~ species + sex)\n\neta_squared(mdl2, \n            partial = TRUE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nspecies   |           0.81 | [0.78, 0.84]\nsex       |           0.53 | [0.46, 0.59]\n\n\nOnce we run a two-way ANOVA, the eta-squared value for species begins to differ. The example shows a partial eta-squared value for species of \\(\\eta^2_p\\) = .81 [.78, .84] and for sex \\(\\eta^2\\) = .53 [.46, .59].\n\n\n10.7.3 Generalized Eta-Squared (\\(\\eta^2_G\\))\nGeneralized eta-squared was devised to allow effect size comparisons across studies with different designs, which eta-squared and partial eta-squared cannot help with (refer to for details). If you can (either you are confident that you calculated it right, or the statistical software that you use just happens to return this measure), report generalized eta-squared in addition to eta-squared or partial eta-squared. The biggest advantage of generalized eta-squared is that it facilitates meta-analysis, which is important for the accumulation of knowledge. To calculate generalized eta-squared, the denominator should be the sums of squares of all the non-manipulated variables (i.e., variance of purely individual differences in the outcome rather than individual differences in treatment effects). Note the formula will depend on the design of the study. In R, the eta_squared function in the effectsize package supports the calculation of generalized eta-squared by using the generalized=TRUE argument.\n\n\n10.7.4 Omega squared corrections (\\(\\omega^2\\), \\(\\omega^2_p\\))\nSimilar to Hedges’ correction for small sample bias in standardized mean differences, \\(\\eta^2\\) is also biased. We can apply a correction to \\(\\eta^2\\) and obtain a relatively unbiased estimate of the population proportion of variance explained by the predictor. To calculate \\(\\omega\\), we need to calculate the within group mean squared errors:\n\\[\nMS_{\\text{within}} = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2.\n\\] Where the predicted values of the outcome, \\(\\hat{y}_i\\), are the mean value for the individual’s respective group.\n\\[\n\\omega^2 = \\frac{SS_{\\text{effect}}-(k-1)\\times MS_{\\text{within}}}{SS_{\\text{total}} + MS_{\\text{within}}}\n\\]\nWhere \\(k\\) is the number of groups in the predictor (effect) variable. For partial omega-squared values, we need the mean squared error of effect and the residuals which can easily be calculated from their sum of squares:\n\\[\nMS_{\\text{effect}} = \\frac{SS_{\\text{effect}}}{n}\n\\] \\[\nMS_{\\text{error}} = \\frac{SS_{\\text{error}}}{n}\n\\] Then to calculate the partial omega squared we can use the following formula:\n\\[\n\\omega_p^2 = \\frac{(k-1)(MS_{\\text{effect}} - MS_{\\text{error}})}{(k-1)\\times MS_{\\text{effect}} + (n-k-1)\\times MS_{\\text{error}}}\n\\]\nIn R, we can use the omega_squared function in the effectsize package to calculate both \\(\\omega^2\\) and \\(\\omega^2_p\\). For the first example we will use a one-way ANOVA.\n\n# Example:\n# group: species\n# outcome: body mass\n\nlibrary(palmerpenguins)\n\n# One-Way ANOVA\nmdl1 &lt;- aov(data = penguins,\n           body_mass_g ~ species)\n\n# omega-squared\nomega_squared(mdl1, \n            partial = FALSE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 |       95% CI\n---------------------------------\nspecies   |   0.67 | [0.61, 0.71]\n\n# partial omega-squared\nomega_squared(mdl1, \n              partial = TRUE,\n              alternative = \"two.sided\")\n\nFor one-way between subjects designs, partial omega squared is\n  equivalent to omega squared. Returning omega squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Omega2 |       95% CI\n---------------------------------\nspecies   |   0.67 | [0.61, 0.71]\n\n\nThe species of the penguin explains the majority of the variation in body mass showing an omega-squared value of \\(\\omega^2\\) = .67 [.61, .71]. Note that the partial and non-partial omega squared values do not show a difference as expected in a one-way ANOVA. Let us now do the same thing with a two-way ANOVA, using both species and sex as our categorical predictors.\n\n# Example:\n# group: species and sex\n# outcome: body mass\n\n# Two-Way ANOVA\nmdl2 &lt;- aov(data = penguins,\n           body_mass_g ~ species + sex)\n\n# omega-squared\nomega_squared(mdl2, \n            partial = FALSE,\n            alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 |       95% CI\n---------------------------------\nspecies   |   0.67 | [0.62, 0.72]\nsex       |   0.17 | [0.10, 0.24]\n\n# partial omega-squared\nomega_squared(mdl2, \n              partial = TRUE,\n              alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 (partial) |       95% CI\n-------------------------------------------\nspecies   |             0.81 | [0.78, 0.84]\nsex       |             0.53 | [0.46, 0.58]\n\n\nOnce we run a two-way ANOVA, the eta-squared value for species diverge. The example shows a partial eta-squared value for species of \\(\\omega^2_p\\) = .81 [.78, .84] and for sex \\(\\omega^2\\) = .53 [.46, .58].\n\n\n10.7.5 Cohen’s \\(f\\)\nCohen’s \\(f\\) is defined as the ratio of the standard deviations of the group means and the common standard deviation within each of the groups (note that ANOVA assumes equal variances among groups). Cohen’s \\(f\\) is the effect size measure asked for by G*Power for power analysis for F-tests. This can be calculated easily from the eta-squared value,\n\\[\nf = \\sqrt{\\frac{\\eta^2}{1-\\eta^2}}\n\\]\nor by the \\(\\omega^2\\) value,\n\\[\nf = \\sqrt{\\frac{\\omega^2}{1-\\omega^2}}\n\\]\nCohen’s \\(f\\) can be interpreted as “the average Cohen’s \\(d\\) (i.e., standardized mean difference) between groups”. Note that there is no directionality to this effect size (\\(f\\) is always greater than zero), therefore two studies showing the same \\(f\\) with the same groups, can have very different patterns of group mean differences. Note that Cohen’s \\(f\\) is also often reported as \\(f^2\\). The confidence intervals for Cohen’s \\(f\\) can be computed from the upper bounds and lower bounds of the confidence intervals from eta-square or omega-square using the formulas to calculate \\(f\\) (e.g., for the upper bound \\(f_{UP} = \\sqrt{\\frac{\\eta^2_{UP}}{1-\\eta^2_{UP}}}\\)).\nIn R, we can use the cohens_f function in the effectsize package to calculate Cohen’s \\(f\\). We will again use example data from the palmerpenguins package.\n\n# Example:\n# group: species\n# outcome: body mass\n\n# ANOVA\nmdl &lt;- aov(data = penguins,\n           body_mass_g ~ species)   \n\ncohens_f(mdl,alternative = \"two.sided\")\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Cohen's f |       95% CI\n------------------------------------\nspecies   |      1.42 | [1.27, 1.57]\n\n\nIn the example above, the difference in body mass between the three penguin species was very large showing a Cohen’s \\(f\\) of 1.42 [1.27, 1.57]."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#reporting-anova-results",
    "href": "Effect-Sizes-for-ANOVAs.html#reporting-anova-results",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "10.8 Reporting ANOVA results",
    "text": "10.8 Reporting ANOVA results\nFor ANOVAs/F-tests, you will always need to report two kinds of effects: the omnibus effect of the factor(s) and the effect of planned contrasts or post hoc comparisons.\nFor instance, imagine that you are comparing three groups/conditions with a one-way ANOVA. The ANOVA will first return an F-statistic, the degrees of freedom, and the associated p-value. Here, you need to calculate the size of this omnibus factor effect in eta-squared, partial eta-squared, or generalized eta-squared. Suppose the omnibus effect is significant. You now know that there is at least one group that differs from the others. You want to know which group(s) differ from the others, and how much they differ. Therefore, you conduct post hoc comparisons on these groups. Because post hoc comparisons compare each group with the others in pairs, you will get a t-statistic and p-value for each comparison. For this, you need to calculate and report Cohen’s \\(d\\) or Hedges’ \\(g\\).\nImagine that you have two independent variables or factors, and you conduct a two-by-two factorial ANOVA. The first thing to do then is look at the interaction. If the interaction is significant, you again report the associated omnibus effect size measures, and proceed to analyze the simple effects. Depending on your research question, you compare the levels of one IV on each level of the other IV. You will report d or g for these simple effects. If the interaction is not significant, you look at the main effects and report the associated omnibus effect. You then proceed to analyze the main effect by comparing the levels of one IV while collapsing/aggregating the levels of the other IV. You will report \\(d\\) or \\(g\\) for these pairwise comparisons.\nNote that lower-order effects are not directly interpretable if higher-order effects are significant. If you have a significant interaction in a two-way ANOVA, you cannot interpret the main effects directly. If you have a significant three-way interaction in a three-way ANOVA, you cannot interpret the main effects or the two-way interactions directly, regardless of whether they are significant or not.\nIn R, we can use the summary function to display the anova table. We can also append the table to include, for example, partial omega squared values and their respective confidence intervals\n\n# ANOVA mdl\nmdl &lt;- aov(data = penguins,\n           body_mass_g ~ species + sex)   \n\n# calculate partial omega-squared values\nomega_values &lt;- omega_squared(mdl, alternative = \"two.sided\")\n\n# create table of partial omega-squared values\nomega_table &lt;- data.frame(omega_sq = MOTE::apa(c(omega_values$Omega2_partial,NA)),\n                     omega_low = MOTE::apa(c(omega_values$CI_low,NA)),\n                     omega_high = MOTE::apa(c(omega_values$CI_high,NA)))\n\n# append omega values to summary of anova table\ncbind(summary(mdl)[[1]],\n      omega_table)\n\n             Df    Sum Sq    Mean Sq  F value        Pr(&gt;F) omega_sq omega_low\nspecies       2 145190219 72595109.6 724.2080 3.079053e-121    0.813     0.781\nsex           1  37090262 37090261.8 370.0121  8.729411e-56    0.526     0.457\nResiduals   329  32979185   100240.7       NA            NA       NA        NA\n            omega_high\nspecies          0.838\nsex              0.585\nResiduals           NA\n\n\n\n\n\n\nBen-Shachar, Mattan S., Daniel Lüdecke, and Dominique Makowski. 2020. “effectsize: Estimation of Effect Size Indices and Standardized Parameters.” Journal of Open Source Software 5 (56): 2815. https://doi.org/10.21105/joss.02815.\n\n\nBuchanan, Erin M., Amber Gillenwaters, John E. Scofield, and K. D. Valentine. 2019. MOTE: Measure of the Effect: Package to Assist in Effect Size Calculations and Their Confidence Intervals. http://github.com/doomlab/MOTE.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nKassambara, Alboukadel. 2019. Datarium: Data Bank for Statistical Analysis and Visualization. https://CRAN.R-project.org/package=datarium.\n\n\nKirby, Kris N, and Daniel Gerlanc. 2013. “BootES: An r Package for Bootstrap Confidence Intervals on Effect Sizes.” Behavior Research Methods 45: 905–27.\n\n\nMorse, David. 2018. “How to Calculate Degrees of Freedom When Using Two Way ANOVA with Unequal Sample Size?”\n\n\nOlkin, Ingram, and Jeremy D. Finn. 1995. “Correlations Redux.” Psychological Bulletin 118 (1): 155–64. https://doi.org/10.1037/0033-2909.118.1.155.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in R with the metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03."
  },
  {
    "objectID": "Effect-Sizes-for-ANOVAs.html#footnotes",
    "href": "Effect-Sizes-for-ANOVAs.html#footnotes",
    "title": "10  Effect Sizes for ANOVAs",
    "section": "",
    "text": "There are variants of ANOVAs that can have each of these assumptions violated.↩︎\nSee this forum discussion for explanation.↩︎"
  },
  {
    "objectID": "Differences-in-Variance.html#sec-vr",
    "href": "Differences-in-Variance.html#sec-vr",
    "title": "11  Differences in Variability",
    "section": "11.1 Variability Ratios",
    "text": "11.1 Variability Ratios\n\n11.1.1 Natural Logarithm of Variability Ratio for Independent Groups (\\(lnVR_\\text{ind}\\))\nThe variability ratio for independent groups can be calculated by taking the natural logarithm of the standard deviation within one group divided by the standard deviation in another group,\n\\[\nlnVR_\\text{ind}=\\ln\\left(\\frac{S_T}{S_C}\\right) + CF\n\\]\nWhere \\(CF\\) is a small sample correction factor calculated as,\n\\[\nCF=\\frac{1}{2(n_T-1)}-\\frac{1}{2(n_C-1)}\n\\]\nA \\(lnVR\\) of zero therefore would indicate no difference in variation between the two groups, a \\(lnVR\\) of &gt;0 would indicate larger variance in group 1, and \\(lnVR\\) of &lt;0 would indicate larger variance in group 2. The standard error of the VR can be calculated as,\n\\[\nSE_{lnVR_\\text{ind}}=\\sqrt{\\frac{n_T}{2(n_T-1)^2}+\\frac{n_C}{2(n_C-1)^2}}\n\\]\nIn R, we can simply use the metafor packages escalc() function from the metafor package (Viechtbauer 2010) as follows:\n\n# Example:\n# Group 1: standard deviation = 4.5, sample size = 50\n# Group 2: standard deviation = 3.5, sample size = 50\n\nlibrary(metafor)\n\n# prepare the data\nSD1 &lt;- 4.5\nSD2 &lt;- 3.5\nn1 &lt;- n2 &lt;- 50\n\nlnVRind &lt;- escalc(\n    measure = \"VR\",\n    sd1i = SD1,\n    sd2i = SD2,\n    n1i = n1,\n    n2i = n2\n  )\n\n\nlnVRind$SE &lt;- sqrt(lnVRind$vi)\n\n# calculate confidence interval\nlnVRind_low &lt;- lnVRind$yi - 1.96*lnVRind$SE\nlnVRind_high &lt;- lnVRind$yi + 1.96*lnVRind$SE\n\n# print the VR value and confidence intervals\ndata.frame(lnVRind = MOTE::apa(lnVRind$yi),\n           lnVRind_low = MOTE::apa(lnVRind_low),\n           lnVRind_high = MOTE::apa(lnVRind_high))\n\n  lnVRind lnVRind_low lnVRind_high\n1   0.251      -0.029        0.531\n\n\nFrom the example, we obtain a natural log variability ratio of \\(lnVR_\\text{ind}\\) = 0.25 [-0.03, 0.53].\n\n\n11.1.2 Natural Logarithm of Variability Ratio for Dependent Groups (\\(lnVR_\\text{dep}\\))\nThe variability ratio for dependent groups can similarly be calculated by taking the natural logarithm of the standard deviation within one group divided by the standard deviation in another group,\n\\[\nlnVR_\\text{dep}=\\ln\\left(\\frac{S_T}{S_C}\\right)\n\\]\nNote, the correction factor for small sample size bias is not relevant here as due to its calculation its value is zero.\n\\[\nSE_{lnVR_\\text{dep}}=\\sqrt{\\frac{n}{n-1} - \\frac{r^2}{n-1} +  \\frac{r^4\\left(S^8_T+S^8_C\\right)}{2(n-1)^2 S^4_T+S^4_C}}\n\\]\nIn R, we can simply use the metafor packages escalc() function as follows:\n\n# Example:\n# Group 1: standard deviation = 4.5\n# Group 2: standard deviation = 3.5\n# Sample size = 50\n# Correlation = 0.4\n\nlibrary(metafor)\n\n# prepare the data\nSD1 &lt;- 4.5\nSD2 &lt;- 3.5\nn &lt;- 50\nr &lt;- 0.4\n\n# use escalc to compute lnVRdep\nlnVRdep &lt;- escalc(\n  measure = \"VRC\",\n  sd1i = SD1,\n  sd2i = SD2,\n  ni = n1,\n  ri = r\n)\n\n\nlnVRdep$SE &lt;- sqrt(lnVRdep$vi)\n\n# calculate confidence interval\nlnVRdep_low &lt;- lnVRdep$yi - 1.96*lnVRdep$SE\nlnVRdep_high &lt;- lnVRdep$yi + 1.96*lnVRdep$SE\n\n# print the VR value and confidence intervals\ndata.frame(lnVRdep = MOTE::apa(lnVRdep$yi),\n           lnVRdep_low = MOTE::apa(lnVRdep_low),\n           v_high = MOTE::apa(lnVRdep_high))\n\n  lnVRdep lnVRdep_low v_high\n1   0.251      -0.005  0.508"
  },
  {
    "objectID": "Differences-in-Variance.html#sec-cvr",
    "href": "Differences-in-Variance.html#sec-cvr",
    "title": "11  Differences in Variability",
    "section": "11.2 Coefficient of Variation Ratios",
    "text": "11.2 Coefficient of Variation Ratios\n\n11.2.1 Natural Logarithm of Coefficient of Variation Ratio for independent groups (lnCVR_)\nThe coefficient of variation ratio for independent groups can be calculated by taking the natural logarithm of the coefficient of variation within one group divided by the coefficient of variation in another group,\n\\[\nlnCVR_\\text{ind}=\\ln\\left(\\frac{CV_T}{CV_C}\\right) + CF\n\\]\nWhere \\(CV_T =S_T / M_T\\), \\(CV_C =S_C / M_C\\), and \\(M\\) indicates the mean of the respective group. The correction factor, \\(CF\\), is a small sample size bias correction factor that combines that from the lnRR (presented earlier) and the lnVR calculated as,\n\\[\nCF=\\frac{1}{2(n_T-1)}-\\frac{1}{2(n_C-1)} + \\frac{S_T^2}{2(n_TM_T^2)} + \\frac{S_C^2}{2(n_CM_C^2)}\n\\] In R, we can simply use the escalc() function from the metafor package as follows:\n\n# Example:\n# Group 1: mean = 22.4, standard deviation = 4.5, sample size = 50\n# Group 2: mean = 20.1, standard deviation = 3.5, sample size = 50\n\nlibrary(metafor)\n\n# prepare the data\nM1 &lt;- 22.4\nM2 &lt;- 20.1\nSD1 &lt;- 4.5\nSD2 &lt;- 3.5\nn1 &lt;- n2 &lt;- 50\n\nlnCVRind &lt;- escalc(\n  measure = \"CVR\",\n  m1i = M1,\n  m2i = M2,\n  sd1i = SD1,\n  sd2i = SD2,\n  n1i = n1,\n  n2i = n2\n)\n\nlnCVRind$SE &lt;- sqrt(lnCVRind$vi) \n\n# calculate confidence interval\nlnCVRind_low &lt;- lnCVRind$yi - 1.96*lnCVRind$SE\nlnCVRind_high &lt;- lnCVRind$yi + 1.96*lnCVRind$SE\n\n# print the VR value and confidence intervals\ndata.frame(lnCVRind = MOTE::apa(lnCVRind$yi),\n           lnCVRind_low = MOTE::apa(lnCVRind_low),\n           lnCVRind_high = MOTE::apa(lnCVRind_high))\n\n  lnCVRind lnCVRind_low lnCVRind_high\n1    0.143       -0.147         0.433\n\n\n\n\n11.2.2 Natural Logarithm of Coefficient of Variation Ratio for independent groups (\\(lnCVR_\\text{dep}\\))\nThe coefficient of variation ratio for dependent groups can be similarly calculated by taking the natural logarithm of the coefficient of variation within one group divided by the coefficient of variation in another group,\n\\[\nlnCVR_\\text{dep}=\\ln\\left(\\frac{CV_T}{CV_C}\\right) + CF\n\\]\nWhere \\(CV_T =S_T / M_T\\), \\(CV_C =S_C / M_C\\) and CF is a small sample size bias correction factor that combines that from the \\(lnVR\\) (presented earlier) and the \\(lnVR\\) (note again for dependent cases this is zero and so omitted) calculated as,\n\\[\nCF = \\frac{S^2_T}{2n M_T^2} - \\frac{S^2_C}{2nM_C^2}\n\\]\nThe standard error of the \\(lnCVR_\\text{dep}\\) can be calculated as,\n\\[\n\\small{SE_{lnCVR_\\text{dep}} = \\sqrt{\\frac{S^2_T}{n M_T^2} + \\frac{S^2_T}{nM_T^2} + \\frac{S^4_T}{2n^2 M_T^4} + \\frac{S^4_T}{2n^2 M_T^4} + \\frac{2rS_CS_T}{n M_C M_T} + \\frac{r^2S^2_T S^2_C (M^4_T + M^4_C)}{2n^2M_T^4M^4_C}}}\n\\] In R, we can simply use the metafor packages escalc() function as follows:\n\n# Example:\n# Group 1: standard deviation = 4.5\n# Group 2: standard deviation = 3.5\n# Sample size = 50\n# Correlation = 0.4\nlibrary(metafor)\n\n# prepare the data\nM1 &lt;- 22.4\nM2 &lt;- 20.1\nSD1 &lt;- 4.5\nSD2 &lt;- 3.5\nn &lt;- 50\nr &lt;- 0.4\n\nlnCVRdep &lt;- escalc(\n  measure = \"CVRC\",\n  m1i = M1,\n  m2i = M2,\n  sd1i = SD1,\n  sd2i = SD2,\n  ni = n1,\n  ri = r\n)\n\nlnCVRdep$SE &lt;- sqrt(lnCVRdep$vi)\n\n# calculate confidence interval\nlnCVRdep_low &lt;- lnCVRdep$yi - 1.96*lnCVRdep$SE\nlnCVRdep_high &lt;- lnCVRdep$yi + 1.96*lnCVRdep$SE\n\n# print the CVR value and confidence intervals\ndata.frame(lnCVRdep = MOTE::apa(lnCVRdep$yi),\n           lnCVRdep_low = MOTE::apa(lnCVRdep_low),\n           lnCVRdep_high = MOTE::apa(lnCVRdep_high))\n\n  lnCVRdep lnCVRdep_low lnCVRdep_high\n1    0.143       -0.120         0.406\n\n\n\n\n\n\nSenior, Alistair M., Wolfgang Viechtbauer, and Shinichi Nakagawa. 2020. “Revisiting and Expanding the Meta-Analysis of Variation: The Log Coefficient of Variation Ratio.” Research Synthesis Methods 11 (4): 553–67. https://doi.org/10.1002/jrsm.1423.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in R with the metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03."
  },
  {
    "objectID": "Non-Parametric-Effect-Sizes.html#wilcoxon-mann-whitney-tests",
    "href": "Non-Parametric-Effect-Sizes.html#wilcoxon-mann-whitney-tests",
    "title": "12  Non-Parametric Tests",
    "section": "12.1 Wilcoxon-Mann-Whitney tests",
    "text": "12.1 Wilcoxon-Mann-Whitney tests\nA non-parametric alternative to the t-test is the Wilcoxon-Mann-Whitney (WMW) group of tests. When comparing two independent samples this is called a Wilcoxon rank-sum test, but sometimes referred to as a Mann-Whitney U Test. When using it on paired samples, or one sample, it is a signed rank test. These are generally referred to as tests of “symmetry” (Divine et al. 2018).\n\n# Paired samples ---- \n\ndata(sleep)\n\n# wilcoxon test\nwilcox.test(extra ~ group,\n            data = sleep,\n            paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  extra by group\nV = 0, p-value = 0.009091\nalternative hypothesis: true location shift is not equal to 0\n\n# Two Sample ------\n# data import from likert\ndata(mass, package = \"likert\")\ndf_mass = mass |&gt;\n  as.data.frame() |&gt;\n  janitor::clean_names() \n\n# function needs input as a numeric\n# ordered factors can be converted to ranks\n# Again, the warning can be ignored\nwilcox.test(rank(math_relates_to_my_life) ~ gender,\n            data = df_mass)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rank(math_relates_to_my_life) by gender\nW = 23, p-value = 0.1104\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "Non-Parametric-Effect-Sizes.html#brunner-munzel-tests",
    "href": "Non-Parametric-Effect-Sizes.html#brunner-munzel-tests",
    "title": "12  Non-Parametric Tests",
    "section": "12.2 Brunner-Munzel Tests",
    "text": "12.2 Brunner-Munzel Tests\nBrunner-Munzel’s tests can be used instead of the WMW tests. The primary reason is the interpretation of the test (Munzel and Brunner 2002; Brunner and Munzel 2000; Neubert and Brunner 2007). Recently, Karch (2021) argued that the Mann-Whitney test is not a decent test of equality of medians, distributions or stochastic equality. The Brunner-Munzel test, on the other hand, provides a sensible approach to test for stochastic equality.\nThe Brunner-Munzel tests measure a rank based “relative effect” or “stochastic superiority probability”. The test statistic (\\(\\hat p\\)) is essentially the probability of a value in one condition being greater than other while splitting the ties1. However, Brunner-Munzel tests can not be applied to the single group or one-sample designs.\n\\[\n\\hat{p} = P(X&lt;Y)+ \\frac{1}{2} \\cdot P(X=Y)\n\\]\nThese tests are relatively new so there are very few packages offer Brunner-Munzel. Moreover, Karch (2021) argues that the stochastic superiority effect size (\\(\\hat{p}\\)) offers a nuanced way to interpret group differences by visualizing observations as competitors in a contest. Propounded by scholars like Cliff (1993) and Divine et al. (2018), it views each observation from one group in a duel with every observation from another. If an observation from the first group surpasses its counterpart, it “wins,” and the group garners a point; tied observations yield half a point to each group. This concept can be further elucidated through a bubble plot, where placement above, below, or on the diagonal indicates the dominance of one group’s observation over the other. Other interpretations, like transforming p to the Wilcoxon-Mann-Whitney (WMW) odds or Cliff’s δ offer deeper insights. There are implementations of the Brunner-Munzel test in a few packages in R (i.e. lawstat, rankFD, and brunnermunzel). Karch (2021) recommends the brunnermunzel.permutation.test function from the brunnermunzel package. The TOSTER R package can also provide coverage (Läkens 2017; Caldwell 2022).\n\n# Install package for data cleaning\n# install.packages('janitor')\nlibrary(janitor)\n\n# Paired samples\nlibrary(TOSTER)\ndata(sleep)\n\n# When sample sizes are small\n# a permutation version should be used.\n# When this is done a seed should be set.\nset.seed(2124)\nbrunner_munzel(extra ~ group,\n               data = sleep,\n               paired = TRUE,\n               perm = TRUE)\n\n\n    Paired Brunner-Munzel permutation test\n\ndata:  extra by group\nt = -3.7266, df = 9, p-value = 0.003906\nalternative hypothesis: true relative effect is not equal to 0.5\n95 percent confidence interval:\n 0.1233862 0.3866138\nsample estimates:\np(X&lt;Y) + .5*P(X=Y) \n             0.255 \n\n# Two Sample\n# data import from likert\ndata(mass, package = \"likert\")\ndf_mass = mass |&gt;\n  as.data.frame() |&gt;\n  clean_names() \n\n# function needs input as a numeric\n# ordered factors can be converted to ranks\n# Again, the warning can be ignored\nset.seed(24111)\nTOSTER::brunner_munzel(\n  rank(math_relates_to_my_life) ~ gender,\n  data = df_mass,\n  paired = FALSE,\n  perm = TRUE\n)\n\n\n    two-sample Brunner-Munzel permutation test\n\ndata:  rank(math_relates_to_my_life) by gender\nt = -2.1665, df = 17.953, p-value = 0.0642\nalternative hypothesis: true relative effect is not equal to 0.5\n95 percent confidence interval:\n 0.04761905 0.54961243\nsample estimates:\np(X&lt;Y) + .5*P(X=Y) \n         0.2738095"
  },
  {
    "objectID": "Non-Parametric-Effect-Sizes.html#rank-based-effect-sizes",
    "href": "Non-Parametric-Effect-Sizes.html#rank-based-effect-sizes",
    "title": "12  Non-Parametric Tests",
    "section": "12.3 Rank-Based Effect Sizes",
    "text": "12.3 Rank-Based Effect Sizes\nSince the mean and standard deviation are not estimated for a WMW or Brunner-Munzel test, it would be inappropriate to present a standardized mean difference (e.g., Cohen’s d) to accompany these tests. Instead, a rank based effect size (i.e., based on the ranks of the observed values) can be reported to accompany the non-parametric statistical tests.\n\n12.3.1 Rank-Biserial Correlation\nThe rank-biserial correlation (\\(r_{rb}\\)) is considered a measure of dominance. The correlation represents the difference between the proportion of favorable and unfavorable pairs or signed ranks. Larger values indicate that more of \\(X\\) is larger than more of \\(Y\\), with a value of (−1) indicates that all observations in the second, \\(Y\\), group are larger than the first, \\(X\\), group, and a value of (+1) indicates that all observations in the first group are larger than the second.\n\n12.3.1.1 Dependent Groups\n\nCalculate difference scores between pairs:\n\n\\[\nD = X_2 - X_1\n\\]\n\nCalculate the positive and negative rank sums:\n\n\\[\n\\text{When } D_i&gt;0,\\;\\;  R_{\\oplus} = \\sum_{i=1} -1\\cdot \\text{sign}(D_i) \\cdot \\text{rank}(|D_i|)\n\\]\n\\[\n\\text{When } D_i&lt;0,\\;\\;  R_{\\ominus} = \\sum_{i=1} -1\\cdot \\text{sign}(D_i) \\cdot \\text{rank}(|D_i|)\n\\]\n\nWe can set a constant, \\(H\\), to be -1 when the rank positive rank sum is greater than or equal to the negative rank sum (\\(R_{\\oplus} \\ge R_{\\ominus}\\)) or we can set \\(H\\) to 1 when the rank positive rank sum is less than the negative rank sum (\\(R_{\\oplus} &lt; R_{\\ominus}\\)).\n\n\\[\nH = \\begin{cases} -1 &  R_{\\oplus} \\ge  R_{\\ominus} \\\\ 1 & R_{\\oplus} &lt; R_{\\ominus} \\end{cases}\n\\]\n\nCalculate rank-biserial correlation:\n\n\\[\nr_{rb} = 4H\\times \\left| \\frac{\\min( R_{\\oplus}, R_{\\ominus}) - .5\\times ( R_{\\oplus} +  R_{\\ominus})}{n(n + 1)} \\right|\n\\]\n\nFor paired samples, or one sample, the standard error is calculated as the following:\n\n\\[\nSE_{r_{rb}} = \\sqrt{ \\frac {2(2 n^3 + 3 n^2 + n)} {6(n^2 + n)} }\n\\]\n\nThe confidence intervals can then be calculated by Z-transforming the correlation.\n\n\\[\nZ_{rb} = \\text{arctanh}(r_{rb})\n\\]\n\nCalculate the standard error of the Z-transformed correlation\n\n\\[\nSE_{Z_{rb}} = \\frac{SE_{r_{rb}}}{1-r_{rb}^2}\n\\]\n\nThen the confidence interval can be calculated and then back-transformed.\n\n\\[\nCI_{r_{rb}} = \\text{tanh}(Z_{rb}  \\pm  1.96 \\cdot SE_{Z_{rb}})\n\\] In R, we can use the ses_calc() function in TOSTER package (Läkens 2017). For the following example, we will calculate the rank-biserial correlation in the sleep dataset:\n\n# Dependent groups\n\ndata(sleep)\nlibrary(TOSTER)\n\n# When sample sizes are small\n# a permutation version should be used.\n# When this is done a seed should be set.\nset.seed(2124)\nses_calc(extra ~ group,\n         data = sleep,\n         paired = TRUE)\n\n                           estimate lower.ci  upper.ci conf.level\nRank-Biserial Correlation 0.9818182 0.928369 0.9954785       0.95\n\n\nThe example shows a rank-biserial correlation is \\(r_{rb}\\) = .982 [.938, .995]. This suggests that nearly every individual in the sample showed an increase in condition 2 relative to condition 1. As you can see from the figure below, only one individual showed a decline (individual shown in red).\n\n\n\n\n\n\n\n12.3.1.2 Independent Groups\n\nCalculate the ranks for each observation across all observations of in group 1 and 2\n\n\\[\nR = \\text{rank}(X)\n\\]\n\nCalculate the rank sums from each group\n\n\\[\nU_1 = \\left(\\sum_{i=1}^{n_1} R_{1i}\\right) - n_1 \\cdot \\frac{n_1 + 1}{2}\n\\]\n\\[\nU_2 = \\left(\\sum_{i=1}^{n_2} R_{2i}\\right) - n_2 \\cdot \\frac{n_2 + 1}{2}\n\\]\n\nCalculate rank biserial correlation\n\n\\[\nr_{rb} = \\frac{U_1}{n_1 n_2} - \\frac{U_2}{n_1 n_2}\n\\]\n\nFor independent samples, the standard error is calculated as the following:\n\n\\[\nSE_{rb} = \\sqrt{\\frac {n_1 + n_2 + 1} { 3  n_1  n_2}}\n\\]\n\nThe confidence intervals can then be calculated by transforming the estimate.\n\n\\[\nZ_{rb} = \\text{arctanh}(r_{rb})\n\\]\n\nCalculate the standard error of the Z-transformed correlation\n\n\\[\nSE_{Z_{rb}} = \\frac{SE_{r_{rb}}}{1-r_{rb}^2}\n\\]\n\nThen the confidence interval can be calculated and then back-transformed.\n\n\\[\nCI_{r_{rb}} = \\text{tanh}(Z_{rb}  \\pm  1.96 \\cdot SE_{Z_{rb}})\n\\]\nIn R, we can use ses_calc in the TOSTER package can be utilized to calculate \\(r_{rb}\\).\n\n# Two Sample\n# install the janitor package for data cleaning\n# clean and import data from likert\ndata(mass, package = \"likert\")\ndf_mass = mass |&gt;\n  as.data.frame() |&gt;\n  janitor::clean_names() \n\n# function needs input as a numeric\n# ordered factors can be converted to ranks\n# Again, the warning can be ignored\nset.seed(24111)\nses_calc(\n  rank(math_relates_to_my_life) ~ gender,\n  data = df_mass,\n  paired = FALSE\n)\n\n                           estimate   lower.ci   upper.ci conf.level\nRank-Biserial Correlation -0.452381 -0.7831567 0.07794462       0.95\n\n\nThe example shows a rank-biserial correlation is \\(r_{rb}\\) = -.45 [-.78, .08].\n\n\n\n12.3.2 Concordance Probability\nIn the two sample case, concordance probability is the probability that a randomly chosen subject from one group has a response that is larger than that of a randomly chosen subject from the other group. In the two sample case, this is roughly equivalent to the statistic of the Brunner-Munzel test. In the paired sample case, it is the probability that a randomly chosen difference score (\\(D\\)) will have a positive (+) sign plus 0.5 times the probability of a tie (no/zero difference). The concordance probability can go by many names. It is also referred to as the c-index, the non-parametric probability of superiority, or the non-parametric common language effect size (CLES).\nThe calculation of concordance can be derived from the rank-biserial correlation. The concordance probability (\\(p_c\\)) can be converted from the correlation.\n\\[\np_c = \\frac{r_{rb} + 1 }{2}\n\\]\nIn R, we can use the ses_calc() function again along with the sleep data set. For repeated measures experiments, the concordance probability in dependent groups can be calculated utilizing the paired=TRUE argument in the ses_calc() function:\n\n# Dependent Groups\nlibrary(TOSTER)\n\ndata(sleep)\n\nses_calc(extra ~ group,\n         data = sleep,\n         paired = TRUE,\n         ses = \"c\")\n\n             estimate  lower.ci  upper.ci conf.level\nConcordance 0.9909091 0.9641845 0.9977392       0.95\n\n\nFor two independent groups, the concordance probability can be calculated similarly without specifying the paired argument:\n\n# Independent Groups\n# data import from likert\ndata(mass, package = \"likert\")\ndf_mass = mass |&gt;\n  as.data.frame() |&gt;\n  janitor::clean_names()\n\nses_calc(rank(math_relates_to_my_life) ~ gender,\n         data = df_mass,\n         ses = \"c\")\n\n             estimate  lower.ci  upper.ci conf.level\nConcordance 0.2738095 0.1084217 0.5389723       0.95\n\n\n\n\n12.3.3 Wilcoxon-Mann-Whitney Odds\nThe Wilcoxon-Mann-Whitney odds (O’Brien and Castelloe 2006), also known as the “Generalized Odds Ratio”(Agresti 1980), essentially transforms the concordance probability into an odds ratio.\nThe odds can be converted from the concordance by taking the logit of the concordance. This will provide the log odds.\n\\[\nO_{WMW} = \\exp \\left[\\text{logit}(p_c)\\right]\n\\]\nThe exponential value of the log-odds will provide the odds on a more interpretable scale. Taking just the logit of the concordance probability would give us the log odds such that,\n\\[\n\\log(O_{WMW}) = \\text{logit}(p_c)\n\\]\nIn R, we can calculate \\(O_{WMW}\\) by using the ses_calc() function from the TOSTER package:\n\n# Dependent Groups\n\ndata(sleep)\n\nTOSTER::ses_calc(extra ~ group,\n                       data = sleep,\n                       paired = TRUE,\n                 ses = \"odds\")\n\n         estimate lower.ci upper.ci conf.level\nWMW Odds      109 26.92087 441.3305       0.95\n\n\nWe can also calculate \\(O_{WMW}\\) in independent groups using the same function:\n\n# Independent Groups\n\n# data import from likert\ndata(mass, package = \"likert\")\ndf_mass = mass |&gt;\n  as.data.frame() |&gt;\n  janitor::clean_names()\n\nTOSTER::ses_calc(  rank(math_relates_to_my_life) ~ gender,\n  data = df_mass,\n                 ses = \"odds\")\n\n          estimate  lower.ci upper.ci conf.level\nWMW Odds 0.3770492 0.1216064 1.169067       0.95\n\n\n\n\n\n\nAgresti, Alan. 1980. “Generalized Odds Ratios for Ordinal Data.” Biometrics, 59–67.\n\n\nBrunner, Edgar, and Ullrich Munzel. 2000. “The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation.” Biometrical Journal 42 (1): 17–25. https://doi.org/10.1002/(SICI)1521-4036(200001)42:1&lt;17::AID-BIMJ17&gt;3.0.CO;2-U.\n\n\nCaldwell, Aaron R. 2022. “Exploring Equivalence Testing with the Updated TOSTER r Package.” PsyArXiv. https://doi.org/10.31234/osf.io/ty8de.\n\n\nCliff, Norman. 1993. “Dominance Statistics: Ordinal Analyses to Answer Ordinal Questions.” Psychological Bulletin 114 (3): 494.\n\n\nDivine, George W, H James Norton, Anna E Barón, and Elizabeth Juarez-Colunga. 2018. “The Wilcoxon–Mann–Whitney Procedure Fails as a Test of Medians.” The American Statistician 72 (3): 278–86.\n\n\nKarch, Julian D. 2021. “Psychologists Should Use Brunner-Munzel’s Instead of Mann-Whitney’s u Test as the Default Nonparametric Procedure.” Advances in Methods and Practices in Psychological Science 4 (2): 2515245921999602.\n\n\nLäkens, Daniel. 2017. “Equivalence Tests: A Practical Primer for t-Tests, Correlations, and Meta-Analyses.” Social Psychological and Personality Science 1: 1–8. https://doi.org/10.1177/1948550617697177.\n\n\nLiddell, Torrin M., and John K. Kruschke. 2018. “Analyzing Ordinal Data with Metric Models: What Could Possibly Go Wrong?” Journal of Experimental Social Psychology 79 (November): 328–48. https://doi.org/10.1016/j.jesp.2018.08.009.\n\n\nMunzel, Ullrich, and Edgar Brunner. 2002. “An Exact Paired Rank Test.” Biometrical Journal 44 (5): 584–93. https://doi.org/10.1002/1521-4036(200207)44:5&lt;584::AID-BIMJ584&gt;3.0.CO;2-9.\n\n\nNeubert, Karin, and Edgar Brunner. 2007. “A Studentized Permutation Test for the Non-Parametric Behrensfisher Problem.” Computational Statistics & Data Analysis 51 (10): 5192–5204. https://doi.org/10.1016/j.csda.2006.05.024.\n\n\nO’Brien, Ralph G, and John Castelloe. 2006. “Exploiting the Link Between the Wilcoxon-Mann-Whitney Test and a Simple Odds Statistic.” In Proceedings of the Thirty-First Annual SAS Users Group International Conference, 209–31. Citeseer."
  },
  {
    "objectID": "Non-Parametric-Effect-Sizes.html#footnotes",
    "href": "Non-Parametric-Effect-Sizes.html#footnotes",
    "title": "12  Non-Parametric Tests",
    "section": "",
    "text": "Note, for paired samples, this does not refer to the probability of an increase/decrease in paired sample but rather the probability that a randomly sampled value of X will be greater/less than Y. This is also referred to as the “relative” effect in the literature. Therefore, the results will differ from the concordance probability.↩︎"
  },
  {
    "objectID": "Regression.html#regression-overview",
    "href": "Regression.html#regression-overview",
    "title": "13  Regression",
    "section": "13.1 Regression Overview",
    "text": "13.1 Regression Overview\nIn a simple linear regression there is only one predictor (\\(x\\)) and one outcome (\\(y\\)) in the regression model,\n\\[\ny = b_0 + b_1 x + e\n\\]\nWe can visualize this model by showing data from the palmer penguins data package:\n\n\n\n\n\nwhere \\(b_0\\) is the intercept coefficient, \\(b_1\\) is the slope coefficient, and \\(e\\) is the error term that is normally distributed with a mean of zero and a variance of \\(\\sigma^2\\). For a simple linear regression we can obtain an unstandardized regression coefficient by finding the optimal value of \\(b_0\\) and \\(b_1\\) that minimizes the variance in \\(e\\), namely, \\(\\sigma^2\\). In a multiple regression we can model \\(y\\) as a function of multiple predictor variables such that,\n\\[\ny = b_0 + b_1 x_{1} + b_2 x_{2} +... + e\n\\] Where the coefficients are all optimized jointly to minimize the error variance. The line produced by the regression equation is our predicted values of \\(y_i\\), however it can also be interpreted as the mean of \\(y\\) given some value of \\(x\\). In a regression equation we can construct more complex models that include non-linear terms such as interactions or polynomials (or any sort of function of \\(x\\)). For example, we can create a model where we include a main effect, \\(x_1\\), a quadratic polynomial term, \\(x^2_1\\) and an interaction term, \\(x_1 x_2\\),\n\\[\ny_i = b_0 + b_1 x_{1} + b_2 x^2_{2}  + b_2 x_{1} x_{2} + e_i\n\\]"
  },
  {
    "objectID": "Regression.html#effect-sizes-for-a-linear-regression",
    "href": "Regression.html#effect-sizes-for-a-linear-regression",
    "title": "13  Regression",
    "section": "13.2 Effect Sizes for a Linear Regression",
    "text": "13.2 Effect Sizes for a Linear Regression\nIf we want to calculate the variance explained in the outcome by all the predictor variables, we can compute an \\(R^2\\) value. The \\(R^2\\) value can be interpreted one of two ways:\n\nthe variance in \\(y\\) explained by the predictor variables\nthe square of the correlation between predicted \\(y\\) values and observed (actual) \\(y\\) values\n\nLikewise we can also take the square root of \\(R^2\\) to get the correlation between predicted and observed \\(y\\) values. We can construct an linear regression model quite easily in base R using the lm() function. We will use the palmerpenguins dataset for our example.\n\nlibrary(palmerpenguins)\n\n\nmdl &lt;- lm(bill_length_mm ~ flipper_length_mm + bill_depth_mm, \n          data = penguins)\n\nsummary(mdl)\n\n\nCall:\nlm(formula = bill_length_mm ~ flipper_length_mm + bill_depth_mm, \n    data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8831  -2.7734  -0.3268   2.3128  19.7630 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -28.14701    5.51435  -5.104 5.54e-07 ***\nflipper_length_mm   0.30569    0.01902  16.073  &lt; 2e-16 ***\nbill_depth_mm       0.62103    0.13543   4.586 6.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.009 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4638,    Adjusted R-squared:  0.4607 \nF-statistic: 146.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\nWe will notice that the linear regression summary returns two \\(R^2\\) values. The first one is the traditional \\(R^2\\) and the other is the adjusted \\(R^2\\). The adjusted \\(R^2_\\text{adj}\\) applies a correction factor since \\(R^2\\) it is often bias when there are more predictor variables and a smaller sample size. If we want to know the contribution for each term in the regression model, we can also use semi-partial \\(sr^2\\) values that are similar to partial eta-squared in the ANOVA section of this book. In R, we can calculate \\(sr^2\\) with the r2_semipartial() function in the effectsize package (Ben-Shachar, Lüdecke, and Makowski 2020):\n\nlibrary(effectsize)\n\nr2_semipartial(mdl,alternative = \"two.sided\")\n\nTerm              |  sr2 |       95% CI\n---------------------------------------\nflipper_length_mm | 0.41 | [0.33, 0.49]\nbill_depth_mm     | 0.03 | [0.01, 0.06]\n\n\nA standardized effect size for each term could also be calculated from standardizing the regression coefficients. Standardized regression coefficients are calculated by re-scaling the predictor and outcome variables to be z-scores (i.e., setting the mean and variance to be zero and one, respectively).\n\nstand_mdl &lt;- lm(scale(bill_length_mm) ~ scale(flipper_length_mm) + scale(bill_depth_mm), \n                data = penguins)\n\nsummary(stand_mdl)\n\n\nCall:\nlm(formula = scale(bill_length_mm) ~ scale(flipper_length_mm) + \n    scale(bill_depth_mm), data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9934 -0.5080 -0.0599  0.4236  3.6199 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -2.328e-15  3.971e-02   0.000        1    \nscale(flipper_length_mm)  7.873e-01  4.899e-02  16.073  &lt; 2e-16 ***\nscale(bill_depth_mm)      2.246e-01  4.899e-02   4.586 6.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7344 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4638,    Adjusted R-squared:  0.4607 \nF-statistic: 146.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively, we can use the standardise function in the effectsize package:\n\nstandardise(mdl)\n\n\nCall:\nlm(formula = bill_length_mm ~ flipper_length_mm + bill_depth_mm, \n    data = data_std)\n\nCoefficients:\n      (Intercept)  flipper_length_mm      bill_depth_mm  \n        4.335e-16          7.873e-01          2.246e-01"
  },
  {
    "objectID": "Regression.html#pearson-correlation-vs-regression-coefficients-in-simple-linear-regressions",
    "href": "Regression.html#pearson-correlation-vs-regression-coefficients-in-simple-linear-regressions",
    "title": "13  Regression",
    "section": "13.3 Pearson correlation vs regression coefficients in simple linear regressions",
    "text": "13.3 Pearson correlation vs regression coefficients in simple linear regressions\nA slope coefficient in a simple linear regression model can be defined as the covariance between predictor \\(x\\) and outcome \\(y\\) divided by the variance in \\(x\\),\n\\[\nb_1 = \\frac{\\text{Cov}(x,y)}{S_x^2}\n\\]\nWhere \\(S_x\\) is the standard deviation of \\(x\\) (the square of the standard deviation is the variance). A Pearson correlation is defined as,\n\\[\nr = \\frac{\\text{Cov}(x,y)}{S_xS_y}\n\\]\nWe can see that these formulas are quite similar, in fact we can express \\(r\\) as a function of \\(b_1\\) such that,\n\\[\nr = b_1 \\frac{S_x}{S_y}\n\\]\nWhich means that if \\(S_x=S_y\\) then \\(r = b_1\\). Furthermore, if the regression coefficient is standardized this would make the outcome and predictor variable to both have a variance of 1, thus making \\(S_x=S_y = 1\\). Therefore a standardized regression coefficient is equal to a pearson correlation."
  },
  {
    "objectID": "Regression.html#multi-level-regression-models",
    "href": "Regression.html#multi-level-regression-models",
    "title": "13  Regression",
    "section": "13.4 Multi-Level Regression models",
    "text": "13.4 Multi-Level Regression models\nWe can allow the regression coefficients such as the intercept and slope to vary randomly with respect to some grouping variable. For example, lets say we think that the intercept will vary between the different species of penguins when we look at the relationship between body mass and bill depth. Using the lme4 package in R, we can construct a model that allows the intercept coefficient to vary between species.\n\nlibrary(palmerpenguins)\nlibrary(lme4)\n\n\nml_mdl &lt;- lmer(bill_length_mm ~ 1 + flipper_length_mm + (1 | species),\n            data = penguins)\nsummary(ml_mdl)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: bill_length_mm ~ 1 + flipper_length_mm + (1 | species)\n   Data: penguins\n\nREML criterion at convergence: 1640.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5568 -0.6666  0.0109  0.7020  4.7678 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 20.06    4.479   \n Residual              6.74    2.596   \nNumber of obs: 342, groups:  species, 3\n\nFixed effects:\n                  Estimate Std. Error t value\n(Intercept)        1.81165    4.97514   0.364\nflipper_length_mm  0.21507    0.02113  10.177\n\nCorrelation of Fixed Effects:\n            (Intr)\nflppr_lngt_ -0.854\n\n\nNote in the table that we have random effects and fixed effects. The random effects shows the grouping (categorical) variable that the parameter is allowed to vary on and then it shows the parameter that is varying, which in our case is the intercept coefficient. It also includes the variance of the intercept, which is the extent to which the intercept varies between species. For the fixed effect terms, we see the intercept displayed as well as the slope, this shows the mean of the intercept across species and, since the slope is equal across species, the slope is just a single value. Let’s visualize how this model looks:\n\n\n\n\n\nNotice that in the plot above the slopes are fixed and equal between each species and only the intercepts (i.e., the vertical height of each line) differs. We can also allow the slope to vary if we may choose by editing the formula:\n\nlibrary(palmerpenguins)\nlibrary(lme4)\n\n\nml_mdl &lt;- lmer(bill_length_mm ~ 1 + flipper_length_mm + (1 + flipper_length_mm | species),\n            data = penguins)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n\nsummary(ml_mdl)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: bill_length_mm ~ 1 + flipper_length_mm + (1 + flipper_length_mm |  \n    species)\n   Data: penguins\n\nREML criterion at convergence: 1638.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6326 -0.6657  0.0083  0.6843  4.9531 \n\nRandom effects:\n Groups   Name              Variance  Std.Dev. Corr \n species  (Intercept)       3.0062118 1.73384       \n          flipper_length_mm 0.0007402 0.02721  -0.61\n Residual                   6.6886861 2.58625       \nNumber of obs: 342, groups:  species, 3\n\nFixed effects:\n                  Estimate Std. Error t value\n(Intercept)        1.56035    4.32870   0.360\nflipper_length_mm  0.21609    0.02623   8.237\n\nCorrelation of Fixed Effects:\n            (Intr)\nflppr_lngt_ -0.863\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n\n\nVarying the slope will include flipper_length_mm in the random effects terms. Also note that the summary returns the correlation between random effect terms, which may be useful to know if there is a strong relationship between the intercept and slope across species. Now we see that the random effects terms now include the slope coefficient corresponding to the flipper_length_mm predictor variable. Let’s visualize\n\n\n\n\n\nThe plot above shows slight variation in the slope between the three species, however the slope does not vary all that much. For multi-level models we can compute a conditional \\(R^2\\) and a marginal \\(R^2\\) which are each described below\n\nMarginal \\(R^2\\): the variance explained solely by the fixed effects\nConditional \\(R^2\\): the variance explained in the whole model, including both the fixed effects and random effects terms.\n\nIn R, we can use the MuMIn package (Bartoń 2023) to compute both the marginal and conditional \\(R^2\\):\n\nlibrary(MuMIn)\n\nr.squaredGLMM(ml_mdl)\n\n           R2m       R2c\n[1,] 0.2470201 0.8210591\n\n\n\n\n\n\nBartoń, Kamil. 2023. MuMIn: Multi-Model Inference. https://CRAN.R-project.org/package=MuMIn.\n\n\nBen-Shachar, Mattan S., Daniel Lüdecke, and Dominique Makowski. 2020. “effectsize: Estimation of Effect Size Indices and Standardized Parameters.” Journal of Open Source Software 5 (56): 2815. https://doi.org/10.21105/joss.02815."
  },
  {
    "objectID": "Artifacts-and-Bias.html#resources",
    "href": "Artifacts-and-Bias.html#resources",
    "title": "14  Artifacts and Bias in Effect Sizes",
    "section": "14.1 Resources",
    "text": "14.1 Resources\nEffect size estimates such as correlation coefficients and Cohen’s \\(d\\) values can be severely biased due to various statistical artifacts such as measurement error and selection effects (e.g., range restriction). Methods have been developed to correct for the bias in effect sizes and thus these corrections are called “artifact corrections”. Artifact correction formulas can be complex and therefore readers are referred to other resources listed below:\n\nJané (2023) : An open-access textbook that contains equations and R code for various types of artifact corrections. Not yet released.\nHunter and Schmidt (1990) : Classic textbook on the topic of artifact corrections. Hunter and Schmidt pioneered the methodology for artifact correction style meta-analyses.\nWiernik and Dahlke (2020) : A paper that serves as a condensed version of Hunter and Schmidt’s book. It contains most of the equations necessary to correct effect sizes.\nDahlke and Wiernik (2019) : An R package for conducting artifact correction meta-analyses. Contains all the functions one would need to correct effect sizes for artifacts in R."
  },
  {
    "objectID": "Artifacts-and-Bias.html#correcting-for-measurement-error",
    "href": "Artifacts-and-Bias.html#correcting-for-measurement-error",
    "title": "14  Artifacts and Bias in Effect Sizes",
    "section": "14.2 Correcting for Measurement Error",
    "text": "14.2 Correcting for Measurement Error\nIf we have reliability estimates of the variables of interest, we can correct a Pearson correlation or a standardized mean difference (Cohen’s \\(d\\)) for measurement error. Non-differential measurement error attenuates Pearson correlations and Cohen’s \\(d\\) therefore we can apply correction factors to adjust for this bias. For a pearson correlation, we can use the correction for attenuation first developed by Spearman (1904),\n\\[\nr_c  = \\frac{r_\\text{obs}}{\\sqrt{r_{xx'}r_{yy'}}}\n\\tag{14.1}\\] where \\(r_c\\) is the corrected correlation, \\(r_\\text{obs}\\) is the observed correlation, \\(r_{xx'}\\) is the reliability of \\(x\\), and \\(r_{yy'}\\) is the reliability of \\(y\\). reliability coefficients can be estimated a number of different ways however the two of the most common estimators is Cronbach Alpha and Test-retest reliability. Alpha measures the internal consistency of a set of sub-component measurements (e.g., question responses on a questionnaire) while test-retest reliability measures the stability over time.\nA Cohen’s \\(d\\) can be corrected similarly to a correlation coefficient, however since it only has one continuous variable we can just correct for reliability in the continuous variable\n\\[\nd_c  = \\frac{d_\\text{obs}}{\\sqrt{r_{yy'}}}\n\\] However in the case of a Cohen’s d, it is important that \\(r_{yy'}\\) is the pooled within-group reliability (calculate pooled reliability the same way you calculate the pooled standard deviation for denominator of Cohen’s \\(d\\)). If all you have is the total sample reliability (more commonly reported) you can follow this three step process (Wiernik and Dahlke 2020),\n\nConvert the d value to a point-biserial correlation (see section on conversions)\nCorrect the point-biserial correlation using Equation 14.1 (setting \\(r_{xx'}=1\\))\nConvert it back to a Cohen’s \\(d\\)\n\nNote that confidence intervals for \\(r_c\\) and \\(d_c\\) must also be corrected. For example, a pearson correlation would need to be corrected such that, \\[\nCI_{r_c} = \\left[\\frac{r_\\text{lower-bound}}{\\sqrt{r_{xx'}r_{yy'}}},\\frac{r_\\text{upper-bound}}{\\sqrt{r_{xx'}r_{yy'}}}\\right]\n\\]"
  },
  {
    "objectID": "Artifacts-and-Bias.html#correcting-for-range-restriction",
    "href": "Artifacts-and-Bias.html#correcting-for-range-restriction",
    "title": "14  Artifacts and Bias in Effect Sizes",
    "section": "14.3 Correcting for Range Restriction",
    "text": "14.3 Correcting for Range Restriction\nRange restriction corrections can be quite complex depending on the selection process. The process for correcting Pearson correlations and Cohen’s \\(d\\) for range restriction is laid out in table 3 of Wiernik and Dahlke (2020).\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “psychmeta: An r Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nJané, Matthew B. 2023. Artifact Corrections for Effect Sizes: Implementation in r and Application to Meta-Analysis. (n.p.). https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "Converting-to-Cohens-d.html#from-independent-samples-t-statistic",
    "href": "Converting-to-Cohens-d.html#from-independent-samples-t-statistic",
    "title": "15  Converting to Cohen’s \\(d\\)",
    "section": "15.1 From Independent Samples \\(t\\)-statistic",
    "text": "15.1 From Independent Samples \\(t\\)-statistic\nTo calculate a between subject standardized mean difference (\\(d_p\\), i.e., pooled standard deviation standardizer), we can use the sample size in each group (\\(n_1\\) and \\(n_2\\)) as well as the \\(t\\)-statistic from an independent sample t-test and plug it into the following formula:\n\\[\nd_{p} = t\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2} }\n\\]\nUsing the t_to_d function in the effectsize package we can convert \\(t\\) to \\(d_p\\).\n\n# Example:\n# unpaired t-statistic = 3.25\n# n1 = 50, n2 = 40\n\nlibrary(effectsize)\n\nt &lt;- 3.25\nn1 &lt;- 50\nn2 &lt;- 40\n\nt_to_d(t, df_error = n1+n2-2, paired = FALSE)\n\nd    |       95% CI\n-------------------\n0.69 | [0.26, 1.12]"
  },
  {
    "objectID": "Converting-to-Cohens-d.html#from-paired-sample-t-statistic",
    "href": "Converting-to-Cohens-d.html#from-paired-sample-t-statistic",
    "title": "15  Converting to Cohen’s \\(d\\)",
    "section": "15.2 From Paired Sample \\(t\\)-statistic",
    "text": "15.2 From Paired Sample \\(t\\)-statistic\nTo calculate a within-subject standardized mean difference (\\(d_z\\), i.e., difference score standardizer), we can use the sample size in each group (\\(n_1\\) and \\(n_2\\)) as well as the \\(t\\)-statistic from an paired sample t-test and plug it into the following formula:\n\\[\nd_{z} = \\frac{t}{\\sqrt{n}}\n\\]\nUsing the t_to_d function in the effectsize package we can convert \\(t\\) to \\(d_z\\).\n\n# Example:\n# paired t-statistic = 3.25\n# n = 50\n\nt &lt;- 3.25\nn &lt;- 50\n\nt_to_d(t, df_error = n-1, paired = TRUE)\n\nd    |       95% CI\n-------------------\n0.46 | [0.17, 0.76]"
  },
  {
    "objectID": "Converting-to-Cohens-d.html#from-pearson-correlation",
    "href": "Converting-to-Cohens-d.html#from-pearson-correlation",
    "title": "15  Converting to Cohen’s \\(d\\)",
    "section": "15.3 From Pearson Correlation",
    "text": "15.3 From Pearson Correlation\nIf a Pearson correlation is calculated between a continuous score and a dichotomous score, this is considered a point-biserial correlation. The point-biserial correlation can be converted into a \\(d_p\\) value using the following formula:\n\\[\nd_p = \\frac{r}{\\sqrt{1-r^2}} \\sqrt{\\frac{n_1+n_2-2}{n_1} + \\frac{n_1+n_2-2}{n_2}}\n\\] Or if sample sizes within each group are unknown (or equal), the equatio simplifies to be approximately,\n\\[\nd_p \\approx \\frac{r\\sqrt{4}}{\\sqrt{1-r^2}}\n\\]\nUsing the r_to_d function in the effectsize package we can convert \\(r\\) to \\(d_p\\).\n\n# Example:\n# r = 3.25\n# n1 = 50, n2 = 40\n\nr &lt;- .50\nn1 &lt;- 50\nn2 &lt;- 40\n\nr_to_d(r = r, n1 = n1, n2 = n2)\n\n[1] 1.148913"
  },
  {
    "objectID": "Converting-to-Cohens-d.html#from-odds-ratio",
    "href": "Converting-to-Cohens-d.html#from-odds-ratio",
    "title": "15  Converting to Cohen’s \\(d\\)",
    "section": "15.4 From Odds-Ratio",
    "text": "15.4 From Odds-Ratio\nAn odds-ratio from a contingency table can also be converted to a \\(d_p\\). Note that this formula is an approximation:\n\\[\nd_{p} = \\frac{\\log(OR)\\sqrt{3}}{\\pi}\n\\]\nUsing the oddsratio_to_d function in the effectsize package we can convert \\(OR\\) to \\(d_p\\).\n\n# Example:\n# OR = 1.62\n\nOR &lt;- 1.46\n\noddsratio_to_d(OR = OR)\n\n[1] 0.2086429"
  },
  {
    "objectID": "Converting-to-Correlation.html#from-t-statistic",
    "href": "Converting-to-Correlation.html#from-t-statistic",
    "title": "16  Converting to Pearson Correlation",
    "section": "16.1 From \\(t\\)-statistic",
    "text": "16.1 From \\(t\\)-statistic\nFrom a \\(t\\) statistic calculated from a correlational test, we can calculate the correlation coefficient using the following formula:\n\\[\nr = \\sqrt{\\frac{t^2}{t^2 + n-2}}\n\\]\nUsing the t_to_r function in the effectsize package we can convert \\(t\\) to \\(r\\).\n\n# Example:\n# t = 4.14, n = 50\n\nlibrary(effectsize)\n\nt &lt;- 4.14\nn &lt;- 50\n\nt_to_r(t = t, df = n-2)\n\nr    |       95% CI\n-------------------\n0.51 | [0.28, 0.67]"
  },
  {
    "objectID": "Converting-to-Correlation.html#from-cohens-d",
    "href": "Converting-to-Correlation.html#from-cohens-d",
    "title": "16  Converting to Pearson Correlation",
    "section": "16.2 From Cohen’s \\(d\\)",
    "text": "16.2 From Cohen’s \\(d\\)\nFrom a between groups Cohen’s \\(d\\) value (\\(d_p\\)), we can calculate the correlation coefficient from the following formula:\n\\[\nr = \\frac{d_p}{\\sqrt{d_p^2+\\frac{n_1+n_2-2}{n_1} + \\frac{n_1+n_2-2}{n_2}}}\n\\]\nUsing the d_to_r function in the effectsize package we can convert \\(d_p\\) to \\(r\\).\n\n# Example:\n# d = 0.60, n1 = 50, n2 = 70\n\nd &lt;- 0.60\nn1 &lt;- 50\nn2 &lt;- 70\n\nd_to_r(d = d, n1 = n1, n2 = n2)\n\n[1] 0.2858532"
  },
  {
    "objectID": "Converting-to-Correlation.html#from-odds-ratio",
    "href": "Converting-to-Correlation.html#from-odds-ratio",
    "title": "16  Converting to Pearson Correlation",
    "section": "16.3 From Odds-Ratio",
    "text": "16.3 From Odds-Ratio\nThe correlation coefficient from an odds ratio can be calculated with the following formula:\n\\[\nr = \\frac{\\log(OR)\\times\\sqrt{3}}{\\pi\\sqrt{\\frac{3\\log(OR)^2}{\\pi^2}+\\frac{n_1+n_2-2}{n_1} + \\frac{n_1+n_2-2}{n_2}}}\n\\]\nUsing the oddsratio_to_r function in the effectsize package we can convert \\(OR\\) to \\(r\\).\n\n# Example:\n# OR = 2.21, n1 = 50, n2 = 70\n\nOR &lt;- 2.21\nn1 &lt;- 50\nn2 &lt;- 70\n\noddsratio_to_r(OR=OR, n1 = n1, n2 = n2)\n\n[1] 0.2124017"
  },
  {
    "objectID": "Converting-to-Odds-Ratio.html#from-cohens-d",
    "href": "Converting-to-Odds-Ratio.html#from-cohens-d",
    "title": "17  Converting to Odds Ratio",
    "section": "17.1 From Cohen’s \\(d\\)",
    "text": "17.1 From Cohen’s \\(d\\)\nWe can calculate an odds-ratio from a between groups cohen’s \\(d\\) (\\(d_p\\)):\n\\[\nOR = \\exp\\left(\\frac{d_p \\pi}{\\sqrt{3}}\\right)\n\\]\nWhere \\(\\exp(\\cdot)\\) is an exponential transformation (this inverses the logarithm). Using the d_to_oddsratio function in the effectsize package we can convert \\(d\\) to \\(OR\\).\n\n# Example:\n# d = 0.60, n1 = 50, n2 = 70\n\nlibrary(effectsize)\n\nd &lt;- 0.60\nn1 &lt;- 50\nn2 &lt;- 70\n\nd_to_oddsratio(d = d, n1 = n1, n2 = n2)\n\n[1] 2.969162"
  },
  {
    "objectID": "Converting-to-Odds-Ratio.html#from-a-pearson-correlation",
    "href": "Converting-to-Odds-Ratio.html#from-a-pearson-correlation",
    "title": "17  Converting to Odds Ratio",
    "section": "17.2 From a Pearson Correlation",
    "text": "17.2 From a Pearson Correlation\nWe can calculate an odds ratio from a Pearson correlation using the following formula:\n\\[\nOR = \\exp\\left(\\frac{r\\pi \\sqrt{\\frac{n_1+n_2-2}{n_1} + \\frac{n_1+n_2-2}{n_2}}}{\\sqrt{3(1-r^2)}}\\right)\n\\]\nWhen sample sizes are equal, this equation can be simplified to be approximately,\n\\[\nOR = \\exp\\left(\\frac{r\\pi \\sqrt{4}}{\\sqrt{3(1-r^2)}}\\right)\n\\]\nUsing the r_to_oddsratio function in the effectsize package we can convert \\(d\\) to \\(OR\\).\n\n# Example:\n# r = .50, n1 = 50, n2 = 70\n\nr &lt;- .40\nn1 &lt;- 50\nn2 &lt;- 70\n\nr_to_oddsratio(r = r, n1 = n1, n2 = n2)\n\n[1] 4.870584"
  },
  {
    "objectID": "Conclusion.html#limitations-and-future-directions",
    "href": "Conclusion.html#limitations-and-future-directions",
    "title": "18  Conclusion",
    "section": "18.1 Limitations and Future Directions",
    "text": "18.1 Limitations and Future Directions\nWhile this guide covers a wide range of effect size and confidence interval methods, there are some limitations to note. First, our instructions focus specifically on applications in behavioral, cognitive, and social science research. The techniques may need to be adapted for other scientific domains. Second, we only cover free and open source options, so proprietary software packages are not discussed. Finally, as new methods and R packages arise, the guide will need to be continually updated, perhaps in a similar manner as Parsons et al. (2022) Open Scholarship terms after publication.\nIn the future, we aim to expand the guide by collaborating with experts in other fields to include discipline-specific recommendations. We also plan to incorporate new R packages and techniques as they emerge. Readers are encouraged to consult the cited packages’ documentation and peer-reviewed sources to further explore limitations and assumptions of the covered techniques."
  },
  {
    "objectID": "Conclusion.html#conclusion",
    "href": "Conclusion.html#conclusion",
    "title": "18  Conclusion",
    "section": "18.2 Conclusion",
    "text": "18.2 Conclusion\nRobust quantification of study results is a central pillar of open and reproducible science. With this collaborative collection of applied instructions, our guide aims to make calculating effect sizes and confidence intervals more accessible. We hope these resources empower both young researchers and experienced scholars across a variety of disciplines to incorporate these crucial statistical practices into their workflows. In our view, more widespread and thoughtful adoption of these methods will greatly strengthen the collective rigor, transparency, and impact of scientific research."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 1980. “Generalized Odds Ratios for Ordinal\nData.” Biometrics, 59–67.\n\n\nAlgina, James, and H. J. Keselman. 2003. “Approximate Confidence\nIntervals for Effect Sizes.” Educational and Psychological\nMeasurement 63 (4): 537–53. https://doi.org/10.1177/0013164403256358.\n\n\nAnvari, Farid, and Daniël Lakens. 2021. “Using Anchor-Based\nMethods to Determine the Smallest Effect Size of Interest.”\nJournal of Experimental Social Psychology 96: 104159.\n\n\nAPA. 2010. Publication Manual of the American Psychological\nAssociation. American Psychological Association. https://thuvienso.hoasen.edu.vn/handle/123456789/8327.\n\n\nBaayen, R Harald, Douglas J Davidson, and Douglas M Bates. 2008.\n“Mixed-Effects Modeling with Crossed Random Effects for Subjects\nand Items.” Journal of Memory and Language 59 (4):\n390–412.\n\n\nBaguley, Thom. 2009. “Standardized or Simple Effect Size: What\nShould Be Reported?” British Journal of Psychology 100\n(3): 603–17.\n\n\nBarr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013.\n“Random Effects Structure for Confirmatory Hypothesis Testing:\nKeep It Maximal.” Journal of Memory and Language 68 (3):\n255–78.\n\n\nBartoń, Kamil. 2023. MuMIn: Multi-Model Inference. https://CRAN.R-project.org/package=MuMIn.\n\n\nBeck, Edward C., Anirudh K. Gowd, Joseph N. Liu, Brian R. Waterman,\nKristen F. Nicholson, Brian Forsythe, Adam B. Yanke, Brian J. Cole, and\nNikhil N. Verma. 2020. “How Is Maximum Outcome Improvement Defined\nin Patients Undergoing Shoulder Arthroscopy for Rotator Cuff Repair? A\n1-Year Follow-up Study.” Arthroscopy: The Journal of\nArthroscopic & Related Surgery 36 (7): 1805–10. https://doi.org/10.1016/j.arthro.2020.02.047.\n\n\nBecker, Betsy J. 1988. “Synthesizing Standardized Mean-Change\nMeasures - UConn Library.” British Journal of Mathematical\nand Statistical Psychology 41 (2): 257278. https://doi.org/https://doi.org/10.1111/j.2044-8317.1988.tb00901.x.\n\n\nBen-Shachar, Mattan S., Daniel Lüdecke, and Dominique Makowski. 2020.\n“effectsize: Estimation of Effect Size\nIndices and Standardized Parameters.” Journal of Open Source\nSoftware 5 (56): 2815. https://doi.org/10.21105/joss.02815.\n\n\nBen-Shachar, Mattan S., Indrajeet Patil, Rémi Thériault, Brenton M.\nWiernik, and Daniel Lüdecke. 2023. “Phi, Fei, Fo, Fum: Effect\nSizes for Categorical Data That Use the Chi-Squared Statistic.”\nMathematics 11 (9): 1982. https://doi.org/10.3390/math11091982.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.\n2019. “Declaring and Diagnosing Research Designs.”\nAmerican Political Science Review 113: 838–59. https://declaredesign.org/paper.pdf.\n\n\nBonini, Matteo, Marcello Di Paolo, Diego Bagnasco, Ilaria Baiardini,\nFulvio Braido, Marco Caminati, Elisiana Carpagnano, et al. 2020.\n“Minimal Clinically Important Difference for Asthma Endpoints: An\nExpert Consensus Report.” European Respiratory Review 29\n(156).\n\n\nBosco, Frank A., Herman Aguinis, Kulraj Singh, James G. Field, and\nCharles A. Pierce. 2015. “Correlational Effect Size\nBenchmarks.” Journal of Applied Psychology 100 (2):\n431–49. https://doi.org/10.1037/a0038047.\n\n\nBrunner, Edgar, and Ullrich Munzel. 2000. “The Nonparametric\nBehrens-Fisher Problem: Asymptotic Theory and a Small-Sample\nApproximation.” Biometrical Journal 42 (1): 17–25. https://doi.org/10.1002/(SICI)1521-4036(200001)42:1&lt;17::AID-BIMJ17&gt;3.0.CO;2-U.\n\n\nBuchanan, Erin M., Amber Gillenwaters, John E. Scofield, and K. D.\nValentine. 2019. MOTE: Measure of the\nEffect: Package to Assist in Effect Size Calculations and Their\nConfidence Intervals. http://github.com/doomlab/MOTE.\n\n\nCaldwell, Aaron R. 2022. “Exploring Equivalence Testing with the\nUpdated TOSTER r Package.” PsyArXiv. https://doi.org/10.31234/osf.io/ty8de.\n\n\nCliff, Norman. 1993. “Dominance Statistics: Ordinal Analyses to\nAnswer Ordinal Questions.” Psychological Bulletin 114\n(3): 494.\n\n\nCoe, R. 2012. “It’s the Effect Size, Stupid What Effect Size Is\nand Why It Is Important.” In. https://www.semanticscholar.org/paper/It%27s-the-Effect-Size%2C-Stupid-What-effect-size-is-it-Coe/c5ac87df5d6e0e6b6de2f745284835c2a368b0f7.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. Academic Press.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “psychmeta: An r Package for Psychometric\nMeta-Analysis.” Applied Psychological Measurement 43\n(5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nDaste, Camille, Hendy Abdoul, Frantz Foissac, Marie-Martine\nLefèvre-Colau, Serge Poiraudeau, François Rannou, and Christelle Nguyen.\n2022. “Patient Acceptable Symptom State for Patient-Reported\nOutcomes in People with Non-Specific Chronic Low Back Pain.”\nAnnals of Physical and Rehabilitation Medicine 65 (1): 101451.\nhttps://doi.org/10.1016/j.rehab.2020.10.005.\n\n\nDivine, George W, H James Norton, Anna E Barón, and Elizabeth\nJuarez-Colunga. 2018. “The Wilcoxon–Mann–Whitney Procedure Fails\nas a Test of Medians.” The American Statistician 72 (3):\n278–86.\n\n\nFaul, Franz, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009.\n“Statistical Power Analyses Using G*Power 3.1: Tests for\nCorrelation and Regression Analyses.” Behavior Research\nMethods 41 (4): 1149–60. https://doi.org/10.3758/BRM.41.4.1149.\n\n\nFritz, Catherine O., Peter E. Morris, and Jennifer J. Richler. 2012.\n“Effect Size Estimates: Current Use, Calculations, and\nInterpretation.” Journal of Experimental Psychology:\nGeneral 141 (1): 2–18. https://doi.org/10.1037/a0024338.\n\n\nFunder, David C., and Daniel J. Ozer. 2019. “Evaluating Effect\nSize in Psychological Research: Sense and Nonsense.” Advances\nin Methods and Practices in Psychological Science 2 (2): 156–68. https://doi.org/10.1177/2515245919847202.\n\n\nGelman, Andrew. 2011. “Why It Doesn’t Make Sense in\nGeneral to Form Confidence Intervals by Inverting Hypothesis Tests |\nStatistical Modeling, Causal Inference, and Social Science.” https://statmodeling.stat.columbia.edu/2011/08/25/why_it_doesnt_m/.\n\n\nGignac, Gilles E., and Eva T. Szodorai. 2016. “Effect Size\nGuidelines for Individual Differences Researchers.”\nPersonality and Individual Differences 102 (November): 74–78.\nhttps://doi.org/10.1016/j.paid.2016.06.069.\n\n\nGlass, Gene V. 1981. “Meta-Analysis in Social Research.”\n(No Title). https://cir.nii.ac.jp/crid/1130000795088566912.\n\n\nGlass, Gene V., Barry McGaw, and Mary L. Smith. 1981.\n“Meta-Analysis in Social Research.” (No Title). https://cir.nii.ac.jp/crid/1130000795088566912.\n\n\nGuilford, J. P. 1965. “The Minimal Phi Coefficient and the Maximal\nPhi.” Educational and Psychological Measurement 25 (1):\n3–8. https://doi.org/10.1177/001316446502500101.\n\n\nHarrell, Frank. 2020. “Author Checklist - Data Analysis.”\nhttps://discourse.datamethods.org/t/author-checklist/3407.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator\nof Effect Size and Related Estimators.” Journal of\nEducational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\nHEIJDE, DÉSIRÉE van der, MARISSA Lassere, JOHN Edmonds, JOHN Kirwan,\nVIBEKE Strand, and Maarten Boers. 2001. “Minimal Clinically\nImportant Difference in Plain Films in RA: Group Discussions,\nConclusions, and Recommendations. OMERACT Imaging Task Force.”\nThe Journal of Rheumatology 28 (4): 914–17.\n\n\nHoekstra, Rink, Richard D. Morey, Jeffrey N. Rouder, and Eric-Jan\nWagenmakers. 2014. “Robust Misinterpretation of Confidence\nIntervals.” Psychonomic Bulletin & Review 21 (5):\n1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://doi.org/10.5281/zenodo.3960218.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of\nmeta-analysis: correcting error and bias in research findings.\nNewbury Park: Sage Publications.\n\n\nJané, Matthew B. 2023. Artifact Corrections for Effect Sizes:\nImplementation in r and Application to Meta-Analysis. (n.p.). https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/.\n\n\nKarch, Julian D. 2021. “Psychologists Should Use Brunner-Munzel’s\nInstead of Mann-Whitney’s u Test as the Default Nonparametric\nProcedure.” Advances in Methods and Practices in\nPsychological Science 4 (2): 2515245921999602.\n\n\nKassambara, Alboukadel. 2019. Datarium: Data Bank for Statistical\nAnalysis and Visualization. https://CRAN.R-project.org/package=datarium.\n\n\nKelley, Ken. 2022. MBESS: The MBESS r Package. https://CRAN.R-project.org/package=MBESS.\n\n\nKelley, Ken, and Kristopher J. Preacher. 2012. “On Effect\nSize.” Psychological Methods 17 (2): 137–52. https://doi.org/10.1037/a0028086.\n\n\nKirby, Kris N, and Daniel Gerlanc. 2013. “BootES: An r Package for\nBootstrap Confidence Intervals on Effect Sizes.” Behavior\nResearch Methods 45: 905–27.\n\n\nLakens, Daniël. 2013. “Calculating and Reporting Effect Sizes to\nFacilitate Cumulative Science: A Practical Primer for t-Tests and\nANOVAs.” Frontiers in Psychology 4. https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863.\n\n\n———. 2014. “The 20.” http://daniellakens.blogspot.com/2014/06/calculating-confidence-intervals-for.html.\n\n\n———. 2022. “Sample Size Justification.” Collabra:\nPsychology 8 (1): 33267. https://doi.org/10.1525/collabra.33267.\n\n\nLakens, Daniël, Anne M Scheel, and Peder M Isager. 2018.\n“Equivalence Testing for Psychological Research: A\nTutorial.” Advances in Methods and Practices in Psychological\nScience 1 (2): 259–69.\n\n\nLäkens, Daniel. 2017. “Equivalence Tests: A Practical Primer for\nt-Tests, Correlations, and Meta-Analyses.” Social\nPsychological and Personality Science 1: 1–8. https://doi.org/10.1177/1948550617697177.\n\n\nLiddell, Torrin M., and John K. Kruschke. 2018. “Analyzing Ordinal\nData with Metric Models: What Could Possibly Go Wrong?”\nJournal of Experimental Social Psychology 79 (November):\n328–48. https://doi.org/10.1016/j.jesp.2018.08.009.\n\n\nLovakov, Andrey, and Elena R. Agadullina. 2021. “Empirically\nDerived Guidelines for Effect Size Interpretation in Social\nPsychology.” European Journal of Social Psychology 51\n(3): 485–504. https://doi.org/10.1002/ejsp.2752.\n\n\nLüdecke, Daniel. 2019. Esc: Effect Size Computation for Meta\nAnalysis (Version 0.5.1). https://doi.org/10.5281/zenodo.1249218.\n\n\nMagnusson, Kristoffer. 2023. “A Causal Inference Perspective on\nTherapist Effects.”\n\n\nMcGlothlin, Anna E., and Roger J. Lewis. 2014. “Minimal Clinically\nImportant Difference: Defining What Really Matters to Patients.”\nJAMA 312 (13): 1342–43. https://doi.org/10.1001/jama.2014.13128.\n\n\nMeehl, Paul E. 1984. “Radical Behaviorism and Mental Events: Four\nMethodological Queries.” Behavioral and Brain Sciences 7\n(4): 563–64. https://doi.org/10.1017/S0140525X00027308.\n\n\nMorey, Richard D., Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and\nEric-Jan Wagenmakers. 2016. “The Fallacy of Placing Confidence in\nConfidence Intervals.” Psychonomic Bulletin & Review\n23 (1): 103–23. https://doi.org/10.3758/s13423-015-0947-8.\n\n\nMorris, Scott B. 2008. “Estimating Effect Sizes From\nPretest-Posttest-Control Group Designs.” Organizational\nResearch Methods 11 (2): 364–86. https://doi.org/10.1177/1094428106291059.\n\n\nMorse, David. 2018. “How to Calculate Degrees of Freedom When\nUsing Two Way ANOVA with Unequal Sample Size?”\n\n\nMunzel, Ullrich, and Edgar Brunner. 2002. “An Exact Paired Rank\nTest.” Biometrical Journal 44 (5): 584–93. https://doi.org/10.1002/1521-4036(200207)44:5&lt;584::AID-BIMJ584&gt;3.0.CO;2-9.\n\n\nNeubert, Karin, and Edgar Brunner. 2007. “A Studentized\nPermutation Test for the Non-Parametric Behrensfisher\nProblem.” Computational Statistics & Data Analysis\n51 (10): 5192–5204. https://doi.org/10.1016/j.csda.2006.05.024.\n\n\nO’Brien, Ralph G, and John Castelloe. 2006. “Exploiting the Link\nBetween the Wilcoxon-Mann-Whitney Test and a Simple Odds\nStatistic.” In Proceedings of the Thirty-First Annual SAS\nUsers Group International Conference, 209–31. Citeseer.\n\n\nOlkin, Ingram, and Jeremy D. Finn. 1995. “Correlations\nRedux.” Psychological Bulletin 118 (1): 155–64. https://doi.org/10.1037/0033-2909.118.1.155.\n\n\nOrben, Amy, and Daniël Lakens. 2020. “Crud (Re)Defined.”\nAdvances in Methods and Practices in Psychological Science 3\n(2): 238–47. https://doi.org/10.1177/2515245920917961.\n\n\nOtgaar, Henry, Paul Riesthuis, Tess Neal, Jason Chin, Irena Boskovic,\nand Eric Rassin. 2023. “If Generalization Is the Grail, Practical\nRelevance Is the Nirvana: Considerations from the Contribution of\nPsychological Science of Memory to Law.” Henry Otgaar, Paul\nRiesthuis, Tess MS Neal, Jason M. Chin, Irena Boskovic & Eric\nRassin,“If Generalization Is the Grail, Practical Relevance Is the\nNirvana: Considerations from the Contribution of Psychological Science\nof Memory to Law”(accepted 2023) Journal of Applied Research in\nMemory and Co.\n\n\nOtgaar, Henry, Paul Riesthuis, Johannes G Ramaekers, Maryanne Garry, and\nLilian Kloft. 2022. “The Importance of the Smallest Effect Size of\nInterest in Expert Witness Testimony on Alcohol and Memory.”\nFrontiers in Psychology 13: 980533.\n\n\nPanzarella, Emily, Nataly Beribisky, and Robert A Cribbie. 2021.\n“Denouncing the Use of Field-Specific Effect Size Distributions to\nInform Magnitude.” PeerJ 9: e11383.\n\n\nPaterson, Ted A., P. D. Harms, Piers Steel, and Marcus Credé. 2016.\n“An Assessment of the Magnitude of Effect Sizes: Evidence From 30\nYears of Meta-Analysis in Management.” Journal of Leadership\n& Organizational Studies 23 (1): 66–81. https://doi.org/10.1177/1548051815614321.\n\n\nPeters, Gjalt-Jorn Ygram, and Stefan Gruijters. 2023. Ufs: A\nCollection of Utilities. https://ufs.opens.science.\n\n\nPogrow, Stanley. 2019. “How Effect Size (Practical Significance)\nMisleads Clinical Practice: The Case for Switching to Practical Benefit\nto Assess Applied Research Findings.” The American\nStatistician 73 (sup1): 223–34. https://doi.org/10.1080/00031305.2018.1549101.\n\n\nRichard, F. D., Charles F. Bond Jr., and Juli J. Stokes-Zoota. 2003.\n“One Hundred Years of Social Psychology Quantitatively\nDescribed.” Review of General Psychology 7 (4): 331–63.\nhttps://doi.org/10.1037/1089-2680.7.4.331.\n\n\nRiesthuis, Paul, Ivan Mangiulli, Nick Broers, and Henry Otgaar. 2022.\n“Expert Opinions on the Smallest Effect Size of Interest in False\nMemory Research.” Applied Cognitive Psychology 36 (1):\n203–15.\n\n\nRossi, Michael J, Jefferson C Brand, and James H Lubowitz. 2023.\n“Minimally Clinically Important Difference (MCID) Is a Low\nBar.” Arthroscopy: The Journal of Arthroscopic & Related\nSurgery. Elsevier.\n\n\nSawilowsky, Shlomo. 2009. “New Effect Size Rules of Thumb.”\nJournal of Modern Applied Statistical Methods 8 (2). https://doi.org/10.22237/jmasm/1257035100.\n\n\nSchäfer, Thomas, and Marcus A. Schwarz. 2019. “The Meaningfulness\nof Effect Sizes in Psychological Research: Differences Between\nSub-Disciplines and the Impact of Potential Biases.”\nFrontiers in Psychology 10. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813.\n\n\nSenior, Alistair M., Wolfgang Viechtbauer, and Shinichi Nakagawa. 2020.\n“Revisiting and Expanding the Meta-Analysis of Variation: The Log\nCoefficient of Variation Ratio.” Research Synthesis\nMethods 11 (4): 553–67. https://doi.org/10.1002/jrsm.1423.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” International Journal of\nEpidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nSteiger, James H. 2004. “Beyond the f Test: Effect Size Confidence\nIntervals and Tests of Close Fit in the Analysis of Variance and\nContrast Analysis.” Psychological Methods 9 (2): 164–82.\nhttps://doi.org/10.1037/1082-989X.9.2.164.\n\n\nTorchiano, Marco. 2020. Effsize: Efficient Effect Size\nComputation. https://doi.org/10.5281/zenodo.1480624.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics\nwith s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in\nR with the metafor\nPackage.” Journal of Statistical Software 36 (3): 1–48.\nhttps://doi.org/10.18637/jss.v036.i03.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical\nInference Without Repeated Sampling.” Synthese 200 (2):\n89. https://doi.org/10.1007/s11229-022-03560-x.\n\n\nW. T. Hoyt, A. C. Del Re &. 2014. MAd: Meta-Analysis with Mean\nDifferences. R Package. https://CRAN.R-project.org/package=MAd.\n\n\nWellington, Ian J., Annabelle P. Davey, Mark P. Cote, Benjamin C.\nHawthorne, Caitlin G. Dorsey, Patrick M. Garvin, James C. Messina, Cory\nR. Hewitt, and Augustus D. Mazzocca. 2023. “Substantial Clinical\nBenefit Values Demonstrate a High Degree of Variability When Stratified\nby Time and Geographic Region.” JSES International 7\n(1): 153–57. https://doi.org/10.1016/j.jseint.2022.10.003.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining\nUnbiased Results in Meta-Analysis: The Importance of Correcting for\nStatistical Artifacts.” Advances in Methods and Practices in\nPsychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611.\n\n\nWilliam Revelle. 2023. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nYang, Yefeng, Helmut Hillebrand, Malgorzata Lagisz, Ian Cleasby, and\nShinichi Nakagawa. 2022. “Low Statistical Power and Overestimated\nAnthropogenic Impacts, Exacerbated by Publication Bias, Dominate Field\nStudies in Global Change Biology.” Global Change Biology\n28 (3): 969–89. https://doi.org/10.1111/gcb.15972."
  }
]